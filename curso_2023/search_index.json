[["apresentação.html", "ESTUDOS ECOLÓGICOS 2023 1 Apresentação", " ESTUDOS ECOLÓGICOS 2023 Oswaldo G Cruz Atualizado em: 29 de setembro de 2023 1 Apresentação Disciplina: Estudos Ecológicos - Introdução aos Métodos de Análises Temporais e Espaciais (ENSP.82.134.2) Carga Horaria: 90 horas (10 aulas) - 3 Créditos Professor: Oswaldo G Cruz Monitores: Denis de Oliveira Rodrigues João Morais Período: 11 de Maio de 2023 a 13 de Outubro 2023 Ementa: Este curso se propõe a estudar métodos de séries temporais e estatística espacial, visando analisar a saúde de grupos populacionais a partir de sua localização temporal e espacial, bem como sua interação com o ambiente. Serão abordados os seguintes tópicos: Introdução e definições de estudos ecológicos; falácia ecológica vs falácia atomista; introdução, definições e importância das sereis temporais; tendência e sazonalidade; autocorrelação serial; filtros e alisamentos; modelo Box &amp; Jenkings (ARIMA); Tipologia dos dados espaciais; Padrão de pontos; Área; Geoestatística. Modelos aditivos generalizados (GAM). O curso terá aulas práticas realizadas em R. Cronograma Aula 01 (11/08) - Estudos Ecológicos Aula 02 (18/08) - Séries Temporais - Análise Exploratória Aula 03 (25/08) - Séries Temporais - Técnicas de Suavização Aula 04 (01/09) - Modelagem em Séries Temporais Aula 05 (08/09) - Introdução à Análise Estatística Espacial &amp; Padrão de Pontos Aula 06 (15/09) - Análise Espacial - Padrão de Pontos II Aula 07 (22/09) - Análise Espacial - Geoestatística Aula 08 (29/09) - Análise Espacial - Dados de Área I Aula 09 (06/10) - Análise Espacial - Dados de Área II Aula 10 (13/10) - Análise Espaço-Temporal &amp; Instruções e Dúvidas para o Trabalho Final Avaliação: Instruções para o Trabalho Final Prazo final: 30/11/2023 Clique AQUI para instruções "],["estudos-ecológicos.html", " 2 Estudos Ecológicos 2.1 O que são Estudos Ecológicos ? 2.2 Principais objetivos 2.3 Tipos de Variáveis Utilizadas 2.4 Tipos de Desenhos de Estudos Ecológicos 2.5 Aspectos históricos 2.6 Epidemiologia social 2.7 Árvores, Bosques ou as Florestas? 2.8 Falácia Ecológica ou viés de agregação 2.9 Problemas práticos 2.10 Vantagens 2.11 Desvantagens 2.12 Resumindo 2.13 Exercícios Propostos 2.14 Bibliografia sugerida", " 2 Estudos Ecológicos 2.1 O que são Estudos Ecológicos ? São estudos nos quais a unidade de análise (ou agregação) é uma população ou um grupo de pessoas, geralmente de uma área geográfica definida (ex: um país, um estado, uma cidade, etc.), em um determinado tempo definido. Definição Clássica: é um estudo observacional com a informação obtida e analisada no nível agregado. Geralmente são mais baratos e mais rápidos do que estudos envolvendo o indivíduo como unidade de análise. Procuram avaliar como os contextos (sociais, ambientais, etc) podem afetar a saúde de grupos populacionais. 2.2 Principais objetivos Gerar hipóteses etiológicas; Testar hipóteses etiológicas; Avaliar a efetividade de intervenções na população; Identificar áreas de risco. Exemplo 1: Em 1960, Friedman mostrou uma correlação positiva entre as taxas de mortalidade por doença coronariana (DC) e as vendas de cigarros per capita, em 44 estados americanos. Esta observação inicial contribuiu para a formulação da hipótese de que o tabagismo poderia causar doença coronariana Figura: Coronary heart disease mortality rates in the United States per capita cigarette sales in 1960, by state. (From FRIEDMAN GD, Cigarette smoking and geographic variation in coronary heart disease mortality in the United States. J. Chronic Dis. 20: 769, 1967) 2.3 Tipos de Variáveis Utilizadas Medidas Agregadas: Medidas agregadas por grupos. ex: incidência, prevalência, mortalidade, proporção de fumantes; Medidas Ambientais: Características físicas do contexto onde o grupo convive. ex: nível de poluição, precipitação; Medidas Globais: Atributos de grupos, organizações ou lugares, que não podem ser mensurados a nível individual. ex: IDH, densidade demográfica, existência de um tipo de sistema de saúde. Em uma análise ecológica, todas as variáveis são medidas agrupadas. Apenas se conhece a distribuição marginal de cada variável. Desfecho (Y) Fator em Estudo (X) Ocorreu Não ocorreu Total Exposto ? ? \\(E_1\\) Não Exposto ? ? \\(E_0\\) Total \\(D_1\\) \\(D_0\\) n 2.4 Tipos de Desenhos de Estudos Ecológicos Múltiplos Grupos: O objetivo desse tipo de estudo é a comparação entre todos os grupos ou conjuntos populacionais envolvidos no estudo. Ex: Análise Espacial. Desenhos de Séries Temporais: Avalia um determinado desfecho ao longo do tempo em uma determinada população geograficamente definida. Ex: Análise de Séries Temporais. Desenhos Mistos: É a combinação entre os dois desenhos citados, pois avalia a evolução de um determinado desfecho em diferentes grupos populacionais ao longo do tempo. Ex: Análise Espaço-Temporais, Estudos Multiníveis. 2.5 Aspectos históricos “Um estudo ecológico ou agregado focaliza a comparação de grupos, ao invés de indivíduos. A razão subjacente para este foco é que dados a nível individual da distribuição conjunta de duas (ou talvez todas) variáveis estão faltando internamente nos grupos; neste sentido um estudo ecológico é um desenho incompleto”. (Rothman, Kenneth J. et al. Modern epidemiology. Philadelphia: Wolters Kluwer Health/Lippincott Williams &amp; Wilkins, 2008.) “… estudar saúde no contexto ambiental. O objetivo é ambicioso: entender como o contexto afeta a saúde de pessoas e grupo através de seleção, distribuição, interação, adaptação, e outras respostas. Medidas de atributos do indivíduo não podem dar conta destes processos […] Sem medir estes contextos, nem padrão de mortalidade e morbidade, nem o espalhamento epidêmico, nem a transmissão sexual podem ser explicados” (Susser, Am.J.Public Health, 1994;84:825-835) A Epidemiologia é frequentemente definida em termos do estudo da determinação da distribuição da doença. Mas não se deve esquecer que quanto mais espalhada é uma causa particular, menos ela contribui para explicar a distribuição da doença.” “…dois tipos de perguntas etiológicas. A primeira busca as causas dos casos, e a segunda as causas da incidência.” “Aplicada à etiologia, a visão centralizada no indivíduo leva ao uso do risco-relativo como a representação básica da força etiológica: ou seja, o risco em indivíduos expostos relativo aos não-expostos. […] Esta pode ser geralmente a melhor medida de força etiológica, mas não é medida de […] importância em saúde pública.” “É rara a doença cuja taxa de incidência não tenha variado largamente, seja ao longo do tempo ou entre populações […] Isto significa que as causas da incidência, desconhecidas que sejam, não são inevitáveis. […] Mas identificar o agente causal pelos métodos tradicionais de caso-controle e coorte não terá sucesso se não houver suficientes diferenças na exposição dentro da população […] Nestas circunstâncias tudo os que os métodos tradicionais fazem é encontrar marcadores de susceptibilidade individual. A chave deve ser buscada nas diferenças entre populações ou em mudanças nas populações ao longo do tempo.” (Rose G. Sick individuals and sick populations. Int J Epidemiol. 2001 Jun;30(3):427-32; discussion 433-4.) “ … torna-se aparente que muitas das explicações convencionais dos determinantes da saúde - porque algumas pessoas são saudáveis e outras não - são, na melhor das hipóteses seriamente incompletas, se não simplesmente erradas. É assim, infelizmente, porque as sociedades modernas dedicam uma parte muito grande de sua riqueza, esforço e atenção tentando manter ou melhorar a saúde dos indivíduos que compõem suas populações. Estes esforços maciços são primeiramente canalizados para os sistemas de assistência à saúde, presumivelmente refletindo uma crença que receber uma boa assistência é o mais importante determinante de saúde.” (Evans,R.G.”Why are some people healthy and others not”) “Grande parte da pesquisa atual em epidemiologia está baseada no individualismo metodológico: a noção que a distribuição da saúde e doença em populações pode ser explicada exclusivamente em termos das características dos indivíduos.” (Diez-Roux AV. Bringing context back into epidemiology: variables and fallacies in multilevel analysis. AJPH,1998;88(2):216-22) “A evidência de modestos efeitos de vizinhança na saúde é razoavelmente consistente, apesar da heterogeneidade dos desenhos dos estudos […] e prováveis erros de medida. Ao chamar a atenção da saúde pública para os riscos associados com a estrutura social e ecológica de vizinhança, enseja-se possíveis intervenções inovadoras no nível da comunidade.” (Pickett KE, PearlL M. Multilevel analyses of neighbourhood socioeconomic context and health outcomes: a critical review. J Epidemiol Community Health 2001;55(2):111-22) 2.6 Epidemiologia social “…o ramo da epidemiologia que estuda a distribuição social e os determinantes sociais da saúde. A epidemiologia social incorpora um novo foco na comunidade como uma entidade em si, uma entidade mais complexa do que a soma das pessoas individuais que compõem a sociedade.” (Berkman L. F. &amp; Kawachi I. (Editors). Social Epidemiology. Oxford University Press, 2000) “Os médicos estão acostumados a pensar nos determinantes socioeconômicos da doença em termos dos fatores de risco de uma pessoa. […] Agora parece claro que a riqueza absoluta ou a renda é um determinante menos importante da saúde do que a relativa disparidade na renda ou a diferença de renda entre os ricos e os pobres.” (Kawachi I.; Kennedy B.P.; Wilkinson R.G. The Society and Population Health Reader: Income Inequality and Health. New Press, 1999). 2.7 Árvores, Bosques ou as Florestas? 2.7.1 As Árvores Suponha os dados abaixo, onde a variável “X” representa um efeito de exposição e a variável “Y” um taxa. Ao fazermos uma regressão obtemos uma correlação de apenas 0,1469 entre as duas variáveis. 2.7.2 Os Bosques Ao estratificarmos os dados evidencia-se uma estrutura, e ajustarmos uma regressão em cada grupo obtém-se: 2.7.3 As Florestas Tirando-se a média para cada grupo iremos obter quatro pontos sob os quais faremos uma regressão. O coeficiente de correlação obtido é rho = 0,9938 2.8 Falácia Ecológica ou viés de agregação “Viés que pode ocorrer porque uma associação entre duas variáveis no nível agregado não necessariamente representa uma associação no nível individual” O problema é que não podemos fazer inferências para níveis distintos: Inferir para o indivíduos a partir de dados agregados (falácia ecológica) Inferir para agregados populacionais a partir de dados individuais (falácia atomística ou individualista) Na estatística esse efeito é conhecido como Paradoxo de Simpson “Textos de Epidemiologia fazem uma avaliação consistente sobre estudos ecológicos: eles são tentativas cruas de estimar correlações em nível individual. […] Examinar esta questão de uma perspectiva diferente - como um problema geral de validade - mostrará que a falácia ecológica, conforme frequentemente usada, encoraja três noções interrelacionadas e falaciosas: que modelos em nível individual são mais perfeitamente especificados que os de nível ecológico, que correlações ecológicas são sempre substitutos para correlações de nível individual, e que variáveis de nível de grupo não causam doença.” (Schwartz, Am.J.Public Health, 1994;84:819-824) Religião e Sucídio Um exemplo clássico de estudo ecológico: Emile Durkheim (em 1897) associação ecológica positiva entre a proporção de indivíduos de religião Protestante e as taxas de suicídio (províncias da Prússia); Durkheim concluiu que Protestantes tinham maior probabilidade de se suicidarem do que os Católicos; Conclusão factível mas a inferência causal não é correta: poderiam ter sido os Católicos em províncias predominantemente Protestantes a cometer os suicídios, e a metodologia ecológica não permite discernir qual das duas hipóteses está certa. Para ler mais sobre este exemplo: Frans van Poppel and Lincoln H. Day. A Test of Durkheim’s Theory of Suicide - Without Committing the “Ecological Fallacy”. American Sociological Review, 1996. https://doi.org/10.2307/2096361 Posse de armas de fogo e suicídio Um exemplo mais recente: Miller et al (2003) realizaram um estudo ecológico no Estados Unidos comparando as frequências de posse doméstica de armas de fogo com as de suicídio por arma de fogo e por outros meios, por estado. Estados com maiores proporções de posse de armas de fogo apresentaram maiores taxas de suicídio por armas de fogo, mas a mesma frequência de suicídios por outros meios. Suicídios por outros meios serve como um “controle”. Segundo os autores, a posse de armas não deveria impactar suicídios por outros meios. Além disso, os autores assumem que os fatores de confundimento seriam os mesmos para suicídios por arma de fogo e por outros meios. Figura. Relação entre posse doméstica de armas de fogo e mortalidade por suicídio nos Estados Unidos, por estado, A) por armas de fogo, B) por outros meios que não armas de fogo, e C) todos. (Fonte: Miller et al. Am J Epidemiol. 2013;178(6):946–955) Concordam que todos os confundimentos são os mesmos para os dois grupos de mortes por suicídio? Concordam que suicídios por outros meios não devem ser afetados pela posse doméstica de armas de fogo? É correto concluir que, nos Estados Unidos: ter uma maior porcentagem de população com posse de armas causa taxas elevadas de suicídio por arma de fogo? possuir uma arma é uma causa para suicídio por arma de fogo? Um outro exemplo: Um pesquisador deseja estudar a relação entre acidentes de trânsito e a renda em três cidade distintas (A, B e C). pop renda_media tx_acidente A 24.08571 57.14 B 22.57143 42.86 C 21.41429 28.57 Observando o gráfico abaixo, o pesquisador observa um possível associaçãom entre a renda e a taxa de acidentes de trânsito; Quanto maior a renda, maior será a taxa de acidentes de trânsito. Observando os microdados, ou seja, os dados no nível individual, temos o seguinte: De posse desses dados no nível individual, é possível fazer a seguinte análise: Dessa forma observamos que os indivíduos que sofreram algum tipo de acidente de trânsito, apresentam a menor renda; Qual dos dois níveis de inferência está errado ? Qual é, então, o problema ? ? ? 2.9 Problemas práticos 1. Numerador: subregistro duplicidade de registros georreferenciamento: não localização informação incorreta preenchimento inadequado mudança na classificação ao longo do tempo 2. Denominador: espaçamento do censo migração mudança de fronteiras (!!!!) 3.Exposição: pode ocorrer em diversos lugares dificilmente mensurável com precisão uso de “proxy” diferentes áreas para medida de exposição e de efeito, e áreas não compatíveis Informações mais detalhadas (PNAD, amostra do censo) não extrapoláveis para populações pequenas 4. Análise: migração multicolinearidade 2.10 Vantagens Baixo custo e execução rápida, devido às fontes de dados secundários disponíveis; Conseguem estimar bem os efeitos de uma exposição quando ela varia pouco na área de estudo, pela comparação entre áreas (os estudos individuais não conseguem); Existem efeitos que somente podem ser medidos no nível ecológico, por ex. implantação de um novo sistema de saúde. 2.11 Desvantagens Informações sobre comportamento, atitudes e história clínica não estão disponíveis (dados pessoais não disponíveis); Depende da qualidade das informações disponíveis (fontes diversas); Não se leva em conta a variabilidade da característica estudada dentro do grupo; Difícil estabelecer temporalidade entre causa e efeito. Migração entre grupos (por exemplo, mora em uma área e trabalha em outra). 2.12 Resumindo Resgatando a ecologia: estudo das complexas inter-relações entre organismos vivos e o seu meio físico. Dados agregados – estudo ecológico clássico Mistura de dados individuais e agregados – modelos multinível Quando se estuda o tempo – séries temporais e modelos dinâmicos Quando é espacial – modelos clássicos de regressão ou espaciais Mistura espaço e tempo – modelos espaço-temporais Envolvendo relações entre indivíduos – redes 2.13 Exercícios Propostos As estatísticas internacionais indicam que o Chile tem uma das mais altas taxas de mortalidade por câncer de estômago. O país caracteriza-se por conter altos níveis de nitrato em seu solo, situação rara no mundo, neste particular. Estabeleceu-se a suspeita de ser o nitrato, em altas concentrações, um agente causal da neoplasia. Comparações regionais dentro do país, contrastando áreas com altas e baixas concentrações de nitrato, mostraram a mesma relação: alto teor da substância no solo, (alta mortalidade por este tipo de neoplasia). Um estudo caso-controle subseqüente foi realizado, mas a nível individual, não foi possível encontrar tal associação. A hipótese, entretanto, não foi totalmente descartada. Qual a importância desse estudo ecológico no estudo sobre causalidade: concentração de nitrato no solo vs câncer de estômago ? Os casos notificados de Influenza são maiores na cidade A do que na cidade B. As taxas de vacinação para a influenza são mais baixas na cidade A do que na cidade B. Quais das seguintes razões são razões pelas quais seria é incorreto presumir que uma maior vacinação na cidade B é o que está fazendo com que a cidade B tenha menos casos relatados de Influenza ? Escolha as opções corretas. A cidade A e a cidade B podem ter diferentes cepas de Influenza A cidade A e a cidade B podem ter proporções diferentes de pessoas nas suas populações que são especialmente vulneráveis à influenza (por exemplo, idosos, crianças e mulheres grávidas) A cidade A e a cidade B podem ter diferenças nos cuidados de saúde , acessibilidade aos serviços e acesso a diagnóstico da influenza A cidade A e a cidade B podem ter climas diferentes, levando a diferenças em como/onde as pessoas entram em contato com um ao outro. Isto pode afetar as taxas de transmissão de Influenza 2.14 Bibliografia sugerida BERKMAN, Lisa F.; KAWACHI, Ichirō; GLYMOUR, M. Maria (Ed.). Social epidemiology. Oxford University Press, 2014. DIEZ-ROUX, Ana V. Bringing context back into epidemiology: variables and fallacies in multilevel analysis. American journal of public health, v. 88, n. 2, p. 216-222, 1998. EVANS, Robert G.; BARER, Morris L.; MARMOR, Theodore R. (Ed.). Why are some people healthy and others not?: The determinants of the health of populations. Transaction Publishers, 1994. MORGENSTERN, Hal. Ecologic studies in epidemiology: concepts, principles, and methods. Annual review of public health, v. 16, n. 1, p. 61-81, 1995. PICKETT, Kate E.; PEARL, Michelle. Multilevel analyses of neighbourhood socioeconomic context and health outcomes: a critical review. Journal of Epidemiology &amp; Community Health, v. 55, n. 2, p. 111-122, 2001. ROSE, Geoffrey. Sick individuals and sick populations. International journal of epidemiology, v. 30, n. 3, p. 427-432, 2001. "],["introdução-às-séries-temporais.html", " 3 Introdução às Séries Temporais 3.1 O que são Séries Temporais ? 3.2 Hipóteses básicas do estudo das séries temporais 3.3 Classificação dos tipos de séries temporais 3.4 Processo Estocástico 3.5 Notação e Nomenclatura 3.6 Objetivos: análise de séries temporais 3.7 Estacionariedade 3.8 Pressuposto da Independência 3.9 Dependência serial 3.10 Função de Autocorrelação - FAC (Autocorrelation function - ACF) 3.11 Componentes de uma Série Temporal 3.12 Tendência 3.13 Sazonalidade 3.14 Ciclo 3.15 Termo Aleatório ou Ruído Branco 3.16 Composição dos Modelos de séries temporais 3.17 Decomposição de séries temporais 3.18 Prática no R 3.19 Exercícios Propostos 3.20 Outros materiais sobre Séries Temporais 3.21 Bibliografia sugerida", " 3 Introdução às Séries Temporais 3.1 O que são Séries Temporais ? Definição: Entende-se por Séries Temporais (ST) todo e qualquer conjunto de dados (absolutos ou relativos, discretos ou contínuos), ordenados cronologicamente. Condição: Esses dados seguem uma ordenação em função do tempo (dependência temporal). De modo geral, as séries temporais apresentam sequências de observações relativas a determinada variável ao longo de um intervalo específico de tempo (dia, mês, trimestre, ano, etc.), isto é, referem-se a fluxos de valores periódicos, os quais dão uma visão geral sobre o andamento ou comportamento da variável em análise. A maneira mais comum de visualizar séries temporais é usar um gráfico de linhas simples, em que o eixo horizontal representa os incrementos de tempo e o eixo vertical representa a variável que está sendo medida. Seguem abaixo alguns exemplo de séries temporais: As séries temporais podem ser de natureza regular ou irregular. As séries temporais regulares ou uniformes são aquelas que podem ser expressas sempre com o mesmo intervalo de tempo (frequência). As séries temporais irregulares ou não uniformes são aquelas em que as frequências de tempo são diferentes ou que apresentam dados ausentes (missing data). Algumas vezes podem ser transformadas em séries regulares agregando ouinterpolando os dados mensurados. 3.2 Hipóteses básicas do estudo das séries temporais Há um sistema causal relacionando as variáveis no tempo; Ao longo do tempo, o sistema influencia todos os dados sob análise, de modo regular e permanente; Os dados históricos refletem a influência média de um conjunto de fatores. Tais hipóteses se baseiam no pressuposto de que as relações apontadas pela experiência pregressa permitem prever o possível comportamento das variáveis sob análise, determinando se seu comportamento apresenta propriedades determinísticas e/ou aleatórias. 3.3 Classificação dos tipos de séries temporais Contínuas: A informação é obtida em qualquer intervalo de tempo (podendo ser discretizando em intervalos iguais) ou é acumulada por período. Ex: Temperatura, pluviosidade, partículas em suspensão. Discretas: Observações obtidas em intervalos de tempo discreto e equidistantes (ano, mês, dias, semanas epidemiológicas). Ex: Mortalidade infantil, notificações por DIC. Multivariada: São várias coleções de observações para a mesma sequência de períodos de tempo, ou seja,envolvem mais de uma série histórica. Ex: Número de homicídios e acidentes no Sudeste. Multidimensional: São várias coleções de observações para a mesma sequência de períodos de tempo, descrevendo o mesmo fenômeno em diferentes contextos. Ex: Número de AVCs em diversas UFs. 3.4 Processo Estocástico Um processo estocástico pode ser pensado de duas formas: um conjunto de possíveis trajetórias de um fenômeno físico que poderiam ser observadas; um conjunto de variáveis aleatórias, uma para cada tempo \\(t\\). Cada valor observado de uma trajetória é um dos possíveis valores que poderiam ter sido observados, de acordo com a distribuição de probabilidades da respectiva variável aleatória. Definir séries temporais consiste em determinar as funções matemáticas que apontam suas componentes básicas e permitem prever a evolução dos fenômenos estudados (como um eventual crescimento ou decrescimento futuro). As séries temporais podem ser matematicamente representadas por funções do tipo: \\[Z_t = f(tempo, a)\\] Sendo \\(Z_t\\) o valor da variável \\(Z\\) no tempo \\(t\\), e \\(a\\) a componente aleatória associada à função matemática do tempo. Série com a mesma estrutura: cada série é uma possível realização do mesmo processo estocástico. Trajetória ou série temporal ou função amostral 3.5 Notação e Nomenclatura Matematicamente, uma série temporal discreta é representada por: \\(Z_t = (Z_1 , Z_2 , Z_3 , ... , Z_n)\\), sendo: \\(Z\\), a variável observável e \\(t = 1,2,...,n\\), o parâmetro do tempo. Simulando duas séries temporais de um evento, com a mesma estrutura: 3.6 Objetivos: análise de séries temporais Objetivo Exemplo Descrição: verificar existência de tendência, sazonalidade, ciclos. Histogramas, boxplots, são ferramentas da análise exploratória descritiva Identificar tendência da AIDS; sazonalidade da dengue visando estabelecer melhor período de intervenção. Estabelecimento de causalidade: estudo da relação de causa-efeito Vacina X sarampo; Mortalidade por DIC X melhor assistência Classificação: identificação de padrões A série de leishmaniose tegumentar é “igual” à visceral? Controle: sistemas dinâmicos, caracterizados por uma entrada \\(X_t\\), uma série de saída \\(Z_t\\) e uma função de transferência \\(V_t\\) Modelar a resposta a medidas de controle de epidemia Monitoramento (nowcast): Detectar variações no comportamento da séries temporais conforme elas ocorram Dosagem de Hormônios ou de sinais vitais em CTI Predição (forecast) : prever o comportamento futuro de uma serie Predição de epidemias Atualização (nowcast): predição sobre o presente corrigir atraso de notificações 3.7 Estacionariedade Uma série temporal é dita estacionária quando ela se desenvolve no tempo aleatoriamente ao redor de uma média constante e com uma variância constante, refletindo alguma forma de equilíbrio estável. Na prática, a maioria das séries que encontramos apresentam algum tipo de não estacionariedade, como por exemplo, tendência. O modelo mais simples de uma séries temporal estacionária pode ser representado por: \\[Z_t = \\mu + a_t\\] Sendo \\(\\mu\\) a média do processo temporal e \\(a_t\\) a componente aleatória, chama de Ruído Branco em análises de séries temporais. A estacionariedade da séries temporais pode ser: 1\\(^a\\) ordem - média constante ao longo de todo o período 2\\(^a\\) ordem - variância constante ao longo de todo o período 3.7.1 Função de Autocovariância de um processo estacionário \\[\\gamma_h = E{\\{[Z_t - E(Z_t)][Z_{t-h} - E(Z_{t-h})]\\}}\\] A covariância não depende do tempo, mas da distância entre as observações. Um processo é considerado fracamente estacionário se: \\(E(Z_t)=\\mu\\), \\(\\forall t\\) (constante) \\(var(Z_t) = \\sigma^2\\), \\(\\forall t\\) (constante) \\(Cov(Z_t, Z_{t-h}) = \\gamma_h\\), \\(\\forall t\\) (não depende do instante no tempo, apenas da distância h) Sendo o ruído branco (White Noise), também chamado de Processo Puramente Randômico, uma variável aleatória \\(a_t\\), com média zero e variância \\(\\sigma²_a\\): \\(a_t \\sim N(0, \\sigma^2_a)\\) \\(Cov(a_t, a_{t-h}) = 0\\), \\(\\forall h \\neq 0\\) (Não correlacionados) 3.7.2 Por que a estacionariedade é importante ? A maioria das técnicas estatísticas utilizadas em séries temporais supõe que estas sejam estacionárias. Caso a série temporal não seja estacionária, será necessário transformar os dados. A transformação mais comum consiste em tomar diferenças sucessivas da série original, até se obter uma série estacionária. A primeira diferença de \\(Z_t\\): \\[\\bigtriangledown Z_t = Z_t - Z_{t-1}\\] A segunda diferença de \\(Z_t\\): \\[\\bigtriangledown^{2} Z_t = \\bigtriangledown[\\Delta Z_t] = \\bigtriangledown[Z_t - Z_{t-1}]\\] A n-ésima diferença de \\(Z_t\\): \\[\\bigtriangledown^{n} Z_t = \\bigtriangledown[\\bigtriangledown^{n-1} Z_t]\\] Logaritmo dos dados - Estabilizar a variância \\[\\bigtriangledown log Z_t = log Z_t - log Z_{t-1}\\] Transformações Box-Cox Pode-se diferenciar tantas vezes quanto necessário até estabilizar (porém, em geral se diferencia apenas uma vez, raramente duas vezes). Como saber se um processo é estacionário ? Visualizando a série, aplicando a decomposição, boxplots, etc. Testes Estatísticos, ex: Dickey-Fuller 3.8 Pressuposto da Independência Os métodos usuais de análise estatística de dados têm como pressuposto básico a independência dos eventos (casos). Ou seja, a ocorrência de um caso de doença em uma dada pessoa seria independente da ocorrência em outra pessoa. Pressupostos básicos para uma análise de regressão: \\(E(e_i) = 0\\) Variância \\(\\sigma^2\\) constante (homocedasticidade); \\(e_i \\sim N(0, \\sigma^2)\\) \\(e_i \\neq e_j\\), são independentes Na análise da incidência de doenças (ou qualquer outro indicador ecológico) ao longo do tempo isso não é verdade: a incidência em um determinado dia/mês ou ano em geral é correlacionada com a ocorrência no dia/mês/ano anterior. Esta correlação é expressa em uma função denominada função de autocorrelação. 3.9 Dependência serial Quanto à dependência, séries temporais podem possuir: Independência (sem dependência serial): série puramente aleatória ou ruído branco; Memória longa: a dependência desaparece lentamente (os valores de pontos no passado influenciam momentos muito adiante no tempo - exemplo: doenças com grande latência como hanseníase); Memória curta: dependência desaparece rapidamente (doenças de alta infecciosidade e “explosivas”” - exemplo: influenza). 3.10 Função de Autocorrelação - FAC (Autocorrelation function - ACF) O coeficiente de correlação entre \\(Z_{t}\\) e \\(Z_{t-h}\\) é chamado de autocorrelação de h-ésima ordem e é denotadado por: \\[ {\\rho}_{k}=\\frac {Cov\\left({Z}_{t},{Z}_{t-h} \\right)}{\\sqrt{Var\\left({Z}_{t},{Z}_{t-h} \\right)}} =\\frac{Cov\\left({Z}_{t},{Z}_{t-h} \\right)}{Var\\left({Z}_{t} \\right)} =\\frac{{\\gamma}_{k}}{{\\gamma}_{0}} \\] Temos então: \\({\\rho}_{0}=1\\) \\(-1\\leq {\\rho}_{l} \\leq 1\\) Um conjunto de autocorrelações, \\(\\left\\{\\rho_{h}\\right\\}\\), é chamado de função de autocorrelação de \\(Z_{t}\\). Para uma dada amostra, \\(\\left\\{Z_{t}\\right\\}_{t=1}^{T}\\), suponha que \\(\\overline{Z}\\) é a média amostral. Então, a autocorrelação amostral de primeira ordem de \\(Z_{t}\\) pode ser definida como: \\[ {\\hat{\\rho}}_{1}=\\frac{\\sum _{t=2}^{T}{\\left({Z}_{t}-\\overline{Z}\\right) \\left({Z}_{t-1}-\\overline{Z}\\right)}}{\\sum_{t=1}^{T}{{\\left({Z}_{t}-\\overline{Z}\\right)}^{2}}} \\] que é um estimador consistente de \\({\\rho}_{1}\\). Em geral, a autocorrelação amostral de h-ésima ordem de \\(Z_{t}\\) pode ser definida como: \\[ {\\hat{\\rho}}_{h}=\\frac{\\sum_{t=h+1}^{T}{\\left({Z}_{t}-\\overline{Z}\\right) \\left({Z}_{t-h}-\\overline{Z} \\right)}}{\\sum_{t=1}^{T}{{\\left({Z}_{t}-\\overline{Z}\\right)}^{2}}} \\] para \\(0\\leq h \\leq T-1\\). Por exemplo, suponha que você está avaliando uma série temporal qualquer e quer visualizar como as defasagens da série podem impactar seu valor atual (ou seja, se \\(Z_{t}\\) é relacionado com \\(Z_{t-h}\\) para \\(k\\ge1\\)). A função de autocorrelação pode ser usada para obter tal informação. Num primeiro momento, visualize os dados da série para 10 lags (defasagens). Observe que os lags se tornam novas colunas e na medida que elas aumentam, incrementa-se as linhas sem observações. Apesar da simples correlação entre os dados nos ajudar a identificar defasagens que poderíam contribuir para o comportamento da série em \\(t\\), precisamos fazer uso de testes estatísticos que verifiquem a significância da relação entre o valor atual e suas lags. Neste sentido, a função de autocorrelação tem grande importância. Abaixo, um exemplo de função de autocorrelação. Observe que há duas linhas horizontais que representam os limites do teste de significância sendo que valores acima ou abaixo da linha são estatisticamente significantes. Neste documento, apresentaremos o teste que é realizado. O correlograma é uma das principais ferramentas de análise exploratória em séries temporais, pois indica como cada valor em um dado instante de tempo \\(t\\) se relaciona com os valores em \\(t+1, t+2,...,t+j\\) Para um dado \\(h\\), os resultados da Função de Autocorrelação podem ser testados usando um teste que pode ser representado pelas seguintes hipóteses: \\[ \\begin{aligned} &amp;&amp; H_{0}: \\rho_{h}=0 \\\\ &amp;&amp; H_{1}: \\rho_{h}\\neq 0 \\end{aligned} \\] 3.11 Componentes de uma Série Temporal As séries temporais podem ser separadas em componentes sistemáticas (apontam movimentos regulares) e não sistemáticas (apontam movimentos irregulares). São elas: Componentes Sistemáticas (podem ou não estar presentes) Tendência Sazonalidade Ciclo Componentes Não Sistemáticas Aleatória ou Ruído Branco As análises exploratórias de séries temporais buscam isolar e interpretar as componentes. Tais componentes podem atuar de maneira isolada ou inter-relacionadas. 3.12 Tendência É a indicadora da direção global dos dados (ou movimento geral da variável), do percurso traçado e de sua linha contínua; É o efeito de longo prazo na média. Pode ser o aumento ou redução a longo prazo… 3.13 Sazonalidade São ciclos de curto prazo (não maiores que um ano), em torno da tendência; Costumam se referir a eventos ligados a estação do ano, vinculados ao calendário e geralmente repetidos a cada doze meses; Efeitos ligados à variações periódicas (semanal, mensal, anual, etc.); Padrões que ocorrem em intervalos fixos. Ex: Medidas de Temperatura (aumenta no verão e diminui no inverno). 3.14 Ciclo Os ciclos são oscilações (aproximadamente regulares) em torno da tendência. Podem dever-se a fenômenos naturais, socioculturais ou econômicos, como variações climáticas (ex: excesso ou falta de chuva pode produzir ciclos agrícolas) Variações que apesar de periódicas não são associadas automaticamente a nenhuma medida do calendário; Aumento ou redução de frequência sem intervalos fixos. Ex: Ciclos Econômicos e Ciclos de epidemias. A diferença entre os ciclos, propriamente ditos, e a sazonalidade é o período de avaliação (curto e longo); A semelhança é que ambos definem oscilações relativamente regulares em torno da tendência. Na área de saúde é pouco comum encontrarmos ciclos, ainda que possam existir. 3.14.1 Como detectar a sazonalidade ? Visualmente Boxplots seasonplot monthplot decomposição 3.15 Termo Aleatório ou Ruído Branco Conceitualmente, a componente aleatória é uma mistura de pertubações bruscas, irregulares e esporádicas nos movimentos das séries que tipificam os fenômenos. Na realidade é resultante dos efeitos de múltiplas causas que dificilmente/não conseguem ser previstos. Exemplos típicos de eventos aleatórios: Secas Enchentes Terremotos Ocorrência de epidemias Crise política Conflitos Socioeconômicos 3.16 Composição dos Modelos de séries temporais A série pode ser descrita como sendo a soma ou multiplicação dos componentes (tendência, sazonalidade, ciclicidade - se houver - e termo aleatório). 3.16.1 Modelo Aditivo \\[Z_t = T_t + S_t + a_t\\] sendo \\(t = 1,2, ..., N\\) Essa composição de modelo sugere que a variação sazonal parece constante, não muda quando da série temporal aumenta. 3.16.2 Modelo Multiplicativo \\[Z_t = T_t . S_t . a_t\\] Essa composição de modelo sugere que a sazonalidade varia em conjunto com a tendência (aumenta de amplitude quando aumenta a tendência). Pode ser transformado em aditivo usando \\(log\\). \\[log(Z_t) = log(T_t . S_t . a_t) = log(T_t) + log(S_t) + log(a_t)\\] 3.17 Decomposição de séries temporais 3.18 Prática no R 3.18.1 A biblioteca ts é a mais utilizada no R Na biblioteca ts a função mais utilizada tem o mesmo nome ts , não é necessário chamar library(ts) pois a mesma já se encontra carregada por default. A função ts tem como argumentos principais: data: um vetor, data.frame ou matriz com dados para a série start: tempo da primeira observação e/ou end: tempo da última observação frequency: quantidade de observações por unidade de tempo, podendo representar: Anual = 1, Trimestral = 4, Mensal = 12 e Semanal = 52 3.18.2 Simulando uma Série Temporal Vamos simular uma série usando a função rnorm para gerar 60 pontos aleatórios , com media 0 e desvio 1 em seguida vamos usar a função ts para transformar o vetor em uma objeto ts e finalmente fazer um gráfico. # Uma serie temporal normalmente distribuída serie &lt;- rnorm(60) # usando a função ts para criar um objeto da classe ts # pode-se usar também end=c(2016,12) mas basta um! serie.ts &lt;- ts(serie,start = c(2012,1), frequency=12) Vamos observar agora como é um objeto do tipo ts serie.ts Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec 2012 -0.850789274 1.098551160 -1.206355025 0.581862276 0.863689895 0.983052004 -1.271260251 -0.001062756 -0.960910158 1.259209306 -0.866606890 0.249044190 2013 -0.086168223 -0.492884341 -0.268998914 -0.399240236 -0.944297510 1.013188332 -0.408704068 -0.181086790 1.183755855 -1.608494491 -2.272439862 -2.040607916 2014 -0.648006354 0.717722237 -0.103400012 0.327017362 -0.977597331 0.341442983 0.043822392 -1.021578363 0.793427719 0.501624458 -1.554470354 0.883497837 2015 -0.207767801 1.831374014 0.177264382 -0.805840648 1.947932470 0.121197052 -0.354949127 1.052866250 1.163113962 -0.798832818 0.775455294 -1.995007352 2016 -0.350562193 0.380864742 0.258213780 0.964510263 -0.491278153 0.081638756 -0.161770723 1.272817069 -0.720269271 1.541051559 0.853661550 0.802399491 Para se obter o gráfico basta usar a função plot # gráfico da série plot(serie.ts) 3.18.3 Importando uma vetor e transformando em Série Temporal Vamos usar agora um exemplo de casos caxumba em Nova York de 1928-1972 proveniente do livro: Yorke, J.A. and London, W.P. (1973) “Recurrent Outbreaks of Measles, Chickenpox and Mumps”, American Journal of Epidemiology, Vol. 98, pp.469 Observe que a partir de um dado puramente vetorial já podemos obter um objeto ts Clique aqui para ver como são os dados brutos Para ler os dados utilizaremos a função scan que importa dados vetoriais. Nesse exemplo estaremos usando os dados diretos de uma URL mas o dado poderia estar no seu disco, assim você importaria localmente! OBSERVAÇÃO: No MS-Windows existe algum problema ao acessar sites seguros (HTTPS) assim vamos definir uma função que permita o acesso a esse tipo de site. podemos tentar duas coisas: options(url.method=&quot;libcurl&quot;) Ou criar uma função: scan.win &lt;- function(x) {scan(url(x,method = &#39;libcurl&#39;))} No Windows 10 aparentemente não é necessário o procedimento acima mas fique atento que ao longo do curso estaremos importando dados com frequência. Descubra como fazer essa importação funcionar no seu computador! Exemplo com dados de Caxumba, não se esqueça de definir a função acima! dados &lt;- scan(&#39;https://raw.githubusercontent.com/ogcruz/dados_eco_2023/main/exemplos/caxumba.dat&#39;) caxumba &lt;- ts(dados,start = c(1928,1),frequency = 12) plot(caxumba) 3.18.4 Utilizando dados da incidência de dengue nas Filipinas, 2008 - 2016 Exemplo serie mensal da Incidência de dengue por 100,000hab em uma região das Filipinas de 2008 to 2016. Fonte: Kaggle dengue &lt;- read.csv(&quot;https://raw.githubusercontent.com/ogcruz/dados_eco_2023/main/dados/denguecases2.csv&quot;) head(dengue) ## Month Year Dengue_Cases ## 1 Apr 2008 131.13331 ## 2 Aug 2008 159.97741 ## 3 Dec 2008 93.65630 ## 4 Feb 2008 49.38712 ## 5 Jan 2008 79.85915 ## 6 Jul 2008 152.63940 Antes de colocando em formato de série temporal utilizando a biblioteca ts do R, precisamos ordenar o dataframe para que possamos transformar corretamente em uma série temporal uma vez que a função baseia-se somente na ordem de entrada. Assim vamos alterar a coluna Month em um fator para que possamos manter a ordem dos meses e em seguida usar a função order para reordenar todo o dataframe. dengue$Month &lt;- factor(dengue$Month,levels = month.abb) dengue &lt;- dengue[order(dengue$Year,dengue$Month),] head(dengue) ## Month Year Dengue_Cases ## 5 Jan 2008 79.85915 ## 4 Feb 2008 49.38712 ## 8 Mar 2008 115.13416 ## 1 Apr 2008 131.13331 ## 9 May 2008 129.20466 ## 7 Jun 2008 210.24223 Com o dado na devida ordem e podemos continuar a transformação em série temporal. # Convertendo os dados para o formato de Séries Temporais # A frequency=12 foi especificado pois queremos mostrar dos dados mensais denguets &lt;- ts(dengue$Dengue_Cases,start=c(2008,1),frequency=12) plot(denguets, ylab=&quot;Casos de Dengue&quot;, xlab=&quot;Tempo&quot;) Verificando e testando a autocorrelação dos casos de dengue. \\[ \\begin{aligned} &amp;&amp; H_{0}: \\rho_{h}=0 \\\\ &amp;&amp; H_{1}: \\rho_{h}\\neq 0 \\end{aligned} \\] acf(denguets, lag.max=20, main=&quot;Função de Autocorrelação&quot;) Box.test(denguets, lag=20, type=&quot;Ljung-Box&quot;) Box-Ljung test data: denguets X-squared = 271.51, df = 20, p-value &lt; 2.2e-16 Através do gráfico e do teste do ACF, é possível verificar que a incidência de dengue é correlacionada ao longo do tempo. Fazendo uma análise descritiva da série temporal denguets Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec 2008 79.85915 49.38712 115.13416 131.13331 129.20466 210.24223 152.63940 159.97741 307.65474 58.74152 36.17346 93.65630 2009 87.96879 96.47914 190.36630 98.15255 124.03703 215.76350 40.66555 39.54446 85.84889 70.55726 95.37771 171.74569 2010 81.62430 95.59756 174.13877 33.54686 32.44285 60.04658 55.22568 90.16454 135.63883 74.17619 71.39315 138.44613 2011 33.66412 46.52083 93.34766 58.76998 134.83900 187.12547 109.27259 88.23193 143.89752 53.87635 91.05780 184.88198 2012 114.48472 251.27249 400.20592 162.21779 191.47219 276.13014 75.88479 128.88087 423.70277 239.40052 421.72803 607.49949 2013 244.44260 374.95060 530.46735 75.91858 132.94225 678.00967 387.48040 448.67952 515.58071 303.43820 530.61714 696.56174 2014 56.28797 113.67882 522.16422 249.49254 326.64717 366.61897 237.79033 509.79051 469.48018 41.43502 87.44494 226.41755 2015 134.40762 255.93445 243.46980 212.50300 485.76066 331.08272 33.81837 60.95752 121.16441 104.00878 210.63816 160.61858 2016 201.73756 362.34941 270.55192 18.94775 57.02598 57.41445 70.58666 145.36292 116.41069 119.08265 245.19665 194.46347 Vamos verificar a propriedades da série: Estatística Função R Valor Comprimento da Serie length() 108 media mean() 192.0458131 mediana median() 134.62331 máximo max() 696.56174 minimo min() 18.947748 amplitude range() 18.947748, 696.56174 frequência frequency() 12 período de inicio start() 2008, 1 período de fim end() 2016, 12 Pode-se pedir também o sumário da série! summary(denguets) Min. 1st Qu. Median Mean 3rd Qu. Max. 18.95 81.18 134.62 192.05 246.27 696.56 hist(denguets,breaks = 10) boxplot(denguets,col=&#39;lightblue&#39;) Mudando a janela de tempo da série temporal: observando apenas os dados de Jan 2010 até Dez de 2012. denguets2 &lt;- window(denguets, start=c(2010,1),end=c(2012,12),frequency=12) plot(denguets2, ylab=&quot;Casos de Dengue&quot;, xlab=&quot;Tempo&quot;) Decompondo a série temporal Decompondo a série temporal dos casos de dengue via método clássico decompose (Decomposição via Médias Móveis): plot(decompose(denguets)) Decompondo a série temporal dos casos de dengue via STL (Seasonal and Trend decomposition using Loess): É mais robusta, mais sensível a vários tipos de sazonalidade e lida melhor com os outliers. plot(stl(denguets, s.window=&quot;periodic&quot;)) decom_dengue &lt;- stl(denguets,12) head(decom_dengue$time.series) seasonal trend remainder Jan 2008 -71.00930 132.7212 18.147256 Feb 2008 -26.81769 132.8553 -56.650467 Mar 2008 81.88596 132.9894 -99.741166 Apr 2008 -65.11369 133.1235 63.123548 May 2008 -22.99327 132.7289 19.468992 Jun 2008 80.32281 132.3344 -2.415003 plot(decom_dengue) Trend &lt;- decom_dengue$time.series[,2] Seasonal &lt;- decom_dengue$time.series[,1] Random &lt;- decom_dengue$time.series[,3] Refazendo o sinal original da séries temporais através das componentes: recomposed_dengue &lt;- Trend+Seasonal+Random par(mfrow=c(1,2)) plot(denguets, ylab=&quot;Incidência Dengue&quot;, main=&quot;Original&quot;) plot(as.ts(recomposed_dengue), ylab=&quot;Incidência Dengue&quot;, main=&quot;Recomposta&quot;) Em algumas séries temporais não é fácil avaliar suas componentes de maneira visual, ou seja, de maneira gráfica. Para podermos avaliar melhor precisamos utilizar alguns testes estatísticos. Outra forma de fazer gráficos é através das bibliotecas ggplot e ggfortify: library(ggplot2) library(ggfortify) autoplot(denguets) autoplot(decompose(denguets)) Avaliando a Estacionariedade da série temporal Segundo o teste de Dickey-Fuller: \\(H_{0}\\): A série temporal não é Estacionária \\(H_{1}\\): A série temporal é Estacionária Alguns exemplos: Testando a estacionariedade da série dos casos de dengue: library(tseries) adf.test(denguets) Augmented Dickey-Fuller Test data: denguets Dickey-Fuller = -1.9795, Lag order = 4, p-value = 0.5851 alternative hypothesis: stationary Como p-valor = 0,5851, não rejeitamos a hipótese nula, ou seja, não há indícios da série temporal ser estacionária. Avaliando a tendência em uma série temporal Construindo uma reta baseado no modelo de regressão linear simples para verificar a tendência da incidência da dengue: plot(denguets, main = &quot;Incidência de Dengue 2008 a 2016&quot;) abline(reg=lm(denguets ~ time(denguets)), col = &quot;red&quot;) Construindo uma curva suavizada baseada na função lowess para verificar tendência da incidência da dengue: plot(denguets, ylab=&quot;Casos de Dengue&quot;, xlab=&quot;Tempo&quot;) library(Kendall) lines(lowess(time(denguets),denguets),lwd=3, col=2) Uma outra forma de mostrar a tendência da série temporal é fazendo a média anual. Observe que a curva se parece um pouco com a curva do lowess porém menos suave. plot(aggregate(denguets, FUN=mean)) Avaliando a Sazonalidade da série temporal De maneira visual podemos utilizar algumas técnicas gráficas, tais como: Boxplot boxplot(denguets ~ cycle(denguets)) Monthplot monthplot(denguets) Seasonplot (funçao disponibilizada pela library forecast) library(forecast) seasonplot(denguets,col=terrain.colors(6),lwd=2) 3.19 Exercícios Propostos Utilizando os bancos: Série mensal de óbitos por doenças respiratórias na região Sul do Brasil de 1996 a 2017 (pode ser acessado na URL https://bit.ly/obtsul, fonte: DataSUS/MS) Série semanal do numero de casos Malaria nos EUA de 1974 a 1984 (pode ser acessado na URL https://bit.ly/cdcmal, fonte:CDC/US) Importe a série para um formato ts e faça: Uma análise exploratórias dos dados em formato séries temporais; Decomponha a série temporal; Através de análises gráficas e/ou testes estatísticos, avalie e verifique a existência de tendência e sazonalidade na série. 3.20 Outros materiais sobre Séries Temporais Time Series Task View: https://cran.r-project.org/web/views/TimeSeries.html Blog, Ebook and Forecast Documentation by Rob Hyndman: https://otexts.org/fpp2/intro.html STL: A seasonal-trend decomposition procedure based on loess https://www.scb.se/contentassets/ca21efb41fee47d293bbee5bf7be7fb3/stl-a-seasonal-trend-decomposition-procedure-based-on-loess.pdf) Stackoverflow Community: https://stackoverflow.com/questions 3.21 Bibliografia sugerida DIGGLE, Peter. Time Series: A Biostatistical Introduction (Oxford Statistical Science Series, No. 5) 1st Edition, 1996 FERREIRA, Pedro Guilherme Costa. Análise de Séries Temporais em R: curso introdutório. 2018. METCALFE, Andrew V.; COWPERTWAIT, Paul SP. Introductory time series with R. Springer-Verlag New York, 2009. MORETTIN, Pedro A.; TOLOI, Clélia M.C. Análise de Séries Temporais: Modelos Lineares Univariados. Bluscher - ABE - Projeto Fisher. Edição 3, 2018. WOODWARD, Wayne A.; GRAY, Henry L.; ELLIOTT, Alan C. Applied time series analysis with R. CRC press, 2017. "],["transformações-e-suavizações.html", " 4 Transformações e Suavizações 4.1 Introdução 4.2 Transfomação dos dados em ST 4.3 Métodos de Alisamento ou Suavização 4.4 Médias móveis 4.5 Kernel 4.6 Loess / Lowess 4.7 Splines 4.8 Exercícios Propostos 4.9 Bibliografia sugerida", " 4 Transformações e Suavizações 4.1 Introdução As transformações e as funções de alisamento (suavização) em séries temporais são normalmente empregados para nos ajudar a ver melhor os padrões existentes - tais como as tendências e sazonalidade - e suavizar as oscilações irregulares (ruídos) para que possamos obter uma série mais limpa com um sinal mais claro. Estas técnicas não nos fornecem um modelo, mas podem ser um bom primeiro passo na descrição para os vários componentes da série. 4.2 Transfomação dos dados em ST Alguns exemplos: Utilização de funções: Logarítmicas, Potências, Exponenciais, ou transformação Box-Cox. Diferenciação: Séries não estacionárias. Objetivo é transformar a série em estacionária, estabilizando a média ( \\(\\bigtriangledown Z_t\\) ). Box-Cox: Estabiliza a variância. (também o log , sqrt etc…) Médias Móveis: Permite a suavização dos dados da ST. Reduz outliers e captura a tendência. Outras técnicas de suavização: Kernel, Loess/Lowess, Splines e Generalized Additive Model (GAM). Para demostrar o resultado dessas transformações, iremos aplicar algumas dessas técnicas. 4.2.1 Transformações Box-Cox, Diferenciação e Logarítmica Esta é uma técnica de transformação de dados útil para estabilizar a variância, tornar os dados mais semelhantes à distribuição normal, melhorar a validade das medidas de associação e para outros procedimentos de estabilização de dados. Box and Cox (1964) propuseram uma transformação na variável da ST \\(Z_t\\), que depende do parâmetro \\(\\lambda\\) da seguinte forma: \\[ Z_{t}(\\lambda) = \\left\\{ \\begin{array}{rc} Z_t^{\\lambda}, &amp;\\mbox{se} \\quad \\lambda \\neq 0, \\\\ ln(Z_t) , &amp;\\mbox{se} \\quad \\lambda = 0. \\end{array}\\right.\\] Se o valor de \\(\\lambda\\) é igual a zero, a transformação logarítmica da sequência inicial é realizada. Se o valor de lambda difere de zero, a transformação é por lei exponencial. Quando \\(\\lambda\\) = 1, a série é analisada em sua escala original. \\(\\lambda = 1/2\\) corresponde à transformação da raiz quadrada. Para que a transformação seja aplicável, a série deve ser estritamente positiva. Ex: Utilizando os dados do dataset Air Passengers, temos: library(forecast) par(mfrow=c(3,2)) # Série original plot(AirPassengers, ylab=&quot;Passageiros&quot;, main=&quot;Original&quot;) # Lambda = 0, Logaritmica t1 &lt;- BoxCox(AirPassengers,lambda =0 ) plot(t1, ylab=&quot;Passageiros&quot;, main=&quot;Lambda = 0, Logaritmica&quot;) # Lambda = 0.1 t2 &lt;- BoxCox(AirPassengers,lambda =.1 ) plot(t2, ylab=&quot;Passageiros&quot;, main=&quot;Lambda = 0.1&quot;) # Gera labda automático lbd &lt;- BoxCox.lambda(AirPassengers) # print(lbd) t3 &lt;- BoxCox(AirPassengers,lambda =lbd ) plot(t3, ylab=&quot;Passageiros&quot;, main=paste0(&quot;Lambda Automático (&quot;,round(lbd,4),&quot;)&quot;)) # Diferenciação t4 &lt;- diff(AirPassengers) plot(t4, ylab=&quot;Passageiros&quot;, main=&quot;Diferenciação &quot;) # Logarítmo t5 &lt;- diff(log(AirPassengers)) plot(t5, ylab=&quot;Passageiros&quot;, main=&quot;Diferença do Log&quot;) 4.3 Métodos de Alisamento ou Suavização Uma função é suave quando contínua e derivável em todos os pontos. Utiliza-se a expressão Funções de Suavização para definir funções que, aplicadas sobre um conjunto numérico, retornam outro conjunto cujos valores tendem à média (local ou global). Funções de suavização são utilizadas quando se supõe que o fenômeno é, de fato, suave, e as observações apresentam variabilidade aleatória pouco relevante. Também são utilizadas quando se deseja modelar a estrutura geral o fenômeno, desconsiderando cada ocorência isolada. 4.3.1 Algumas funções de suavização Existem diversos tipos de funções de suavização no R. Dentre as principais, destacamos: Médias móveis: O método mais simples de suavização. Kernel density: Estimativa de densidade de probabilidade – equivale à uma média ponderada. Loess/Lowess: locally (weighted) estimated scatterplot smoothing – estende a mesma idéia, mas os valores entram em uma regressão (ponderada ou não), retornando não apenas a média, mas também uma inclinação (\\(\\beta\\)). Splines: cúbica, quadrática, p-splines, thin plate splines etc… Todas essas funções podem ser uni ou multi-dimensionais. Existem ainda várias outras funções para suavizar. 4.4 Médias móveis Considere a ST estacionária e localmente constante, composta de seu nível e mais um ruído aleatório: \\[ Z_t = \\mu_t + a_t , \\] \\[t = 1,2,...,N \\] Sendo \\(E(a_t) = 0\\), \\(Var(a_t)=\\sigma^2_a\\) e \\(\\mu_t\\) é um parâmetro desconhecido que varia com o tempo. A técnica de média móvel consiste em calcular a média aritmética das \\(h\\) observcações mais recentes, ou seja: \\[M_t = \\dfrac{Z_t+Z_{t-1}+...+Z_{t-h+1}}{h}\\] Denotamos por \\(h\\) o comprimento da média. Desta forma, \\(M_t\\) é uma estimativa do nível \\(\\mu_t\\) que não leva em consideração as observações mais antigas. Note que a cada período a observação mais antiga é substituída pela mais recente, calculando-se uma média nova. Vamos exemplificar o que acontece quando se usa uma janela de 3 meses. Repare as 3 primeiras observações na série original: Agroa a série com Média Móvel 3. Note que o primeiro e o último valores da série agora são NA: Observe que o valor para fevereiro é calculado a partir dos 3 primeiros valores. (16.87 + 15.08 + 15.07)/3 = 15.67 Note que o exemplo acima é uma media móvel centrada com uma janela de 3 meses. Nesse caso, se perde o primeiro e o último pontos da série. Existe ainda a possibilidade de se alinhar a media móvel à direita ou à esquerda, sendo a media centrada o default na grande maioria das funções. Exemplo: Utilizando a ST mensal de mortes em estradas no Reino Unido (1969–1984), temos : library(forecast) plot(UKDriverDeaths,ylab=&#39;&#39;,main=&quot;Média Móvel Centrada&quot;) lines(ma(UKDriverDeaths,12),col=2,lty=1,lwd=2) lines(ma(UKDriverDeaths,4),col=4,lty=2,lwd=2) legend(&quot;topright&quot;,legend=c(&quot;Óbitos&quot;,&quot;Média Móvel 4&quot;,&quot;Média Móvel 12&quot;), col = c(&quot;black&quot;,&#39;blue&#39;,&#39;red&#39;), lty=c(1,2,1), cex=1) Existem muitas funções em diversos pacotes que possibilitam fazer a media móvel. No exemplo anterior, usamos a função ma() do pacote forecast. Vamos a seguir usar a função rollmean() do pacote zoo, mostrando como ficariam as médias móveis com esses outros alinhamentos. plot(UKDriverDeaths,ylab=&#39;&#39;,main=&quot;Média Móvel alinhada à direita&quot;) lines(zoo::rollmean(UKDriverDeaths,k=6,align = &#39;right&#39;),col=4,lwd=2) plot(UKDriverDeaths,ylab=&#39;&#39;,main=&quot;Média Móvel alinhada à esquerda&quot;) lines(zoo::rollmean(UKDriverDeaths,k=6,align = &#39;left&#39;),col=6,lwd=2) 4.4.1 Utilização das Médias Móveis Processo de Transformação - “Suavização da Série” Remoção de outliers Identificação de tendências Ex: Médias Móveis - ordem 5 4.5 Kernel O algoritmo: Seja \\((x)\\) ponto para o qual se deseja estimar f(.) Defina a função kernel: simétrica, unimodal, centrada em \\((x)\\), que cai a zero nos limites da vizinhança. Esse subconjunto dos dados, também denominado janela, largura de banda ou parâmetro de suavização controla flexibilidade do kernel. Repita o procedimento para diversos \\((x)\\) Conecte os pontos \\[\\hat{f_h}(x) = \\dfrac{1}{Nh}\\sum{K \\left( \\dfrac{x-x_i}{h}\\right)}\\] \\(h\\) \\(\\rightarrow\\) largura de banda \\(K\\) \\(\\rightarrow\\) função de suavização Kernel Gaussiano: \\[k(x)=\\dfrac{1}{\\sqrt{2\\pi}} exp(1/2 x^2)\\] 4.5.1 Diferentes Funções Kernel Vamos a uma demostração: Ao chamar a função demo() você vai ver um painel de controle como o abaixo: E uma gráfico de uma função kernel como abaixo. Utilize o controle deslizante mostrado pela seta e altere as opções. Série Temporal: Leptospirose vs Clima. #declara local na internet onde estão os dados local &lt;- &quot;https://raw.githubusercontent.com/ogcruz/dados_eco_2023/main/dados/&quot; ## a função paste0 junta o local e o nome do arquivo! lepto &lt;- read.csv2(paste0(local,&quot;leptoruido.csv&quot;), header=T) dia &lt;- read.table(paste0(local,&quot;climadia.dat&quot;), header=T) library(lattice) library(car) library(sm) library(survival) #XY condicionado xyplot(cases + totrain + tempmed ~ week, outer=TRUE, layout=c(1, 3), pch=1, type=&quot;l&quot;,ylab=&quot;&quot;, scales=list(x=list(relation=&#39;same&#39;), y=list(relation=&#39;free&#39;)), data=lepto) Exemplos de Kernel em histogramas #### density - default &quot;gaussian&quot; hist(lepto$totrain, breaks=seq(0,400,by=10),freq=F, col=&quot;darkgray&quot;, main = &quot;Chuva&quot;) lines(density(lepto$totrain , 4),col=2) lines(density(lepto$totrain , 12),col=&quot;blue&quot;) legend(&quot;topright&quot;, c(&quot;4&quot;,&quot;12&quot;), title=&quot;BW&quot;, col=c(2,&quot;blue&quot;), lty=c(1,1)) hist(lepto$cases, breaks=seq(0,36,by=1),freq=F, col=&quot;darkgray&quot;, main = &quot;Casos Leptospirose&quot;) lines(density(lepto$cases , 1),col=2) lines(density(lepto$cases , 2),col=&quot;green&quot;) lines(density(lepto$cases , 4),col=&quot;blue&quot;) legend(&quot;topright&quot;, c(&quot;1&quot;,&quot;2&quot;,&quot;4&quot;), title=&quot;BW&quot;, col=c(2,&quot;green&quot;,&quot;blue&quot;), lty=c(1,1,1)) hist(lepto$cases, breaks=seq(0,36,by=1),freq=F, col=&quot;darkgray&quot;) lines(density(lepto$cases , 2, kernel = &quot;rectangular&quot; ),col=2) lines(density(lepto$cases , 2, kernel = &quot;triangular&quot; ),col=&quot;green&quot;) lines(density(lepto$cases , 2, kernel = &quot;gaussian&quot; ),col=&quot;blue&quot;) legend(&quot;topright&quot;, c(&quot;retangular&quot;,&quot;triangular&quot;,&quot;gaussiano&quot;), title=&quot;BW&quot;, col=c(2,&quot;green&quot;,&quot;blue&quot;), lty=c(1,1,1)) Aplicando o Kernel em ST: plot(totrain ~ week, data=lepto, cex=.3, pch=19,col=&quot;gray35&quot;,ylab=&quot;Kernel Chuva&quot;, xlab=&quot;Semana&quot;, main=&quot;Kernel Smooth&quot;) lines(ksmooth(lepto$week, lepto$totrain, kernel = &quot;normal&quot;, bandwidth = 52), col=3) lines(ksmooth(lepto$week, lepto$totrain, kernel = &quot;normal&quot;, bandwidth = 21), lwd=2,col=7) lines(ksmooth(lepto$week, lepto$totrain, kernel = &quot;normal&quot;, bandwidth = 13), col=2) lines(ksmooth(lepto$week, lepto$totrain, kernel = &quot;normal&quot;, bandwidth = 4), col=4) legend(&quot;topright&quot;,c(&quot;52 sem.&quot;,&quot;21 sem.&quot;,&quot;13 sem.&quot;,&quot;4 sem.&quot;),fill=c(3,7,2,4), title=&quot;Janela&quot;,bty=&quot;n&quot;) 4.5.2 Largura de Banda Como estimar a largura de banda ? Validação cruzada \\[CV(\\lambda) = \\frac{1}{n} \\sum_{j=1}^n \\left( y_i - \\hat{f}_{\\lambda(j)} (x_j) \\right) ^2\\] O ponto deixado fora do ajuste a cada vez, \\(\\lambda\\) o valor que minimiza essa equação. ### efeito de borda # exluindo os pontos 1 a 3 para forcar borda lepto2 &lt;- lepto[-c(1,2,3),] hcv(lepto2$week, lepto2$totrain, display=&quot;lines&quot;) [1] 6.129 abline(v= 6.129356, col=2,lty=2) plot(totrain ~ week, data=lepto2, cex=.3, pch=19,col=&quot;gray35&quot;,ylab=&quot;Kernel Chuva&quot;, xlab=&quot;Semana&quot;, main=&quot;Kernel Smooth -- Efeito de Borda&quot;) lines(ksmooth(lepto$week, lepto$totrain, kernel = &quot;normal&quot;, bandwidth = 21 ), col=2) lines(ksmooth(lepto2$week, lepto2$totrain, kernel = &quot;normal&quot;, bandwidth = 6.1 ), col=1) legend(&quot;topright&quot;,c(&quot;21 sem.&quot;,&quot;6 sem.&quot;),fill=c(2,1), title=&quot;Janela&quot;,bty=&quot;n&quot;) 4.5.3 Utilização do Kernel Vantagens: simples, ótimo para análise exploratória. Desvantagens: efeito de borda. Muito sensível à largura de banda. Escolha automática de largura de banda pode não ser o desejável. Pouco sensível à forma da função, desde que suave. 4.6 Loess / Lowess Loess (Locally Estimated Scatterplot Smoothing) e lowess (Locally Weighted Scatterplot Smoothing) são funções semelhante ao kernel, mas os valores são estimados a partir de uma regressão local ao invés da média ponderada. No caso do loess a cada ponto do conjunto de dados um polinômio é ajustado utilizando uma reta a um sub-conjunto dos dados através de mínimos quadrados. Já para o lowess, é ajustada uma reta por mínimos quadrados ponderados de forma a dar maior peso aos pontos próximos. O subconjunto dos dados é também denominado janela, largura de banda ou parâmetro de suavização e controla flexibilidade da função de regressão – se a janela for igual ao total de pontos fica igual à regressão. Quanto maior o tamanho da janela, maior o alisamento e vice-versa. O grau do polinômio de cada regressão local em geral é baixo. Polinômio de primeiro grau é a regressão linear local; de segundo regressão quadrática. A largura da janela é indicado por uma fração dos dados que varia de 0 a 1. Exemplo: largura 0,1 significa que a janela tem largura que equivale a 10% do eixo horizontal; Polinômio de grau 0 é a média móvel; Se a janela for de 100% dos pontos = média. Existem varias funções que implementam o loess/lowess no R, por exemplo: lowess() e loess() . plot(lepto$week, lepto$totrain,cex=.3, pch=19, col=&quot;gray35&quot;,ylab=&quot;Lowess Chuva&quot;, xlab=&quot;Semana&quot;, main=&quot;Lowess - Bandwidth e Grau do Polinômio&quot;) lines(lowess(lepto$totrain ~ lepto$week, f = 0.75),col=4) lines(lowess(lepto$totrain ~ lepto$week, f = 0.015),col=2) legend(&quot;topright&quot;,c(&quot;f=0.75&quot;,&quot;f=0.015&quot;),col=c(4,2), title=&quot;&quot;,bty=&quot;n&quot;,lty=c(1,1)) # Loess – Span &amp; Grau plot(lepto$week, lepto$totrain,cex=.3, pch=19, col=&quot;gray35&quot;,ylab=&quot;Loess Chuva&quot;, xlab=&quot;Semana&quot;, main=&quot;Loess - Bandwidth e Grau do Polinômio&quot;) lines(predict(loess(totrain ~ week , data=lepto,span = 0.75, degree = 2)),col=2) lines(predict(loess(totrain ~ week , data=lepto,span = 0.75, degree = 1)),col=2,lwd=2,lty=2) lines(predict(loess(totrain ~ week , data=lepto,span = 0.05, degree = 2)),col=&quot;darkblue&quot;) lines(predict(loess(totrain ~ week , data=lepto,span = 0.05, degree = 1)),col=&quot;darkblue&quot;,lwd=2,lty=2) legend(&quot;topright&quot;,c(&quot;75% - 2º&quot;,&quot;75% - Linear (default)&quot;,&quot;5% - 2º&quot;,&quot;5% - Linear&quot;),lty=c(1,2,1,2), col=c(2,2,&quot;darkblue&quot;,&quot;darkblue&quot;), title=&quot;Janela e Grau&quot;,bty=&quot;n&quot;) # Loess – Span # lowess - f plot(lepto$week, lepto$totrain,cex=.3, pch=19, col=&quot;gray35&quot;,ylab=&quot;Loess Chuva&quot;, xlab=&quot;Semana&quot;, main=&quot;Loess - Bandwidth&quot;) lines(predict(loess(totrain ~ week , data=lepto,span = 0.15, degree = 2)),col=1, lwd=1.5) lines(predict(loess(totrain ~ week , data=lepto,span = 0.05, degree = 2)),col=2, lwd=1.5) lines(predict(loess(totrain ~ week , data=lepto,span = 0.10, degree = 2)),col=&quot;darkblue&quot;, lwd=1.5) legend(&quot;topright&quot;,c(&quot; 5% (21 sem.)&quot;,&quot;10% (41 sem.)&quot;,&quot;15% (61 sem.)&quot;),col=c(2,&quot;darkblue&quot;,1), lty=c(1,1,1),title=&quot;Janela&quot;,bty=&quot;n&quot;) # Loess – Span &amp; Borda plot(lepto$week, lepto$totrain,cex=.3, pch=19, col=&quot;gray35&quot;,ylab=&quot;Loess Chuva&quot;, xlab=&quot;Semana&quot;, main=&quot;Loess - Bandwidth&quot;) lines(predict(loess(totrain ~ week , data=lepto2,span = 0.15, degree = 2)),col=1, lwd=1.5) lines(predict(loess(totrain ~ week , data=lepto2,span = 0.05, degree = 2)),col=2, lwd=1.5) lines(predict(loess(totrain ~ week , data=lepto2,span = 0.10, degree = 2)),col=&quot;darkblue&quot;, lwd=1.5) legend(&quot;topright&quot;,c(&quot; 5% (21 sem.)&quot;,&quot;10% (41 sem.)&quot;,&quot;15% (61 sem.)&quot;),col=c(2,&quot;darkblue&quot;,1), lty=c(1,1,1),title=&quot;Janela&quot;,bty=&quot;n&quot;) # Loess – Outra forma, mas fica pouco suave scatter.smooth(lepto$week, lepto$tempmed, span = .05, degree = 2, family = &quot;gaussian&quot;, col= &quot;darkgray&quot;, cex=.5, ylab=&quot;Temperatura&quot;, xlab=&quot;Semana&quot;) scatter.smooth(lepto$week, lepto$totrain, span = .05, degree = 2, family = &quot;gaussian&quot;, col= &quot;darkgray&quot;, cex=.5, ylab=&quot;Chuva&quot;, xlab=&quot;Semana&quot;) scatter.smooth(lepto$week, lepto$cases, span = .05, degree = 2, family = &quot;gaussian&quot;, col= &quot;darkgray&quot;, cex=.5, ylab=&quot;Casos Leptospirose&quot;, xlab=&quot;Semana&quot;) 4.6.1 Utilização do loess/lowess Vantagens: simples, ótimo para análise exploratória. Menos sensível à borda. Desvantagens: pode ser sensível a valores extremos. 4.6.2 Comparando Kernel average smoother Local linear regression 4.7 Splines Splines são um conjunto de funções polinomiais que possuem muitos usos. No nosso caso, estaremos usando como uma função de suavização que juntam nós, de forma suave, mantendo propriedades matemáticas ótimas. A mais usada para este fim é a regressão penalizada chamada spline cúbica natural, com nós em valores distintos de \\(x_i\\). A escolha do parâmetro de suavização pode ser visual ou mais formal, quando a definição é dada pelo valor que minimiza o erro quadrático médio do ajuste, ou através de alguma forma de validação cruzada. Na regressão penalizada quer se encontrar a solução \\(\\hat{f}(x)\\) que minimize: \\[\\sum\\left[y_i - f(x_i) \\right]^2 + \\tau \\int f&#39;&#39;(x)]^2 dx\\] sendo \\(\\tau\\) o parâmetro de alisamento: Se \\(\\tau = 0 \\Rightarrow \\hat{f}(x)\\) é interpolação pontual (conecta os pontos) Se \\(\\tau = 1 \\Rightarrow \\hat{f}(x)\\) é interpolação linear simples Se \\(\\tau\\) é muito grande \\(\\Rightarrow \\hat{f}(x)\\) será tal que \\(f&#39;&#39;(x)\\) seja zero em todos os pontos, ou seja, mínimos quadrados. 4.7.1 Splines vs Loess/Lowess No spline se minimiza uma função-objetivo explícita, é mais elegante matematicamente que loess/lowess. O ajuste dos dois é muito semelhante para o mesmo número de graus de liberdade. Pode-se ajustar essas funções para diversos preditores. Existe uma forma mais complexa de spline mais complexo para splines (thin plate splines) que pode ser usada em modelos GAM. Ambas as funções podem também ser utilizadas no espaço. plot(lepto$week, lepto$cases, ylab=&quot;Casos Leptospirose&quot;, xlab=&quot;Semana&quot;, main = &quot;spline(.) -- 3 methods&quot;) lines(spline(lepto$week, lepto$cases, n=10 ), col = 2) lines(spline(lepto$week, lepto$cases, n=10, method = &quot;natural&quot;), col = 3) lines(spline(lepto$week, lepto$cases, n=10, method = &quot;periodic&quot;), col = 4) legend(&quot;topright&quot;,c(&quot;Defaut&quot;,&quot;Natural&quot;,&quot;Periódico&quot;),col=c(2,3,4), lty=c(1,1,1),title=&quot;Janela&quot;,bty=&quot;n&quot;) lepto.spl &lt;- smooth.spline(lepto$week, lepto$cases) lepto.spl.cv &lt;- smooth.spline(lepto$week, lepto$cases, cv =T) plot(lepto$week, lepto$cases, cex=.6, col=&quot;darkgray&quot;, ylab=&quot;Casos Leptospirose&quot;, xlab=&quot;Semana&quot;) lines(lepto.spl, col=1) lines(lepto.spl.cv, col=5) lines(smooth.spline(lepto$week, lepto$cases, df=10),col=3) lines(smooth.spline(lepto$week, lepto$cases, df=50),col=4) lines(smooth.spline(lepto$week, lepto$cases, df=2),col=2) legend(&quot;topright&quot;,c(&quot;Sem CV&quot;,&quot;Com CV&quot;,&quot;df=10&quot;, &quot;df=50&quot;, &quot;df=2&quot;),col=c(1,5,3,4,2), lty=c(1,1,1),title=&quot;Janela&quot;,bty=&quot;n&quot;) 4.8 Exercícios Propostos Seja \\(Z_t\\) (temperaturaNY.csv) uma série temporal referente às médias anuais das temperaturas na cidade de Nova York durante os anos de 1912 e 1971. Utilize e discuta alguns métodos de transformação e/ou suavização para descrever a série. Faça o mesmo com a série de dengue nas Filipinas (denguecases2.csv) Verifique as séries com atenção e veja como será a melhor maneira de ler e transformar o dado em ST. Lembre-se que no windows você talvez precise precisar rodar o código: options(url.method=&quot;libcurl&quot;) scan.win &lt;- function(x) {scan(url(x,method = &#39;libcurl&#39;))} 4.9 Bibliografia sugerida Faraway, J.J. Extending the Linear Model with R. Chapman &amp; Hall/CRC Texts in Statistical Science Series, 2006. Hastie, T.; Tibshirani, R. Generalised Additive Models. Chapman &amp; Hall, 1990. Wood, S.N. Generalized Additive Models: An Introduction with R. Chapman &amp; Hall/CRC Texts in Statistical Science Series, 2006. Venables, W.N. &amp; Ripley, B.D. Modern Applied Statistics with S. (MASS) Fourth Edition 2002 "],["modelagem-em-séries-temporais.html", " 5 Modelagem em Séries Temporais 5.1 O que são os Modelos Box &amp; Jenkins 5.2 Alguns processos estocásticos 5.3 Modelo ARIMA 5.4 Modelos ARIMA sazonais (SARIMA) - \\(ARIMA(p,d,q)(P,D,Q)_m\\) 5.5 Função de Autocorrelação Parcial (FACP) - (Partial Autocorrelation Function - PACF) 5.6 Processo de Modelagem 5.7 Processo de Identificação 5.8 Métricas para avaliar a qualidade do ajuste 5.9 Análise de Resíduos 5.10 Ajuste manual de modelo ARIMA 5.11 Ajuste automático de modelos ARIMA 5.12 Modelos de Previsão 5.13 Correlação Cruzada (Cross-Correlation) 5.14 GAM (Generalized Additive Models) 5.15 Exercícios Propostos 5.16 Bibliografia sugerida", " 5 Modelagem em Séries Temporais 5.1 O que são os Modelos Box &amp; Jenkins A abordagem de Box-Jenkins para a modelagem dos processos ARIMA foi descrita num livro publicado por George Box e Gwilym Jenkins em 1970. BOX, G.E.P. and G.M. JENKINS (1970) Time series analysis: Forecasting and control, San Francisco: Holden-Day. O método criado por Box &amp; Jenkins envolve a identificação de um processo ARIMA (modelos autorregressivos, integrados e de médias móveis) adequado, ajustando-o aos dados e, uma vez ajustados permite também utilizar esses modelos para a descrição e/ou previsão (forecast). Uma das características atraentes da abordagem Box-Jenkins é que os processos ARIMA são uma classe muito rica de modelos e geralmente é possível encontrar um modelo que forneça uma descrição adequada dos dados. Ajustam simultaneamente tendência, sazonalidade, ciclicidade e estrutura de dependência serial. A dependência serial é a influência que um dado evento no tempo recebe de pontos anteriores. O processo de modelagem B&amp;J é feito em um ciclo iterativo de 3 estágios (repetido até o ajuste do modelo mais adequado): Identificação - análise exploratória, baseada em gráficos dos dados brutos, autocorrelação, autocorrelação parcial, buscando identificar o tipo de modelo + adequado; Estimação - estimativa de termos e parâmetros e seleção do “melhor modelo”; Diagnóstico - critérios de ajuste, parcimônia. 5.2 Alguns processos estocásticos Processo aleatório (ruido branco): sequência de variáveis aleatórias (\\(a_t\\)) que são mutuamente independentes e identicamente distribuídas. Possui média e variância constantes e os coeficientes de autocorrelação são iguais a: \\[ \\rho_{h} = \\left\\{ \\begin{array}{rc} 1, &amp;\\mbox{se} \\quad h = 0, \\\\ 0 , &amp;\\mbox{se} \\quad h = \\pm 1, \\pm 2, ... \\end{array}\\right.\\] Passeio aleatório (random walk): Denomina-se passeio aleatório quando a variável aleatória \\(Z_t\\) é igual à \\(Z_{t-1}\\) mais um erro aleatório \\(\\rightarrow\\) \\(Z_t = Z_{t-1} + a_t\\). Quando \\(t = 0 \\rightarrow Z_1 = a_1\\), logo \\[Z_t = \\sum_{t}^{i=1} a_i\\] 5.3 Modelo ARIMA Na análise de séries temporais, um Modelo Autorregressivo Integrado de Médias Móveis (Autoregressive Integrated Moving Average ou ARIMA, na sigla em inglês) é uma generalização de um Modelo Autorregressivo de Médias Móveis (ARMA). Ambos os modelos são ajustados aos dados da série temporal para entender melhor os dados ou para prever pontos futuros na série. Modelos ARIMA são aplicados em alguns casos em que os dados mostram evidências de não estacionariedade, em que um passo inicial de diferenciação (correspondente à parte “integrada” do modelo) pode ser aplicado uma ou mais vezes para eliminar a não estacionariedade. Temos então: AR (Autorregressivo): avalia a relação entre os períodos (lags) através da autocorrelação, ou seja, indica que a variável de interesse é “regressada” em seus próprios valores defasados, isto é, anteriores. O objetivo de desse modelo e extrair essa influência. I (Integrated): Aplica a diferenciação, se necessária, ou seja, indica que os valores de dados foram substituídos com a diferença entre seus valores. E os valores anteriores e este processo diferenciador pode ter sido realizado mais de uma vez. MA (Moving Average): Indica que o erro de regressão é na verdade uma combinação linear dos termos de erro, cujos valores ocorreram contemporaneamente e em vários momentos no passado, ou seja, avalia os erros entre períodos e extrai estes erros (não tem relação com MA usados para suavização da ST). p é a ordem (número de defasagens) do modelo autorregressivo; d é o grau de diferenciação (o número de vezes em que os dados tiveram valores passados subtraídos); q é a ordem do modelo de média móvel. Exemplos: Parâmetro Descrição \\(p = 1\\) Significa que uma determinada observação pode ser explicada pela observação prévia + erro \\(p = 2\\) Significa que uma determinada observação pode ser explicada por duas observações prévias + erro \\(d = 0\\) Significa que não é aplicada a diferenciação \\(d = 1\\) Significa que será aplicada diferenciação de primeira ordem \\(d = 2\\) Significa que será aplicada diferenciação de segunda ordem \\(q = 1\\) Significa que uma determinada observação pode ser explicada pelo erro da observação prévia \\(q = 2\\) Significa que uma determinada observação pode ser explicada pelo erro de duas observações prévias ARIMA Descrição AR(1) ou ARIMA(1,0,0) Apenas elemento autorregressivo, de \\(1^{a}\\) ordem AR(2) ou ARIMA(2,0,0) Apenas elemento autorregressivo, de \\(2^{a}\\) ordem MA(1) ou ARIMA(0,0,1) Apenas Média Móvel ARMA(1,1) Autorregressão e média móvel de \\(1^{a}\\) ordem 5.3.1 Modelo Autorregressivo de ordem p - AR(p) ou ARIMA(p,0,0) Supondo que a variável aleatória \\(Z_t\\) é linearmente correlacionada com seus próprios valores defasados, este é um modelo autorregressivo geral de ordem p. \\[Z_t = c + \\phi_1 Z_{t-1} + \\phi_2 Z_{t-2} + ... + \\phi_p Z_{t-p} + a_t\\], sendo \\(t=1,2,...,p\\). O objetivo é estimar: a constante c - média do processo ou intercepto a ordem p do modelo - até onde vai a dependência os parâmetros \\(\\phi\\) de cada termo - peso de cada ponto passado na determinação do ponto t Para estimar os parâmetros \\(\\phi\\) de um AR, a estacionariedade de \\(1^a\\) e \\(2^a\\) ordens são fundamentais !!! Processo Modelo AR(1) \\[Z_t = c + \\phi_1 Z_{t-1} + a_t\\] AR(2) \\[Z_t = c + \\phi_1 Z_{t-1} + \\phi_2 Z_{t-2} + a_t\\] \\(\\dots\\) \\(\\dots\\) AR(p) \\[Z_t = c + \\phi_1 Z_{t-1} + \\phi_2 Z_{t-2} + ... + \\phi_p Z_{t-p} + a_t\\] 5.3.2 Condições de estacionariedade Uma série é estacionária quando suas propriedades não variam ao longo do tempo. Em um processo AR, a estacionariedade se reflete na estimação dos parâmetros: No caso AR(1) basta que \\(|\\phi_1| &lt; 1\\) para que o processo seja estacionário. No caso AR(2): \\(|\\phi_2|&lt; 1\\) \\(\\phi_2 + \\phi_1 &lt; 1\\) \\(\\phi_2 - \\phi_1 &lt; 1\\) 5.3.3 Modelo de Médias Móveis de ordem q - MA(q) ou ARIMA(0,0,q) Independente do processo autorregressivo, cada elemento da série pode também ser afetado pelo erro passado - processo “Médias Móveis”. Neste caso, o valor de \\(Z_t\\) depende de valores do componente aleatório em pontos anteriores (usa-se a denominação choque aleatório). \\[Z_t = C + a_t - \\theta_1 a_{t-1} - \\theta_2 a_{t-2} - ... - \\theta_p a_{t-q}\\] Por convenção os termos em a são escritos com sinais negativos. Cada observação é a soma de um componente aleatório \\(a_t\\) e uma combinação dos componentes aleatórios anteriores. O modelo pode ser escrito baseado nas defasagens (informações passadas) do ruído branco. Processo Modelo MA(1) \\[Z_t = c + a_t - \\theta_1 a_{t-1}\\] MA(2) \\[Z_t = c + a_t - \\theta_1 a_{t-1} - \\theta_2 a_{t-2}\\] \\(\\dots\\) \\(\\dots\\) MA(q) \\[Z_t = c + a_t - \\theta_1 a_{t-1} - \\theta_2 a_{t-2} - ... - \\theta_p a_{t-q} + a_t\\] 5.3.4 Condições de invertibilidade No modelo MA não há restrição sobre os \\(\\theta\\)’s para que o processo seja estacionário, mas é necessário garantir a invertibilidade. Existe uma dualidade entre processos médias móveis e autorregressivo, onde a equação de MA pode ser reescrita na forma AR (de ordem infinita). Se isso for possível, podemos dizer que o processo é invertível, ou seja, se puder utilizar um AR(p) para explicar um MA(q). Mas, para isso algumas condições devem ser satisfeitas: No caso MA(1) basta que \\(|\\theta| &lt; 1\\) para que o processo é invertível. No caso MA(2): \\(|\\theta_2|&lt; 1\\) \\(\\theta_2 + \\theta_1 &lt; 1\\) \\(\\theta_2 - \\theta_1 &lt; 1\\) 5.3.5 Modelo Autorregressivo de Médias Móveis de ordem p e q - ARMA(p,q) ou ARIMA(p,0,q) A importância de um modelo ARMA está no fato de poder descrever uma série estacionária por um modelo que envolve menos parâmetros que um MA puro ou um AR puro. \\[Z_t = c + \\phi_1 Z_{t-1} + \\phi_2 Z_{t-2} + ... + \\phi_p Z_{t-p} + a_t - \\theta_1 a_{t-1} - \\theta_2 a_{t-2} - ... - \\theta_p a_{t-q}\\] Cada observação é definida por combinação linear de observações anteriores e combinação de componentes aleatórios anteriores. Neste modelo misto, as duas condições - estacionariedade e invertibilidade - são necessárias. Processo Modelo ARMA(1,1) \\[Z_t = c + \\phi_1 Z_{t-1} + a_t - \\theta_1 a_{t-1}\\] ARMA(2,2) \\[Z_t = c + \\phi_1 Z_{t-1} + \\phi_2 Z_{t-2} + a_t - \\theta_1 a_{t-1} - \\theta_2 a_{t-2}\\] \\(\\dots\\) \\(\\dots\\) ARMA(p,q) \\[Z_t = c + \\phi_1 Z_{t-1} + \\phi_2 Z_{t-2} + ... + \\phi_p Z_{t-p} + a_t - \\theta_1 a_{t-1} - \\theta_2 a_{t-2} - ... - \\theta_p a_{t-q}\\] No caso ARMA(1,1), o processo será estacionário se \\(|\\phi_1| &lt; 1\\) e \\(|\\theta_1| &lt; 1\\), respectivamente. 5.3.6 Modelo Autorregressivo Integrado de Médias Móveis de ordem p, d e q - ARIMA(p,d,q) Neste modelo se utiliza o método de diferenças para obter a estacionariedade da série. Também chamado de operador de deslocamento (backshift). \\[W_t = \\bigtriangledown Z_t = (1-B)Z_t = Z_t - Z_{t-1}\\] O modelo então passa a ser: \\[W_t = \\phi_1 W_{t-1} + ... + \\phi_p W_{t-p} + a_t - \\theta_1 a_{t-1} - ... - \\theta_q a_{t-q}\\] \\[\\phi(B)W_t = \\theta(B)a_t\\] \\[\\phi(B)(1-B)^d Z_t = \\theta(B)a_t\\] Assim, se a série for estacionária, podemos representá-la por um modelo ARMA(p,q). A figura abaixo mostra a série não estacionária antes e após diferenciação - \\(d(1)\\) A figura abaixo mostra a ACF antes e após a diferenciação: 5.3.7 Resumo - Modelos ARIMA não sazonais Resumindo, os modelos ARIMA não sazonais são geralmente denotados como ARIMA(p,d,q), em que os parâmetros p,d,q são números inteiros não negativos. Robusto: Pode ser usado em praticamente qualquer tipo de ST Dados estáveis, com poucos outliers Requer dados estacionários: pode ser transformada usando diferenciação: remove tendências Subtrai a observação do período atual do período anterior A diferenciação pode ser feita 1x: diferenciação de primeira ordem Ou pode ser necessário uma segunda vez: diferenciação de segunda ordem (mais raro) 5.4 Modelos ARIMA sazonais (SARIMA) - \\(ARIMA(p,d,q)(P,D,Q)_m\\) Em epidemiologia é comum observar sazonalidadenos dados. Ou seja, considerando medidas mensais, pode-se esperar que a série dependa também dos termos \\(Z_{t-12}\\) e talvez \\(Z_{t-24}\\). \\[\\phi(B) \\Phi(B^s)\\bigtriangledown_{s}^{D} \\bigtriangledown^{d}Z_t = C + \\theta(B) \\Theta(B^s) a_t\\] \\(\\phi(B) \\Phi(B^s)\\bigtriangledown_{s}^{D} \\bigtriangledown^{d}Z_t\\) \\(C + \\theta(B) \\Theta(B^s) a_t\\) AR(p) backshift x AR(P) sazonal backshift sazonal x diferenciação sazonal x diferenciação x tendência \\(Z_t\\) Média do processo + MA(q) backshift x MA(Q) sazonal backshift sazonal x erro aleatório São geralmente denotados como \\(ARIMA(p,d,q)(P,D,Q)_{m}\\), em que: m se refere ao número de períodos em cada temporada; P, D e Q se referem aos termos de autorregressão, diferenciação e média móvel para a parte sazonal do modelo ARIMA. 5.5 Função de Autocorrelação Parcial (FACP) - (Partial Autocorrelation Function - PACF) A correlação medida diretamente em \\(t-1\\), \\(t-2\\) até \\(t-p\\) é a função de autocorrelação. Uma outra ferramenta utilizada no processo de identificação do modelo é a Função de Autocorrelação Parcial (FACP). Esta medida corresponde a correlação de \\(Z_t\\) e \\(Z_{t-h}\\) removendo o efeito dos pontos intermediários \\(Z_{t-1}, Z_{t-2}, \\dots , Z_{t-h+1}\\) e é denotada por \\(\\rho_{h}\\). Ou seja, PACF é a correlação da série temporal com um atraso de si mesmo porém com a dependência linear de todos os desfasamentos entre eles removidos. \\[\\rho_{kk} = Corr(X_t, X_{t-l}|X_{t-1}, X_{t-2},\\dots,X_{t-h+1})\\] Segue abaixo um exemplo um exemplo de ACF, com já vimos anteriormente, e do PACF. Observe que há duas linhas horizontais que representam os limites do teste de significância sendo que valores acima ou abaixo da linha são estatisticamente significantes. No \\(lag = 1\\), a ACF e a PACF são iguais. Na PACF somente existe correlação até o lag igual a ordem do modelo AR - ex: modelo de ordem 3 somente apresenta valores de PACF até o \\(3^o\\) lag. As formas gráficas do ACF e PACF servem para definir valores de p e q. Olhando para os correlogramas, podemos determinar que tipo de modelo selecionar e quais serão os valores de p, d e q. Modelo Padrão do ACF Padrão do PACF AR(p) Decaimento exponencial ou padrão de onda senoidal amortecida ou ambos Picos significantes através de primeiros lags MA(q) Picos significantes através de primeiros lags Decaimento exponencial ou padrão em forma de senoides ARMA(1,1) Decaimento exponencial a partir do lag 1 Decaimento exponencial a partir do lag 1 ARMA(p,q) Decaimento exponencial Decaimento exponencial 5.6 Processo de Modelagem Para a construção do modelo podemos seguir o seguinte roteiro no qual a escolha da estrutura do modelo é baseado nos próprios dados: Identifica-se um modelo com base na análise de autocorrelações, autocorrelações parciais e outros critérios; Estima-se os parâmetros do modelo identificado; Verifica-se se o modelo ajustado é adequado aos dados através de uma análise de resíduos. Caso o modelo não seja adequado o roteiro é repetido, voltando à fase de identificação do modelo. Como definir valores de p, d e q ? p: ordem da parte autoregressiva – PACF d: grau de diferenciação – Teste de Estacionariedade q: ordem da média móvel – ACF 5.7 Processo de Identificação Esse processo pode ser extremamente difícil, mesmo para experientes. E nem sempre o modelo mais sugestivo é o melhor. Existem vários critérios para identificação de um modelo, por isso, é possível identificar modelos diferentes dependendo do critério que foi escolhido para identificação. Testar todas as combinações possíveis dos parâmetros \\(p,d,q\\) do modelo ARIMA seria uma boa ideia, mas isso pode ser um pouco demorado se for feito de forma manual. 5.7.1 Simulando os dados das ST com estruturas ARIMA Simulando um processo AR(1) # Simulando 100 observações através de um processo AR(1) com média 30 ar.sim&lt;-arima.sim(model=list(ar=.9),n=200, mean = 30) # Construindo o gráfico de ST ts.plot(ar.sim) # ACF e PACF par(mfrow=c(1,2)) ar.acf &lt;- acf(ar.sim,type=&quot;correlation&quot;,plot=T) ar.pacf &lt;- acf(ar.sim,type=&quot;partial&quot;,plot=T) plot(ar.acf) plot(ar.pacf) Simulando um processo AR(2) # Simulando 100 observações através de um processo AR(2) com média 30 ar.sim &lt;- arima.sim(model=list(ar=c(.9,-.2)),n=200, mean = 30) # Construindo o gráfico de ST ts.plot(ar.sim) # ACF e PACF par(mfrow=c(1,2)) ar.acf &lt;- acf(ar.sim,type=&quot;correlation&quot;,plot=T) ar.pacf &lt;- acf(ar.sim,type=&quot;partial&quot;,plot=T) plot(ar.acf) plot(ar.pacf) Simulando um processo MA(2) # Simulando 200 observações através de um processo MA(2) com média 30 ma.sim &lt;- arima.sim(model=list(ma=c(-.7,.1)),n=200, mean = 30) # Construindo o gráfico de ST ts.plot(ma.sim) # ACF e PACF par(mfrow=c(1,2)) ma.acf &lt;- acf(ma.sim,type=&quot;correlation&quot;,plot=T) ma.pacf &lt;- acf(ma.sim,type=&quot;partial&quot;,plot=T) plot(ma.acf) plot(ma.pacf) Simulando um processo ARMA(2,2) # Simulando 200 observações através de um processo ARMA(2,2) com média 30 arma.sim &lt;-arima.sim(model=list(ar=c(.9,-.2),ma=c(-.7,.1)),n=200, mean = 30) # Construindo o gráfico de ST ts.plot(arma.sim) # ACF e PACF par(mfrow=c(1,2)) arma.acf &lt;- acf(arma.sim,type=&quot;correlation&quot;,plot=T) arma.pacf &lt;- acf(arma.sim,type=&quot;partial&quot;,plot=T) plot(arma.acf) plot(arma.pacf) 5.8 Métricas para avaliar a qualidade do ajuste Para determinar a ordem de um modelo ARIMA não sazonal, um critério útil é o critério de informação de Akaike (AIC). \\[AIC = - 2log(L) + 2(p+q+k+1)\\] em que L é verossimilhança dos dados, p é a ordem da parte autoregressiva e q é a ordem da parte de média móvel. O parâmetro k neste critério é definido como o número de parâmetros no modelo sendo ajustado aos dados. O AIC corrigido para modelos ARIMA (AICc) pode ser escrito como: \\[AICc = AIC + \\dfrac{2(p+q+k+1)(p+q+k+2)}{T-p-q-k-2}\\] O critério de informação bayesiano (BIC) pode ser escrito como: \\[BIC = AIC + (log(n) - 2) (p+q+k+1)\\] O objetivo é o minimizar os valores de AIC, AICc e BIC para um bom modelo. Quanto menor o valor de um destes critérios para uma gama de modelos investigados, melhor o modelo se adequará aos dados. O AICc pode ser usado apenas para comparar modelos ARIMA com as mesmas ordens de diferenciação. Para modelos ARIMA como ordens distintas de diferenciação, a raiz do erro quadrático médio pode ser usada para comparação de modelos. A abordagem do BIC penaliza mais intensamente os modelos por levar em consideração o \\(n\\). 5.9 Análise de Resíduos Se o modelo está correto, as nossas suposições iniciais feitas para os resíduos devem ser satisfeitas, isto é, \\(a_t \\sim N(0, \\sigma^{2}_{a})\\) e independentes. Assim, a análise de resíduos é feita da seguinte forma: Faz-se um gráfico da série \\(a_t\\) e observa-se a sua estacionariedade e se sua média é igual a zero (aproximadamente). Se a série \\(a_t\\) for estacionária, calcula-se suas funções de autocorrelação e autocorrelação parcial amostral; Se as funções em (2) indicarem que o processo gerador de \\(a_t\\) é um ruído branco, o modelo escolhido para \\(Y_t\\) poderá ser ser considerado adequado. Senão, podemos utilizar a análise dos resíduos para identificar outro modelo para a série. Lembrando que o processo \\(a_t\\) é um ruído branco se: \\(E(a_t) = 0\\), \\(a_t \\sim N(0, \\sigma^2_a)\\), \\(Cov(a_t, a_{t-h}) = 0\\), \\(\\forall h \\neq 0\\) (Não correlacionados) 5.10 Ajuste manual de modelo ARIMA library(forecast) ggtsdisplay(ldeaths) modelo0 &lt;- Arima(ldeaths,order = c(1,0,0),seasonal = c(1,1,1)) checkresiduals(modelo0) Ljung-Box test data: Residuals from ARIMA(1,0,0)(1,1,1)[12] Q* = 11, df = 11, p-value = 0.5 Model df: 3. Total lags used: 14 tsdiag(modelo0) plot(ldeaths) lines(fitted(modelo0),col=&quot;red&quot;) 5.11 Ajuste automático de modelos ARIMA O pacote forecast de autoria Rob Hyndman e colaboradores possui diversas funções para visualização, análise e predições de séries temporais. Entre elas existe uma função que faz ajustes automáticos para modelos ARIMA. Essa função se chama auto.arima(). Testa diferentes combinações de \\(p\\), \\(d\\) e \\(q\\) buscando o melhor ajuste Extremamente flexível Mesmo intuindo um modelo, você pode usá-la para confirmar sua parametrização Alguns parâmetros importantes da função auto.arima(): stationary - Se TRUE, restringe sua busca por modelos estacionários. seasonal - Se FALSE, restringe sua busca por modelos não sazonais. stepwise - Se TRUE, utilizará o métodos de stepwise de seleção (mais rápido). Caso contrário, irá buscar todas as combinações. O método de seleção non-stepwise pode ser muito lento, especialmente para os modelos sazonais. trace - Se TRUE, a lista de modelos ARIMA considerados será reportada. approximation - Se TRUE, o processo de estimação é feito através das somas dos quadrados condicionais e os critérios de informação utilizados para a seleção de modelos são aproximados. O modelo final ainda é calculado usando estimativa de máxima verossimilhança. A aproximação deve ser usada para séries temporais longas ou um período sazonal elevado para evitar tempos de computação excessivos. # Utilizando trace = T, será possível verificar todo o processo de criação e teste dos modelos modelo1 &lt;- auto.arima(ldeaths, trace = F, allowdrift=F) # Neste modelo, será feito uma busca maior para uma solução &quot;mais otimizada&quot; modelo2 &lt;- auto.arima(ldeaths, trace = F, stepwise = F, approximation = F,parallel = TRUE) Vamos verificar cada um dos modelos obtidos: modelo1 ## Series: ldeaths ## ARIMA(0,0,1)(2,1,0)[12] ## ## Coefficients: ## ma1 sar1 sar2 ## 0.427 -0.860 -0.361 ## s.e. 0.138 0.143 0.145 ## ## sigma^2 = 70422: log likelihood = -423.3 ## AIC=854.6 AICc=855.3 BIC=863 tsdiag(modelo1) modelo2 ## Series: ldeaths ## ARIMA(2,0,0)(0,1,2)[12] with drift ## ## Coefficients: ## ar1 ar2 sma1 sma2 drift ## 0.255 -0.339 -1.163 0.357 -5.334 ## s.e. 0.134 0.140 0.362 0.205 0.905 ## ## sigma^2 = 49449: log likelihood = -416.4 ## AIC=844.8 AICc=846.4 BIC=857.4 tsdiag(modelo2) Comparando os ajustes dos modelos: plot(ldeaths,lwd=2) lines(fitted(modelo0),col=&quot;red&quot;,lty=2,lwd=2) lines(fitted(modelo1),col=&quot;blue&quot;,lty=2,lwd=2) lines(fitted(modelo2),col=&quot;orange&quot;,lty=2,lwd=2) 5.12 Modelos de Previsão Uma vez ajustado um modelo ARIMA a uma série temporal é possível fazer uma predição de \\(K\\) passos a frente, permitindo obter a previsão da série no instante \\(t + k\\), denotada por \\(\\widehat {Z}_{t+k}\\). Podemos obter também os limites de confiança. Os modelos de previsão são uma área importante e um campo muito ativo de desenvolvimento de métodos estatísticos e computacionais. Aqui vamos somente apresentar um exemplo utilizando um dos modelos já ajustados para a serie ldeaths. Uma maneira simples de obter o valor predito é usando a função predict(). predict(modelo1, n.ahead = 12) O pacote forecast possui funções para facilitar a predição e visualização. Veja na função abaixo que o modelo já retorna não só o valor predito mas também os limites superior e inferior para os níveis de confiança 80 e 95. forecast(modelo1,h=12) Ele também facilita a confecção de gráficos. plot(pred) # a função autoplot produz no formato ggplot Para saber mais sobre séries temporais e previsões recomendamos o excelente livro online Forecasting: Principles and Practice Rob J Hyndman and George Athanasopoulos 5.13 Correlação Cruzada (Cross-Correlation) Vamos considerar a situação em que temos duas ou mais séries temporais e queremos explorar as relações entre elas. A ideia seria modelar a série \\(Z_{t}^{1}\\) usando os pontos anteriores de \\(Z_{t}^{2}\\). A correlação cruzada descreve o grau de correlação entre duas séries. A correlação cruzada é usada para determinar quando uma mudança em um série pode potencializar a causa da mudança na outra série. \\[Z_{t} = v(B) X_t + N_t\\] para a qual: \\(v(B)\\) é a função de transferência (filtro) \\(Z_t,X_t\\) são séries estacionarias \\(N_t\\) é um ruído independente de \\(X_t\\) Vamos analisar: O atraso de uma série em relação a outra O feedback Correlação dos resíduos, removida a estrutura temporal A título de demonstração da função de correlação cruzada (Cross-correlation Function), vamos usar como exemplo dados de temperatura da área central do Reino Unido, obtidos do UK Met Office e vamos correlacionar com a série ldeaths . uktemp &lt;- read.table(&#39;https://raw.githubusercontent.com/ogcruz/dados_eco_2023/main/dados/UKtemp.tsv&#39;) uktemp &lt;- ts(as.vector(t(uktemp[,2:13])),start = c(1970,1),frequency = 12) uktemp.ts &lt;- window(uktemp,start=c(1974,1),end=c(1979,12)) fcc &lt;- ccf(uktemp.ts,ldeaths) cbind(fcc$lag,fcc$acf) -1.2500 -0.1859 -1.1667 -0.4816 -1.0833 -0.6586 -1.0000 -0.6518 -0.9167 -0.4912 -0.8333 -0.1953 -0.7500 0.1562 -0.6667 0.5214 -0.5833 0.7748 -0.5000 0.8002 -0.4167 0.5991 -0.3333 0.2354 -0.2500 -0.1907 -0.1667 -0.5479 -0.0833 -0.7891 0.0000 -0.8320 0.0833 -0.6264 0.1667 -0.2453 0.2500 0.1974 0.3333 0.5818 0.4167 0.8115 0.5000 0.8207 0.5833 0.6039 0.6667 0.2426 0.7500 -0.1756 0.8333 -0.5441 0.9167 -0.7285 1.0000 -0.7110 1.0833 -0.5332 1.1667 -0.2181 1.2500 0.1562 max(abs(fcc$acf)) ## [1] 0.832 5.14 GAM (Generalized Additive Models) Um modelo aditivo generalizado (Hastie and Tibishirani, 1990) é um modelo linear generalizado com um preditor linear envolvendo a soma de funções suavizadas das covariáveis + os efeitos fixos das mesmas. \\[\\eta = \\sum X \\beta + f_1(x_{1i}) + f_2(x_{2i}) + \\ldots\\] Pode ser considerado uma extensão do GLM, onde o preditor linear \\(\\eta = E(Y_i)\\) não é limitado para a regressão linear, sendo \\(Y_i \\sim\\) alguma distribuição da família exponencial. \\(Y_i\\) é a variável resposta (desfecho) \\(X\\) é o vetor das variáveis explicativas (exposição) \\(\\beta\\) representa o vetor de parâmetros a serem estimados pelo modelo. O modelo inclui qualquer função das covariáveis independentes (\\(x_i\\)) \\(f(x)\\) pode ser uma função não-paramétrica, cuja forma não é especificada a priori. Mas pode ser estimada através de curvas de alisamento (ex: kernel, loess, splines, etc.) A curva alisada permite descrever a forma e revelar possíveis não linearidades nas relações estudadas, uma vez que não apresenta a estrutura rígida de uma função paramétrica, como nos GLM’s. Quando usar? Quando o efeito da covariável muda dependendo do seu valor. 5.14.1 Tipos de splines Cubic regression spline – polinômios de grau 3 ajustados aos intervalos dos dados definidos pelos nós distribuı́dos regularmente em todo o escopo dos dados. A base é a soma de nós mais 2, que correspondem ao início e fim da curva. Os polinômios são unidos de forma a existir as 1 e 2 derivadas. Cyclic cubic regression spline – força a função a ter o mesmo valor e mesmas derivadas no início e final. Interessante para séries temporais com sazonalidade. P-splines – Baseia-se na B-spline, na qual a função vai obrigatoriamente a zero em m+3 nós adjacentes. A p-spline se baseia nessa, porém aplica uma penalidade diferencial entre β’s adjacentes para controlar a “wiggliness”. Thin plate – menor erro quadrático, menor número de parâmetros considerado o estimador ótimo. Facilmente adaptável para duas/três dimensões. Tensor Product – Semelhante ao Thin Plate, mas permite escalas bs= Descrição Vantagens Desvantagens “tp” Thin Plate Suaviza múltiplas covariáveis. Não se altera com a rotação. Estimador ótimo. Computacionalmente custoso. Não é invariante quanto à escala. “cr” Cubic Regression Computacionalmente barato. Parâmetros interpretáveis diretamente. Só uma covariável. Baseado na escolha dos nós. Estimador não ótimo. “cc” Cyclic CRS Começo e fim = ’s Idem. “ps” P-splines Qualquer combinação de base e ordem. Nós em intervalos iguais. Penalidades difíceis de interpretar. Estimador não ótimo. 5.14.2 Por que não usar ? Os modelos estatísticos visam explicar os dados observados, não simplesmente reproduzi-lo \\(\\rightarrow\\) overfitting Modelos paramétricos em geral são melhores para estimar erros padrão ou intervalos de confiança Modelos paramétricos são mais eficientes, se corretamente especificados (menor número de observações) 5.14.3 GAM em Séries Temporais A idéia principal é modelar o efeito de covariáveis em alguns eventos de saúde ao longo do tempo Razões: Permitir a inclusão da dependência do tempo Relação não-linear Tendência e sazonalidade podem ser facilmente incorporadas Considerando a variável resposta uma contagem, as escolhas para as distribuições são: Poisson: \\(\\lambda\\) \\(=\\) valores esperados e \\(=\\) variância \\(\\rightarrow\\) superdispersão Quasipoisson: não é uma distribuição, mas uma maneira de relaxar suposição do modelo anterior e levar em consideração a superdispersão. (Não estima o AIC automaticamente). Negative Binomial: tem uma média \\(\\mu\\), um parâmetro de escala \\(\\theta\\) e a variância como a função \\(V(\\mu)=\\mu+\\mu^2/\\theta\\) Modelos com inflação zero: modelos de mistura que combinam uma massa pontual a zero com uma distribuição de contagem como Poisson, geométrica ou binomial negativa 5.14.4 Exemplo de modelo GAM aplicado a ST ldeaths Transforma o exemplo ldeaths de ts em data.frame preservando as estruturas temporais (tempo e mês) ldeaths.df &lt;- data.frame(tempo=time(ldeaths),obitos=ldeaths,mes=cycle(ldeaths)) ldeaths.df tempo obitos mes 1974 3035 1 1974 2552 2 1974 2704 3 1974 2554 4 1974 2014 5 1974 1655 6 Modelando como GAM: mod0 &lt;- gam ( obitos ~ s(tempo) + s(mes),data=ldeaths.df,family = poisson) mod0 Sumário do modelo A. parametric coefficients Estimate Std. Error t-value p-value (Intercept) 7.5908 0.0027 2814.3329 &lt; 0.0001 B. smooth terms edf Ref.df F-value p-value s(tempo) 8.8095 8.9889 543.0363 &lt; 0.0001 s(mes) 8.8911 8.9965 9819.3608 &lt; 0.0001 R-quadrado(ajustado) = 0.86 Deviance Explicada = 0.91 AIC = 1772.74 Gráfico dos termos suaves: Análise de resíduos Vamos verificar se sobraram estruturas nos resíduos usando a ACF dos resíduos e o teste de Ljung-Box: acf(residuals(mod0)) Box.test(residuals(mod0),type=&#39;Ljung-Box&#39;) Box-Ljung test: residuals(mod0) Test statistic df P value 0.09495 1 0.758 Como podemos observar a variável tempo que captura a tendência ficou completamente linear. Vamos agora usar a variavel tempo de forma linear no modelo. mod1 &lt;- gam ( obitos ~ tempo + s(mes),data=ldeaths.df,family=poisson) mod1 A. parametric coefficients Estimate Std. Error t-value p-value (Intercept) 64.7222 3.0104 21.4994 &lt; 0.0001 tempo -0.0289 0.0015 -18.9769 &lt; 0.0001 B. smooth terms edf Ref.df F-value p-value s(mes) 8.8911 8.9965 10136.2804 &lt; 0.0001 R-quadrado(ajustado) = 0.86 Deviance Explicada =0.9 AIC = 1946.68 acf(residuals(mod1)) Box.test(residuals(mod1),type=&#39;Ljung-Box&#39;) Box-Ljung test: residuals(mod1) Test statistic df P value 1.869 1 0.1716 Exercícios ajuste o gam supondo uma distribuição normal; ajuste com uma distribuição quasipoisson; transforme a serie de óbitos com log, sqrt ou Box.Cox ; qual a diferença de usar tempo e s(tempo) ? 5.14.5 Exemplo GAM: Casos de Leptospirose vs Clima Vamos utilizar uma a série temporal dos casos de leptospirose com as variáveis de exposição total de chuva e temperatura. \\[\\text{Lepto}(t) = \\text{rain}(t-?) + \\text{humidity}(t-?) + AR(t,t-1) + trend + seasonality + \\varepsilon\\] Tendência e Sazonalidade \\(\\to\\) função suavizadora Covariáveis \\(\\to\\) lag no tempo É possível incluir a variabilidade populacional em risco (offset) lepto &lt;- read.csv2(&quot;https://raw.githubusercontent.com/ogcruz/dados_eco_2023/main/dados/leptoruido.csv&quot;, header=T) dia &lt;- read.table(&quot;https://raw.githubusercontent.com/ogcruz/dados_eco_2023/main/dados/climadia.dat&quot;, header=T) library(lattice) library(car) library(sm) library(survival) library(mgcv) #XY condicionado xyplot(cases + totrain + tempmed ~ week, outer=TRUE, layout=c(1, 3), pch=1, type=&quot;l&quot;,ylab=&quot;&quot;, scales=list(x=list(relation=&#39;same&#39;), y=list(relation=&#39;free&#39;)), data=lepto) Modelando a Chuva como resposta e colocando a função suavizadora para capturar o efeito nas semanas epidemológicas xyplot(totrain ~ week, outer=TRUE, layout=c(1, 1), pch=1, type=&quot;l&quot;,ylab=&quot;Chuva&quot;, scales=list(x=list(relation=&#39;same&#39;), y=list(relation=&#39;free&#39;)), data=lepto) # Ajustando a chuva como resposta par(mfrow=c(2,2)) rain.tp.auto &lt;- gam(totrain ~ s(week,bs=&quot;tp&quot;), data=lepto ) plot(rain.tp.auto,main=paste(&#39;K = &#39;,rain.tp.auto$smooth[[1]]$bs.dim)) rain.tp.k1 &lt;- gam(totrain ~ s(week,bs=&quot;tp&quot;,k=1), data=lepto ) plot(rain.tp.k1,main = paste(&#39;K =&#39;,rain.tp.k1$smooth[[1]]$bs.dim)) rain.tp.k30 &lt;- gam(totrain ~ s(week,bs=&quot;tp&quot;,k=30), data=lepto ) plot(rain.tp.k30,main=paste(&#39;K =&#39;,rain.tp.k30$smooth[[1]]$bs.dim)) rain.ad &lt;- gam(totrain ~ s(week,bs=&quot;ad&quot;), data=lepto ) plot(rain.ad,main=paste(&#39;K(adaptável) =&#39;, rain.ad$smooth[[1]]$bs.dim)) # ThinPlate &lt;- predict(rain.tp) # P.spline &lt;- predict(rain.ps) # CubicRegression &lt;- predict(rain.cr) # CyclicCR &lt;- predict(rain.cc) # Adaptative &lt;- predict(rain.ad) Ajustando a série dos casos de leptospirose via distribuição Poisson: library(quantmod) # para utilizar a funcao Lag chuvl0 &lt;- gam(cases ~ s(totrain) ,family=poisson,data=subset(lepto, week&gt;5)) chuvl1 &lt;- gam(cases ~ s(Lag(totrain,1)),family=poisson,data=subset(lepto, week&gt;5)) chuvl2 &lt;- gam(cases ~ s(Lag(totrain,2)),family=poisson,data=subset(lepto, week&gt;5)) chuvl3 &lt;- gam(cases ~ s(Lag(totrain,3)),family=poisson,data=subset(lepto, week&gt;5)) chuvl4 &lt;- gam(cases ~ s(Lag(totrain,4)),family=poisson,data=subset(lepto, week&gt;5)) chuvl5 &lt;- gam(cases ~ s(Lag(totrain,5)),family=poisson,data=subset(lepto, week&gt;5)) AIC(chuvl0,chuvl1,chuvl2,chuvl3,chuvl4,chuvl5) Ajustando a série dos casos de leptospirose via distribuição Binomial Negativa chuvnb0 &lt;- gam(cases ~ s(Lag(totrain,0)),data=lepto, family=nb(), subset=week&gt;5) chuvnb1 &lt;- gam(cases ~ s(Lag(totrain,1)),data=lepto, family=nb(), subset=week&gt;5) chuvnb2 &lt;- gam(cases ~ s(Lag(totrain,2)),data=lepto, family=nb(), subset=week&gt;5) chuvnb3 &lt;- gam(cases ~ s(Lag(totrain,3)),data=lepto, family=nb(), subset=week&gt;5) chuvnb4 &lt;- gam(cases ~ s(Lag(totrain,4)),data=lepto, family=nb(), subset=week&gt;5) chuvnb5 &lt;- gam(cases ~ s(Lag(totrain,5)),data=lepto, family=nb(), subset=week&gt;5) chuvnb5$family$getTheta() [1] 0.4864 AIC(chuvnb0,chuvnb1,chuvnb2,chuvnb3,chuvnb4,chuvnb5) Comparando os AIC’s de ambos os modelos entre os lags plot(0:5,AIC(chuvl0,chuvl1,chuvl2,chuvl3,chuvl4,chuvl5)[,2], type=&quot;l&quot;,ylim=c(2090,2850), ylab=&quot;AIC&#39;s&quot;, xlab = &quot;Lags&quot;) lines(0:5,AIC(chuvnb0,chuvnb1,chuvnb2,chuvnb3,chuvnb4,chuvnb5)[,2], type=&quot;l&quot;,col=2) legend(&quot;right&quot;,c(&quot;Poisson&quot;,&quot;Bin. Neg.&quot;),col=1:2,fill=1:2) Verificando a distribuição da chuva entre todos os lags boxplot(lepto$totrain) par(mfrow=c(3,2),mai=c(0.05,0.5412 ,0.5412,0.05),mgp=c(2,1,0)) plot(chuvnb0,ylab=&quot;Lag=0&quot;) abline(h=0,lty=2,col=2) plot(chuvnb1,ylab=&quot;Lag=1&quot;) abline(h=0,lty=2,col=2) plot(chuvnb2,ylab=&quot;Lag=2&quot;) abline(h=0,lty=2,col=2) plot(chuvnb3,ylab=&quot;Lag=3&quot;) abline(h=0,lty=2,col=2) plot(chuvnb4,ylab=&quot;Lag=4&quot;) abline(h=0,lty=2,col=2) plot(chuvnb5,ylab=&quot;Lag=5&quot;) abline(h=0,lty=2,col=2) E agora? incluir humidade, temperatura, etc etc… 5.15 Exercícios Propostos Utilize o banco de dados do R chamado Seatbelts, que é baseado em uma série histórica que mostra os totais mensais dos condutores de automóveis na Grã-Bretanha mortos ou gravemente feridos, de 1969 a dezembro de 1984. O uso obrigatório dos cintos de segurança foi introduzido em 31 de janeiro de 1983. Variáveis Descrição DriversKilled Motoristas de carro mortos drivers Motoristas front Passageiros do banco da frente mortos ou gravemente feridos rear Passageiros do banco traseiro mortos ou gravemente feridos kms Distância percorrida PetroPrice Preço da gasolina VanKilled Número de condutores de van (veículo leve de mercadorias) law A lei estava em vigor naquele mês (1/0) Pede-se: Ajuste um modelo ARIMA com a variável de desfecho DriversKilled; Ajuste um modelo GAM utilizando a variável de desfecho DriversKilled e verifique quais variáveis estão mais associadas ao modelo. Dados climatológicos de temperaturas máxima, mínima e pluviosidade para as 10 áreas de planejamento (AP) da cidade do Rio de Janeiro de 2001 a 2016 estão disponíveis nos arquivos abaixo. Use a função read.csv2 para ler o dado. temperatura máxima temperatura mínima pluviosidade Escolha uma delas, descreva a série e tente modelar usando os modelos B&amp;J ou GAM. 5.15.1 Exercícios Resolvidos Acidentes de transito UK Dados Climaticos 5.16 Bibliografia sugerida Asteriou, D.; Hall, S.G. Applied econometrics. Macmillan International Higher Education, 2015. Brockwell, Peter J.; DAVIS, Richard A.; CALDER, Matthew V. Introduction to time series and forecasting. New York: springer, 2002. Harvey, A. C. and Durbin, J. The effects of seat belt legislation on British road casualties: A case study in structural time series modelling. Journal of the Royal Statistical Society series A, 149, 187–227. 1986. Hastie, T.; Tibshirani, R. Generalised Additive Models. Chapman &amp; Hall, 1990. Hyndman, R. J. and Khandakar, Y. Automatic time series forecasting: The forecast package for R, Journal of Statistical Software, 26(3). 2008. Hyndman, R. J. and Athanasopoulos, G. Forecasting: Principles and Practice 3rd edition. Webster, C.E.J. Time series properties of econometric models and their implied ARIMA representation. 1983. Wood, S.N. Generalized Additive Models: An Introduction with R. Chapman &amp; Hall/CRC Texts in Statistical Science Series, 2006. "],["introdução-à-análise-estatística-espacial.html", " 6 Introdução à Análise Estatística Espacial 6.1 O que é Análise Estatística Espacial? 6.2 Origem da Estatística Espacial 6.3 Quando usar métodos de análise espacial? 6.4 Dependência espacial 6.5 Principais aplicações em Saúde 6.6 Tipologia dos dados espaciais 6.7 Dados de Processos Pontuais 6.8 Geoestatı́stica 6.9 Dados de Área 6.10 Geoprocessamento 6.11 Tecnologias de Geoprocessamento 6.12 Sistema de Informações Geográficas 6.13 Análise Espacial no R 6.14 Alguns Conceitos 6.15 Aplicações 6.16 Material on line sobre Estatística Espacial 6.17 Onde encontrar dados espaciais ? 6.18 Exemplo I: Malhas no R com geobr 6.19 Exemplo II: Geocodificação 6.20 Bibliografia sugerida", " 6 Introdução à Análise Estatística Espacial 6.1 O que é Análise Estatística Espacial? São métodos estatísticos que levam em consideração a localização espacial do fenômeno estudado; Define-se “Análise estatística espacial quando os dados são espacialmente localizados e se considera explicitamente a possível importância de seu arranjo espacial na análise ou interpretação dos resultados” (Bailey &amp; Gatrell, 1995). Principal característica: a localização geográfica é utilizada explicitamente na análise. Neste curso serão abordadas basicamente as técnicas estatísticas de análise espacial. Diversas operações realizadas em um SIG são também chamadas de análise espacial, mas não são objeto deste curso. 6.2 Origem da Estatística Espacial Dr. John Snow (1813-1858) Considerado pai da Epidemiologia Moderna: Mapeamento dos casos de coléra (\\(\\bullet\\)) e as bombas de água (X). (Londres, 1854) 6.3 Quando usar métodos de análise espacial? A primeira pergunta a ser feita é: A distribuição dos dados apresenta um padrão aleatório ou apresenta algum outro tipo de padrão? (Tendências, clusters, Regular) Heterogeneidade espacial: A magnitude e a direção do fenômeno de interesse varia no espaço. Violação da suposição de independência: Os atributos da observação i influenciam nos atributos na observação j. 6.4 Dependência espacial A visão da Geografia Conhecida como 1\\(^a\\) Lei da Geografia! “Todas as coisas são parecidas, mas coisas mais próximas se parecem mais que coisas mais distantes.” (Waldo Tobler, 1979) wikipedia Quais as possíveis implicações de não considerar a localização espacial na modelagem? Dependência Estatística Espacial “Independência é um pressuposto muito conveniente que faz grande parte da teoria estatı́stica matemática tratável. Entretanto, modelos que envolvem dependência estatı́stica são freqüentemente mais realı́sticos. […] dados espaciais, onde a dependência está presente em todas as direções e fica mais fraca a medida em que aumenta a dispersão na localização dos dados.” (Noel Cressie, Statistics for spatial data. 1991)   Generalizando, a maior parte dos fenômenos, sejam estes socioeconômicos ou da área da saúde, apresentam entre si uma relação - semelhança ou inibição - que depende da distância. 6.5 Principais aplicações em Saúde Mapeamento de doenças: O objetivo geral é avaliar a variação geográfica na ocorrência das doenças visando identificar diferenciais de risco, orientar a alocação de recursos, levantar hipóteses etiológicas. Clusters: O objetivo da detecção de cluster espacial é estabelecer a significância de um sobrerrisco em um determinado espaço ou tempo e espaço. Este cluster pode ser causado por diferentes fatores: agentes infecciosos, contaminação ambiental localizada, efeitos colaterais de tratamentos, etc. Estudos ecológicos: Essencialmente modelos de regressão, onde se busca explicar a variação na incidência da doença através de outras variáveis. Avaliação e monitoramento ambiental: Estimativa e monitoramento da distribuição espacial de fatores ambientais relevantes para a saúde. Por exemplo, poluentes químicos, insolação (Raios UV), vegetação, clima, etc. A quantidade e disponibilidade de dados nesta área atualmente é bem grande, com ênfase particular para as imagens de satélite, com resolução e acessibilidade cada vez maiores. 6.6 Tipologia dos dados espaciais Os diferentes tipos de dados espaciais são tradicionalmente classificados de acordo com uma tipologia. Esta caracterização diz respeito a natureza estocástica da observação. Cressie divide a estatística espacial em 3 grandes áreas: Dados de processos pontuais; Dados de geoestatística; Dados de área; Existem métodos estatísticos diferentes para descrever ou analisar estes tipos de dados. Tipo de Dado Exemplos Técnicas Análises de padrões pontuais Eventos localizados Ocorrência de doenças - Determinação de Padrões - Cluster Geoestatística Amostras Variáveis ambientais - Interpolação de superfícies Análises de dados de Área Atributos de um polígono Dados censitários - Correlação espacial* modelos de Regressão OBSERVAÇÃO! Existem ainda outros tipos de dados como por exemplo imagens de satélites (RASTER), redes e fluxos que não serão vistos nesse curso. Eventualmente misturas de diferentes tipos de dados estão presentes em um mesmo estudo. Em algumas situações pode-se converter o dado de um tipo para outro (troca de suporte). 6.7 Dados de Processos Pontuais O principal interesse está no conjunto de coordenadas geográficas representando as localizações exatas de eventos. Exemplos: Localização de crimes, localização da residência dos casos de dengue, localização de espécies vegetais, etc. Neste caso, o dado aleatório de interesse é a localização espacial do evento. O objetivo é estudar a distribuição espacial dos pontos testando a hipótese sobre o padrão observado: existe aglomeração/cluster de casos ou eles estão dispostos aleatoriamente? - Localização da ocorrência de todos os crimes violentos no ano de 1998 registrados pela PM de Belo Horizonte: Existe aglomeração de casos em alguma área da cidade? - Localização da ocorrência de casos de Dengue em Belo Horizonte: (Jean Barrado) - Detection and modelling of case clusters for urban leptospirosis: Fonte: Tassinari et al. (2008). - Spatial distribution of leptospirosis in the city of Rio de Janeiro, Brazil, 1996-1999: Fonte: Tassinari et al. (2004) 6.8 Geoestatı́stica São dados que compreendem um conjunto de localizações (em geral latitudes e longitudes), mas agregados a eles uma medida contínua, como por exemplo o volume de chuva. Neste caso estaremos interessados em entender o padrão nos valores amostrados nestas localizações e também modelar e estimar valores em localizações não medidas. Bastante utilizada em ciências ambientais (chuva, temperatura, umidade, poluentes no ar, etc.) Exemplo: Mapa sobre o teor de argila no solo. Krigagem da chuva no Rio de Janeiro Fonte: Teixeira e Cruz (2011) 6.9 Dados de Área Este tipo de dado pode ser visualizado em mapas onde o espaço é particionado em áreas e cada área é colorida de acordo com alguma variável. As áreas podem ser: Irregulares: divisões de caráter polı́tico ou administrativo. Por exemplo: municípios, setores censitários, etc. Regulares: medidas em grade regular. Por exemplo, imagens de satélite. Para cada uma destas áreas temos informações agregadas de um determinado fenômeno (em geral somas ou médias). Bastante utilizados em epidemiologia, economia e demografia, etc. Objetivo: Identificar áreas de risco. -Taxas de câncer de pulmão na população branca masculina nos Estados Unidos, por condados no ano de 1998: 6.10 Geoprocessamento Geoprocessamento é um conjunto de técnicas , softwares e hardware capazes de coletar , tratar , analisar e disseminar informações georreferenciadas permitindo o desenvolvimento de novos dados, analises e aplicações; Utiliza programas de computador que permitem o uso integrado de informações cartográficas (mapas, cartas topográficas , imagens de satélites etc…) e informações tabulares (dados alfanumericos) e possibilita se associar coordenadas desses dados a mapas ; Duas das principais ferramentas de geoprocessamento são os Sistema de Informações Geográficas (SIG) e o Sensoriamento Remoto (SR) Este curso não vai abordar a tecnologias de Sensoriamento Remoto ainda que exista a possibilidade, cada vez maior de se integrar todas as técnicas usando o R. 6.11 Tecnologias de Geoprocessamento 6.12 Sistema de Informações Geográficas Um Sistema de Informação Geográfica (SIG ou GIS - Geographic Information System) é um sistema de hardware, software, informação espacial e procedimentos computacionais que permite e facilita a análise, gestão ou representação do espaço e dos fenômenos que nele ocorrem. 6.12.1 QGIS https://qgis.org/pt_BR/site/ 6.13 Análise Espacial no R CRAN Task View: Analysis of Spatial Data 6.14 Alguns Conceitos 6.14.1 Autocorrelação espacial É uma medida estatística que quantifica e testa o grau de dependência entre observações no espaço. Caso haja evidência de estrutura espacial, o postulado de independência das amostras, é inválido. Nestes casos os modelos de regressão devem levar em conta explicitamente o espaço em suas formulações. Dados espaciais são considerados uma única realização de um processo estocástico. Diferente da amostragem tradicional, em que cada observação traz uma informação independente, todas as observações são utilizadas de forma conjunta para descrever o padrão do fenômeno estudado. 6.14.2 Estacionariedade O processo é estacionário se a média é constante e a covariância entre dois pontos quaisquer é função apenas da distancia entre as duas localizações. 6.14.3 Isotropia O processo é isotrópico se além de estacionário, a covariância depende somente da distância entre as localizações. 6.15 Aplicações 6.15.1 Dengue em Dourados/MS 6.15.2 Vigilância dos vetores da Dengue usando modelagem espaço-temporal Bayesiana 6.15.3 Modelo Espaço-Temporal para a Análise da Morbimortalidade por Influenza Fonte de dados: Sistema Nacional de Informação de Agravos de Notificação (SINAN) do Ministério da Saúde, Brasil. Desfecho: Casos confirmados e autóctones de influenza A (H1N1) de 5 de abril a 26 de setembro de 2009. Nı́vel de agregação espacial: Município de residência (399) Nı́vel de agregação temporal: Semana epidemiológica (25) Covariáveis: Precipitação, temperatura (mı́nima e máxima), umidade relativa do ar, altitude, taxa de pobreza, IDH municipal, densidade demográfica e presença dos principais meios de transporte (ônibus municipal, ônibus intermunicipal, barco e avião). 6.15.4 Análise da temperatura na costa central da Califórnia Os dados consistem de medidas mensais de temperatura em graus centı́grados em 23 estações monitoradoras localizadas na 5 a Região Climática da Califórnia. As séries temporais correspondem ao perı́odo de janeiro de 1992 a dezembro de 2002. A altitude para cada uma das estações também estava disponı́vel. A estrutura de média foi modelada usando um nı́vel variando ao longo do tempo, uma componente sazonal e um termo linear para a altitude. 6.15.5 A Relação entre a Malária e a Chuva no Estado do Pará: Uma Análise Espaço-Temporal. Objetivos: Estudar as similaridades/diferenças entre as ocorrências de malária no Pará; Estudar a relação entre a incidência de malária e a quantidade de chuva; Verificar existência de estrutura espacial e temporal. Material: As informações deste trabalho referem-se ao número de casos de malária em alguns municı́pios do estado do Pará através de dados mensais coletados durante os anos de 96 à 98. Os dados de chuva são dados mensais observados durante os anos de 96 a 98 coletados em 78 estações monitoradoras espalhadas pelos municı́pios. 6.15.6 Modelagem espaço temporal da Chikungunya no município do Rio de Janeiro - Spatio-temporal modelling of the first Chikungunya in Rio de Janeiro: Fonte: Freitas et al. (2020) 6.15.7 Mapas de Fluxo Fonte: Dissertação “Análisis espacial de factores socioeconómicos, de servicios de salud y de mortalidad por cáncer de mama, Argentina, 2009-2011” por Andrea Perinetti. 6.16 Material on line sobre Estatística Espacial Análise Espacial de Dados Geográficos http://www.dpi.inpe.br/gilberto/livro/analise/ R Spatial https://www.r-spatial.org/ Geocomputation with R https://bookdown.org/robinlovelace/geocompr/ Spatial Data Science With Applications in R https://r-spatial.org/book/ 6.17 Onde encontrar dados espaciais ? 6.17.0.1 Malhas Digitais Nacionais IBGE: Malhas territoriais IBGE: Portal de mapas IpeaGeo: Malhas Pacote R: geobr INDE: Infraestrutura Nacional de Dados Espaciais portal Dados Abertos 6.17.0.2 Agregadores de Links e Bases LabGIS UERJ GeoAplicada ForestGIS 6.18 Exemplo I: Malhas no R com geobr O pacote geobr desenvolvido pelo IPEA facilita a obtenção de malhas de estados, municípios e outras regionalizações diretamente no R. Vamos demonstrar um breve exemplo de suas funcionalidades. library(tidyverse) # Caso geobr não esteja instalado, rodar: # install.packages(&#39;geobr&#39;) library(geobr) 6.18.0.1 Malha de municípios: Estado de São Paulo As principais funções do pacote geobr para obtenção de malhas têm o prefixo “read_”. Em seguida, especificamos o nível de agregação que desejamos, como por exemplo: unidades da federação (read_state()), municípios (read_municipality()), ou bairros (read_neighborhood). Neste primeiro exemplo, vamos recuperar a malha dos municípios do estado de São Paulo: malha_sp &lt;- read_municipality(code_muni = &quot;SP&quot;) E assim, obtemos o seguinte objeto: head(malha_sp) ## Simple feature collection with 6 features and 4 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -51.18 ymin: -23.04 xmax: -46.55 ymax: -21.2 ## Geodetic CRS: SIRGAS 2000 ## code_muni name_muni code_state abbrev_state geom ## 1 3500105 Adamantina 35 SP MULTIPOLYGON (((-51.09 -21.... ## 2 3500204 Adolfo 35 SP MULTIPOLYGON (((-49.7 -21.3... ## 3 3500303 Aguaí 35 SP MULTIPOLYGON (((-47.01 -22.... ## 4 3500402 Águas Da Prata 35 SP MULTIPOLYGON (((-46.73 -21.... ## 5 3500501 Águas De Lindóia 35 SP MULTIPOLYGON (((-46.63 -22.... ## 6 3500550 Águas De Santa Bárbara 35 SP MULTIPOLYGON (((-49.29 -22.... Note a coluna geom; é ela que contém as informações sobre a geometria dos municípios. Para visualizar podemos usar a função nativa plot(), mas também podemos usar o ggplot2, com a função geom_sf() ggplot(malha_sp) + geom_sf(aes(geometry=geom)) + # especificamos a coluna referente à geometria theme_void() # para gerar um gráfico limpo, sem os eixos 6.18.0.2 Malha de bairros: Rio de Janeiro Agora vamos realizar o mesmo procedimento, mas com um nível de agregação menor: bairros. Podemos obter dados nesse nível utilizando agora a função read_neighborhood. Essa função não permite um filtro de localidade diretamente na chamada da função; assim temos que recuperar a malha de bairros do Brasil inteiro para posteriormente realizar o filtro desejado: malha_rio &lt;- read_neighborhood() %&gt;% filter(name_muni == &quot;Rio De Janeiro&quot;) # filtrando apenas o município do Rio head(malha_rio) ## Simple feature collection with 6 features and 11 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -43.27 ymin: -22.96 xmax: -43.17 ymax: -22.91 ## Geodetic CRS: SIRGAS 2000 ## code_muni name_muni name_neighborhood code_neighborhood code_subdistrict name_subdistrict code_district name_district code_state abbrev_state reference_geom geom ## 1 3304557 Rio De Janeiro Catumbi 330455705005 33045570508 Rio Comprido 330455705 Rio De Janeiro 33 RJ neighborhood MULTIPOLYGON (((-43.2 -22.9... ## 2 3304557 Rio De Janeiro Botafogo 330455705014 33045570509 Botafogo 330455705 Rio De Janeiro 33 RJ neighborhood MULTIPOLYGON (((-43.17 -22.... ## 3 3304557 Rio De Janeiro Tijuca 330455705030 33045570513 Tijuca 330455705 Rio De Janeiro 33 RJ neighborhood MULTIPOLYGON (((-43.26 -22.... ## 4 3304557 Rio De Janeiro Cosme Velho 330455705013 33045570509 Botafogo 330455705 Rio De Janeiro 33 RJ neighborhood MULTIPOLYGON (((-43.2 -22.9... ## 5 3304557 Rio De Janeiro Flamengo 330455705009 33045570509 Botafogo 330455705 Rio De Janeiro 33 RJ neighborhood MULTIPOLYGON (((-43.18 -22.... ## 6 3304557 Rio De Janeiro Glória 330455705010 33045570509 Botafogo 330455705 Rio De Janeiro 33 RJ neighborhood MULTIPOLYGON (((-43.18 -22.... Repetimos o código usado anteriormente; a coluna geom agora refere-se aos limites dos bairros. ggplot(malha_rio) + geom_sf(aes(geometry = geom)) + theme_void() Nos capítulos referentes à análise de dados de área outras funções serão exploradas. 6.19 Exemplo II: Geocodificação Há diversas formas de realizar o processo de geocodificação dentro e fora do R. O pacote tidygeocoder permite o envio de um data frame de endereços no R e retorna um objeto tibble com os resultados da geocodificação. O pacote permite a escolha do servidor de preferência - OpenStreetMap, ArcGIS, Bing, Google (Requer API) e outros. # se não estiver instalado, rodar: # install.packages(&#39;tidygeocoder&#39;) library(tidygeocoder) Vamos criar um objeto tibble (equivalente ao data frame, no universo tidyverse) com alguns pontos no Rio de Janeiro: enderecos &lt;- tribble(~local, ~endereco, &quot;ENSP&quot;, &quot;R. Leopoldo Bulhões, 1480 - Manguinhos, Rio de Janeiro - RJ&quot;, &quot;UPA Engenho Novo&quot;, &quot;Rua Sousa Barros, 70 - Engenho Novo, Rio de Janeiro - RJ&quot;, &quot;Endereço sem bairro&quot;, &quot;Av. de Santa Cruz, 7138, Rio de Janeiro - RJ&quot;, &quot;Praia de Sepetiba&quot;, &quot;Praia de Sepetiba, Rio de Janeiro - RJ&quot;) enderecos_cod &lt;- enderecos %&gt;% geocode(endereco, method = &quot;osm&quot;, lat = latitude, long = longitude) Conferindo o resultado da solicitação: enderecos_cod ## # A tibble: 4 × 4 ## local endereco latitude longitude ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ENSP R. Leopoldo Bulhões, 1480 - Manguinhos, Rio de Janeiro - RJ -22.9 -43.3 ## 2 UPA Engenho Novo Rua Sousa Barros, 70 - Engenho Novo, Rio de Janeiro - RJ NA NA ## 3 Endereço sem bairro Av. de Santa Cruz, 7138, Rio de Janeiro - RJ -22.9 -43.5 ## 4 Praia de Sepetiba Praia de Sepetiba, Rio de Janeiro - RJ -23.0 -43.7 Vê-se que nem todos os endereços foram encontrados utilizando o OpenStreetMap. Vamos visualizar os pontos obtidos junto aos bairros do Rio de Janeiro obtidos anteriormente: ggplot(enderecos_cod) + geom_sf(data = malha_rio, aes(geometry = geom)) + geom_point(aes(longitude, latitude), color = &quot;black&quot;) + geom_label(aes(longitude, latitude, label = local), nudge_y = -0.018) + theme_void() 6.20 Bibliografia sugerida Bailey, Trevor C.; Gatrell, Anthony C. (1995) Interactive Spatial Data Analysis. Harlow Essex: Longman. Cressie, N. A. C. (1991). Statistic for Spatial Data. New York. Costa, Ana C. C., et al. (2015). Surveillance of dengue vectors using spatio-temporal Bayesian modeling. BMC medical informatics and decision making 15.1: 93. Sansó, B., Schmidt, A. M. e Nobre, A. A. (2008). Bayesian Spatio-temporal models based on discrete convolutions. Canadian Journal of Statistics, 36, 239-258. Nobre, A. A., Schmidt, A. M. e Lopes, H. F. (2005). Spatio-temporal models for mapping the incidence of malaria in Pará. Environmetrics, 16, 291-304. Pfeiffer, D. U., et al. (2008) Spatial Analysis in Epidemiology. Oxford University Press. "],["padrões-pontuais-i.html", " 7 Padrões Pontuais I 7.1 O que são Padrões Pontuais? 7.2 Tipos de Distribuições 7.3 Processos pontuais 7.4 Completa Aletoriedade Espacial (CSR - complete spatial randomness) 7.5 Estimativa de Kernel 7.6 Kernel por atributo 7.7 Razão de Kernel 7.8 Análise preliminar de um processo pontual 7.9 Propriedade de segunda ordem 7.10 Detecção de cluster 7.11 Exercícios Propostos 7.12 Bibliografia sugerida", " 7 Padrões Pontuais I 7.1 O que são Padrões Pontuais? A análise de padrão de pontos é o tipo mais simples de análise de dados espaciais. Baseia-se na localização dos eventos em um determinada área de estudo a partir das coordenadas. O objetivo é estudar a disposição espacial dos pontos a partir de suas coordenadas. Os processos pontuais são definidos como um conjunto de pontos cuja localização está em \\(R^2\\), sendo esse processo gerado por um mecanismo estocástico. Os pontos são os pares de coordenadas (x, y), que representam os eventos (observações, indivíduos, lugares ou qualquer outro objeto discreto definido no espaço). Evento Coord X Coord Y 1 4,30 2,45 2 5,39 3,35 3 4,10 3,50 Em geral o que se deseja é verificar a ocorrência de padrões espaciais nos pontos e se estão aglomerados espacialmente (clusters) ou se estão aleatoriamente distribuídos . No exemplo abaixo temos os pontos dos casos suspeitos em uma investigação de surto de hepatite A no Rio de Janeiro no verão de 2017/2018. As estrelas amarelas mostram locais onde foi encontrado o vírus da hepatite A (HAV) em amostras de água. 7.2 Tipos de Distribuições Aleatória: qualquer ponto tem a mesma probabilidade de ocorrer em qualquer local e a posição de qualquer ponto não é afetada pela posição de qualquer outro ponto; Uniforme: cada ponto é tão longe de todos os seus vizinhos quanto possível; Cluster: Muitos pontos estão concentrados juntos, e grandes áreas podem conter pouquíssimos pontos, se houver algum. Fonte: Geospatial Training Workshop Lendo os bancos com as localizações dos casos de homicídios, suicídios e acidentes de carro em Porto Alegre/RS. As principais bibliotecas do R que estaremos usando nos exemplos abaixo são: sf spatstat splancs library(tidyverse) library(spatstat) library(sf) library(splancs) # lendo os bancos local &lt;- &quot;https://raw.githubusercontent.com/ogcruz/dados_eco_2023/main/dados/&quot; homic &lt;- read.table(paste0(local, &quot;homic.dat&quot;), col.names = c(&quot;x&quot;, &quot;y&quot;)) suic &lt;- read.table(paste0(local, &quot;suic.dat&quot;), col.names = c(&quot;x&quot;, &quot;y&quot;)) acid &lt;- read.table(paste0(local, &quot;acid.dat&quot;), col.names = c(&quot;x&quot;, &quot;y&quot;)) # Plotando os casos de homicídios em um plano # cartesiano g &lt;- ggplot(homic) + geom_point(aes(x, y, color = &quot;Homicídios&quot;), shape = 1) + labs(color = &quot;&quot;) g Porto Alegre é uma cidade disposta ao longo do eixo norte/sul. O grafico perdeu a estrutura espacial, ajustando para o tamanho e forma da janela. Por isso é necessario informar ao programa que este tipo de objeto é um objeto espacial e tem uma escala, que deve ser preservada. Vamos fazer isso com transformando-o em um objeto espacial do pacote sf. # Transformando os pontos em geometria espacial # de pontos homic_sf &lt;- homic %&gt;% st_as_sf(coords = c(&quot;x&quot;, &quot;y&quot;)) suic_sf &lt;- suic %&gt;% st_as_sf(coords = c(&quot;x&quot;, &quot;y&quot;)) acid_sf &lt;- acid %&gt;% st_as_sf(coords = c(&quot;x&quot;, &quot;y&quot;)) # Repetindo o mesmo gráfico. Repare que agora # usamos a função `geom_sf()`: g &lt;- ggplot() + geom_sf(data = homic_sf, aes(geometry = geometry, color = &quot;Homicídios&quot;), shape = 1) + labs(color = &quot;&quot;) g Agora vamos adicionar os pontos referentes a suicídios e acidentes: g &lt;- g + geom_sf(data = suic_sf, aes(geometry = geometry, color = &quot;Suícidios&quot;), shape = 1) + geom_sf(data = acid_sf, aes(geometry = geometry, color = &quot;Acidentes&quot;), shape = 1) g Importando o polígono do contorno de Porto Alegre: # contorno de porto alegre contorno.poa &lt;- read.table(paste0(local, &quot;/contpoa.dat&quot;), col.names = c(&quot;x&quot;, &quot;y&quot;)) # plotando com a função `geom_polygon`: g + geom_polygon(data = contorno.poa, aes(x, y), fill = &quot;transparent&quot;, color = &quot;black&quot;) Os pontos fora do contorno sao das ilhas, não devem ser incorporados a análise. Vamos transformar o contorno também em um objeto espacial (st_polygon) para identificar os pontos fora do contorno: library(sf) # Para transformar em polígono espacial, # transformamos primeiro em matriz e em seguida # em uma lista: poa_sf &lt;- contorno.poa %&gt;% as.matrix() %&gt;% list() %&gt;% st_polygon() Agora sim, podemos utilizar a função st_within() para identificar os pontos fora do contorno: # A função st_within() tem essa funcionalidade homic_sf &lt;- homic_sf %&gt;% mutate(dentro = lengths(st_within(homic_sf, poa_sf))) ggplot(homic_sf, aes(geometry = geometry)) + geom_sf(aes(color = as.factor(dentro))) + geom_sf(data = poa_sf, fill = &quot;transparent&quot;) + labs(color = &quot;Dentro do polĩgono&quot;) # Filtra somente as observações dentro do # polígono homic_sf2 &lt;- homic_sf %&gt;% filter(dentro == 1) 7.3 Processos pontuais Os processos pontuais podem ser descritos como: Processo de primeira ordem, considerados globais ou de larga escala, que correspondem a variações no valor médio do processo no espaço. Tal processo pode ser representado por mensurações da intensidade baseado na densidade dos pontos (média dos eventos) na área de estudo (ex: Estimativa de Kernel). Processo de segunda ordem, denominados locais ou de pequena escala, é o processo representado pela interação entre dois pontos arbitrários. O objetivo desse processo é a mensuração da dependência espacial baseado na distância entre os pontos (ex: Vizinhos mais próximos, Função K). 7.4 Completa Aletoriedade Espacial (CSR - complete spatial randomness) \\(H_0\\): Os pontos estão distribuidos aleatoriamente no espaço \\(H_1\\): Os pontos podem formar clusters ou estão dispersos no espaço CSR assume que os pontos seguem um processo homogêneo de Poisson na área de estudo. Simulando alguns padrões dos dados de ponto, temos o seguinte: Simulando os processo espaciais: O pacote sf possui a função st_sample() que permite a geração de \\(n\\) pontos dentro de uma janela: set.seed(9999) # padrão aleatório alea_sf &lt;- st_sample(poa_sf, size = 100, type = &quot;random&quot;) g_alea &lt;- ggplot(alea_sf, aes(geometry = geometry)) + geom_sf() + geom_sf(data = poa_sf, fill = &quot;transparent&quot;) + ggtitle(&quot;Distribuição aleatória&quot;) g_alea # padrão regular uni_sf &lt;- st_sample(poa_sf, size = 100, type = &quot;regular&quot;) %&gt;% st_as_sf() %&gt;% # a função jitter é bastante utilizada e # serve para adicionar uma flutuação (desvio) # aleatória aos pontos st_jitter(amount = 100) %&gt;% filter(lengths(st_within(., poa_sf)) == 1) g_uni &lt;- ggplot(uni_sf, aes(geometry = x)) + geom_sf() + geom_sf(data = poa_sf, aes(geometry = geometry), fill = &quot;transparent&quot;) + ggtitle(&quot;Distribuição regular&quot;) g_uni # padrão clusterizado A função st_sample() permite a geração de pontos clusterizados a partir de um processo determinado, como um processo de cluster de Poisson, ou processo Thomas (para mais informações, ver ??spatsat.random). Essas funções levam um tempo para gerar as amostras, por isso iremos implementar uma função mais simples: # Função para gerar uma distribuição # clusterizada: Recebe um conjunto de centroides # dos clusters e gera pontos ao redor deles st_make_clusters &lt;- function(centroids, n, amount, factor = 0.002) { clustered &lt;- centroids # começa com centroides dos clusters e adiciona pontos em volta for (i in seq_len(n)) { clustered &lt;- clustered %&gt;% rbind(centroids %&gt;% st_jitter(amount = amount, factor = factor)) } clustered } # agora sim, gerando um padrão clusterizado clu_sf &lt;- st_sample(poa_sf, size = 10, type = &quot;random&quot;) %&gt;% st_as_sf() %&gt;% st_make_clusters(10, amount = 200, factor = 0.5) %&gt;% filter(lengths(st_within(., poa_sf)) == 1) g_clu &lt;- ggplot(clu_sf, aes(geometry = x)) + geom_sf() + geom_sf(data = poa_sf, aes(geometry = geometry), fill = &quot;transparent&quot;) + ggtitle(&quot;Distribuição aglomerada&quot;) gridExtra::grid.arrange(g_alea, g_clu, g_uni, ncol = 3) Através da função quadratcount() do pacote spatstat, podemos analisar a contagem de observações por quadrantes, que nos auxilia na análise da distribuição dos pontos. Para isso, precisamos converter nossos objetos para o formato específico ppp (spatial point pattern): # Convertendo para a class ppp alea_ppp &lt;- alea_sf2 %&gt;% as.ppp() uni_ppp &lt;- uni_sf2 %&gt;% as.ppp() clu_ppp &lt;- clu_sf2 %&gt;% as.ppp() # Construindo os quadrantes com as respectivas # contagens alea_qc &lt;- quadratcount(alea_ppp, nx = 5, ny = 6) uni_qc &lt;- quadratcount(uni_ppp, nx = 5, ny = 6) clu_qc &lt;- quadratcount(clu_ppp, nx = 5, ny = 6) par(mfrow = c(1, 3)) plot(alea_qc, main = &quot;Aleatório&quot;) plot(alea_ppp, add = TRUE, col = &quot;gray&quot;) plot(uni_qc, main = &quot;Regular&quot;) plot(uni_ppp, add = TRUE, col = &quot;gray&quot;) plot(clu_qc, main = &quot;Agregado&quot;) plot(clu_ppp, add = TRUE, col = &quot;gray&quot;) Testando a Completa Aletoriedade Espacial (CSR - complete spatial randomness): quadrat.test(alea_qc) Chi-squared test of CSR using quadrat counts data: X2 = 27, df = 29, p-value = 0.9 alternative hypothesis: two.sided Quadrats: 5 by 6 grid of tiles quadrat.test(clu_qc) Chi-squared test of CSR using quadrat counts data: X2 = 170, df = 29, p-value &lt;2e-16 alternative hypothesis: two.sided Quadrats: 5 by 6 grid of tiles quadrat.test(uni_qc) Chi-squared test of CSR using quadrat counts data: X2 = 7.1, df = 29, p-value = 2e-05 alternative hypothesis: two.sided Quadrats: 5 by 6 grid of tiles 7.5 Estimativa de Kernel Uma análise exploratória de um processo pontual geralmente é iniciada pela estimação da intensidade de ocorrências do processo em toda a região em estudo. Com isso, gera-se uma superfície cujo valor é proporcional à intensidade de eventos por unidade de área. O estimador Kernel é um interpolador, que possibilita a estimação da intensidade do evento em toda a área, mesmo nas regiões onde o processo não tenha gerado nenhuma ocorrência real. Essa técnica de alisamento, utiliza janela móvel que, para cada área, estima um peso variável conforme a distância. O objetivo é de estimar a intensidade do processo pontual \\(=\\) número de eventos por unidade de área. \\[\\hat{\\lambda}(s) = \\sum\\limits_{i=1}^{n} \\dfrac{1}{\\tau^2} K \\left( \\frac{(s - s_i)}{\\tau} \\right) \\nonumber\\] Sendo: \\(\\hat{\\lambda}(s)\\) o valor estimado por área; A função \\(K(\\bullet)\\) uma FDP, escolhida de forma adequada para construir uma superfície contínua sobre os dados; O parâmetro \\(\\tau\\) denominado “largura de banda ou faixa” (bandwidth), controla a suavização da superfície gerada; \\(s\\) o centro da área, representado por uma localização qualquer na área de estudo; \\(s_i\\) as localizações dos eventos observados; \\(n\\) o número de pontos (eventos). 7.5.1 Estimativa de Kernel com correção por bordas Primeiramente calcula-se o volume sob o Kernel que está de fato dentro da região de estudo. \\[\\delta_{\\tau}(s) = \\int_{R}\\dfrac{1}{\\tau^2}k \\left( \\dfrac{(s-u)}{\\tau}\\right) du\\] Aplicando a correção das bordas, obtém-se um estimador corrigido: \\[\\hat{\\lambda}(s) = \\dfrac{1}{\\delta_{\\tau}(s)} \\sum\\limits_{i=1}^{n} \\dfrac{1}{\\tau^2} K \\left( \\frac{(s - s_i)}{\\tau} \\right) \\nonumber\\] Exemplo: alea_den &lt;- density(alea_ppp, sigma = 100) clu_den &lt;- density(clu_ppp, sigma = 100) uni_den &lt;- density(uni_ppp, sigma = 100) par(mfrow = c(1, 3)) plot(alea_den, main = &quot;Aleatório&quot;) plot(alea_ppp, add = T, col = &quot;white&quot;, pch = 20) plot(clu_den, main = &quot;Aglomerado&quot;) plot(clu_ppp, add = T, col = &quot;white&quot;, pch = 20) plot(uni_den, main = &quot;Regular&quot;) plot(uni_ppp, add = T, col = &quot;white&quot;, pch = 20) Na própria função density() (veja: ?density.ppp) podemos alterar alguns parâmetros para o kernel, como sua largura de banda (sigma), tipo de Kernel (kernel), correção de bordas (diggle), vetor de pesos (weights), etc. Alterando a largura de banda de forma exploratória para os dados clusterizados: clu_den1 &lt;- density(clu_ppp, sigma = 200) clu_den2 &lt;- density(clu_ppp, sigma = 500) clu_den3 &lt;- density(clu_ppp, sigma = 1000) # vamos apresentar agora os gráficos em ggplot g_den1 &lt;- ggplot(as_tibble(clu_den1), aes(x, y)) + geom_tile(aes(fill = value)) + geom_point(data = as_tibble(clu_ppp), color = &quot;white&quot;) + scale_fill_viridis_c(option = &quot;B&quot;) + labs(title = &quot;Cluster&quot;, subtitle = &quot;Sigma = 200&quot;) + guides(fill = &quot;none&quot;) + theme_void() g_den2 &lt;- ggplot(as_tibble(clu_den2), aes(x, y)) + geom_tile(aes(fill = value)) + geom_point(data = as_tibble(clu_ppp), color = &quot;white&quot;) + scale_fill_viridis_c(option = &quot;B&quot;) + labs(title = &quot;Cluster&quot;, subtitle = &quot;Sigma = 500&quot;) + guides(fill = &quot;none&quot;) + theme_void() g_den3 &lt;- ggplot(as_tibble(clu_den3), aes(x, y)) + geom_tile(aes(fill = value)) + geom_point(data = as_tibble(clu_ppp), color = &quot;white&quot;) + scale_fill_viridis_c(option = &quot;B&quot;) + labs(title = &quot;Cluster&quot;, subtitle = &quot;Sigma = 1000&quot;) + guides(fill = &quot;none&quot;) + theme_void() gridExtra::grid.arrange(g_den1, g_den2, g_den3, ncol = 3) Alterando o tipo de kernel (padrão: \"gaussian\"): Com correção de bordas: clu_den1_dig &lt;- density(clu_ppp, sigma = 200, diggle = TRUE) O ggplot possui funções próprias que permitem a visualizar de estimativas de densidade, entre elas o kernel espacial. Nesses casos, o kernel é ajustado na própria função de visualização (stat_density2d). # vamos inserir linhas de contorno também g_contour1 &lt;- ggplot(as_tibble(alea_ppp), aes(x, y)) + stat_density2d_filled(h = c(500, 500)) + stat_density2d(h = c(500, 500), n = 200, contour_var = &quot;count&quot;) + geom_point(color = &quot;white&quot;) + labs(title = &quot;Aleatório&quot;) + guides(fill = &quot;none&quot;) + theme_void() g_contour2 &lt;- ggplot(as_tibble(clu_ppp), aes(x, y)) + stat_density2d_filled(h = c(500, 500)) + stat_density2d(h = c(500, 500), n = 200, contour_var = &quot;count&quot;) + geom_point(color = &quot;white&quot;) + labs(title = &quot;Cluster&quot;) + guides(fill = &quot;none&quot;) + theme_void() g_contour3 &lt;- ggplot(as_tibble(uni_ppp), aes(x, y)) + stat_density2d_filled(h = c(500, 500)) + stat_density2d(h = c(500, 500), n = 200, contour_var = &quot;count&quot;) + geom_point(color = &quot;white&quot;) + labs(title = &quot;Regular&quot;) + guides(fill = &quot;none&quot;) + theme_void() gridExtra::grid.arrange(g_contour1, g_contour2, g_contour3, ncol = 3) As linhas de contorno também podem ser visualizadas com a função contour() do R base: par(mfrow = c(1, 3)) plot(alea_den, main = &quot;Aleatório&quot;) contour(alea_den, add = T, col = &quot;white&quot;) plot(clu_den, main = &quot;Aglomerado&quot;) contour(clu_den, add = T, col = &quot;white&quot;) plot(uni_den, main = &quot;Regular&quot;) contour(uni_den, add = T, col = &quot;white&quot;) O pacote MASS possui uma função que determina uma largura de banda mais adequada para a estimação do kernel, e é a opção default quando visualizamos um kernel com ggplot(). O pacote splancs também implementa essa funcionalidade utilizando outra metodologia, atravé da função Mse2d(). opt_bw &lt;- c(MASS::bandwidth.nrd(clu_ppp$x), MASS::bandwidth.nrd(clu_ppp$y)) opt_bw [1] 1118 1450 g_contour_opt &lt;- ggplot(as_tibble(clu_ppp), aes(x, y)) + stat_density2d_filled(h = opt_bw) + stat_density2d(h = opt_bw, n = 200, contour_var = &quot;count&quot;) + geom_point(color = &quot;white&quot;) + labs(title = &quot;Cluster&quot;, subtitle = &quot;Largura de banda ótima&quot;) + guides(fill = &quot;none&quot;) + theme_void() gridExtra::grid.arrange(g_contour2 + labs(subtitle = &quot;Largura de banda arbitrária (500)&quot;), g_contour_opt, ncol = 2) 7.6 Kernel por atributo O alisamento utilizando o Kernel permite estimar a densidade de eventos por unidade de área. É possível, também, se usar uma covariável (atributo). Como exemplo, pode-se estimar a “população por unidade de área”, e até mesmo fazer a razão entre dois Kernels obtendo uma estimativa alisada de “eventos por população”. \\[\\hat{\\lambda}(s) = \\sum\\limits_{i=1}^{n} \\dfrac{1}{\\tau^2} K \\left( \\frac{(s - s&#39;_j)}{\\tau} \\right) y_i \\nonumber\\] Sendo: \\(\\lambda&#39;\\) : Estimativa do atributo para unidade de área \\(\\tau\\) : Largura de banda \\(y_i\\) : valor do atributo em cada ponto Pode-se atribuir ao centróide do setor censitário ou ao centro populacional o número de habitantes de toda a área (atributo), por exemplo. 7.7 Razão de Kernel Vamos criar uma “taxa suavizada” a partir da divisão dos alisamentos dos eventos por unidade de área dividido pelo alisamento da população por unidade de área. \\[\\hat{\\lambda}(s) = \\dfrac{\\sum\\limits_{i=1}^{n} \\dfrac{1}{\\tau^2} K \\left( \\frac{(s - s_i)}{\\tau} \\right)}{\\sum\\limits_{i=1}^{n} \\dfrac{1}{\\tau^2} K \\left( \\frac{(s - s&#39;_j)}{\\tau} \\right) y_i} \\nonumber\\] Pode-se usar diferentes larguras de banda (em geral maior no denominador para estabilizar mais). Pode-se usar outro evento pontual como “estimador da população a risco”. Exemplo: Comparando 2 eventos: casos clusterizados (casos) vs casos aleatórios (controle): alea_den1 &lt;- density(alea_ppp, sigma = 200) clu_alea_ratio &lt;- clu_den1/alea_den1 par(mfrow = c(1, 3)) plot(clu_den1, main = &quot;Cluster&quot;) plot(clu_ppp, add = T, col = &quot;white&quot;, pch = 20) plot(alea_den1, main = &quot;Aleatório&quot;) plot(alea_ppp, add = T, col = &quot;white&quot;, pch = 20) plot(clu_alea_ratio, main = &quot;Razão&quot;) Exemplo: Verificando o padrão espacial de segunda ordem dos casos de homicídos em POA/RS: # Além de transformar os pontos para ppp, vamos # especificar uma janela para delimitar a área de # estimação para o kernel: homic_ppp &lt;- homic_sf2 %&gt;% as.ppp() Window(homic_ppp) &lt;- as.owin(poa_sf) homic_den1 &lt;- density(homic_ppp, sigma = 200, diggle = TRUE) g_homic_den1 &lt;- ggplot(as_tibble(homic_den1), aes(x, y)) + geom_tile(aes(fill = value)) + geom_point(data = as_tibble(homic_ppp), color = &quot;white&quot;, shape = 1, size = 0.5) + geom_sf(data = poa_sf, aes(geometry = geometry), fill = &quot;transparent&quot;, inherit.aes = FALSE) + scale_fill_viridis_c(option = &quot;B&quot;) + labs(title = &quot;Homicídios&quot;, subtitle = &quot;sigma = 200&quot;) + guides(fill = &quot;none&quot;) + theme_void() suic_ppp &lt;- suic_sf %&gt;% as.ppp() Window(suic_ppp) &lt;- as.owin(poa_sf) suic_den1 &lt;- density(suic_ppp, sigma = 200, diggle = TRUE) g_suic_den1 &lt;- ggplot(as_tibble(suic_den1), aes(x, y)) + geom_tile(aes(fill = value)) + geom_point(data = as_tibble(suic_ppp), color = &quot;white&quot;, shape = 1, size = 0.5) + geom_sf(data = poa_sf, aes(geometry = geometry), fill = &quot;transparent&quot;, inherit.aes = FALSE) + scale_fill_viridis_c(option = &quot;B&quot;) + labs(title = &quot;Suicídios&quot;, subtitle = &quot;sigma = 200&quot;) + guides(fill = &quot;none&quot;) + theme_void() acid_ppp &lt;- acid_sf %&gt;% as.ppp() Window(acid_ppp) &lt;- as.owin(poa_sf) acid_den1 &lt;- density(acid_ppp, sigma = 200, diggle = TRUE) g_acid_den1 &lt;- ggplot(as_tibble(acid_den1), aes(x, y)) + geom_tile(aes(fill = value)) + geom_point(data = as_tibble(acid_ppp), color = &quot;white&quot;, shape = 1, size = 0.5) + geom_sf(data = poa_sf, aes(geometry = geometry), fill = &quot;transparent&quot;, inherit.aes = FALSE) + scale_fill_viridis_c(option = &quot;B&quot;) + labs(title = &quot;Acidentes&quot;, subtitle = &quot;sigma = 200&quot;) + guides(fill = &quot;none&quot;) + theme_void() gridExtra::grid.arrange(g_homic_den1, g_suic_den1, g_acid_den1, ncol = 3) homic_den2 &lt;- density(homic_ppp, sigma = 100, diggle = TRUE) g_homic_den2 &lt;- ggplot(as_tibble(homic_den2), aes(x, y)) + geom_tile(aes(fill = value)) + geom_point(data = as_tibble(homic_ppp), color = &quot;white&quot;, shape = 1, size = 0.5) + geom_sf(data = poa_sf, aes(geometry = geometry), fill = &quot;transparent&quot;, inherit.aes = FALSE) + scale_fill_viridis_c(option = &quot;B&quot;) + labs(title = &quot;Homicídios&quot;, subtitle = &quot;sigma = 100&quot;) + guides(fill = &quot;none&quot;) + theme_void() homic_den3 &lt;- density(homic_ppp, sigma = 500, diggle = TRUE) g_homic_den3 &lt;- ggplot(as_tibble(homic_den3), aes(x, y)) + geom_tile(aes(fill = value)) + geom_point(data = as_tibble(homic_ppp), color = &quot;white&quot;, shape = 1, size = 0.5) + geom_sf(data = poa_sf, aes(geometry = geometry), fill = &quot;transparent&quot;, inherit.aes = FALSE) + scale_fill_viridis_c(option = &quot;B&quot;) + labs(title = &quot;Homicídios&quot;, subtitle = &quot;sigma = 500&quot;) + guides(fill = &quot;none&quot;) + theme_void() gridExtra::grid.arrange(g_homic_den2, g_homic_den1, g_homic_den3, ncol = 3) Exemplo: Fazendo a razão de kernel entre as causas de homicídio e suicídios de Porto alegre/RS: suic_homic_ratio &lt;- suic_den1/homic_den1 g_suic_homic &lt;- ggplot(as_tibble(suic_homic_ratio), aes(x, y)) + geom_tile(aes(fill = value)) + geom_sf(data = poa_sf, aes(geometry = geometry), fill = &quot;transparent&quot;, inherit.aes = FALSE) + scale_fill_viridis_c(option = &quot;B&quot;) + labs(title = &quot;Razão&quot;, subtitle = &quot;Suicídios/Homicídios&quot;) + guides(fill = &quot;none&quot;) + theme_void() gridExtra::grid.arrange(g_homic_den1, g_suic_den1, g_suic_homic, ncol = 3) 7.8 Análise preliminar de um processo pontual 7.8.1 Função F e G - Distância do vizinho mais próximo Kernel e quadrat permitem explorar a variação da média do processo na região de estudo (propriedade de primeira ordem). Para investigar a propriedade de segunda ordem é necessário observar as distâncias entre os eventos. O método do vizinho mais próximo estima a função de distribuição cumulativa baseado nas distâncias entre eventos ou pontos em uma região de análise. Dois tipos de distâncias: evento-evento (W) e ponto aleatório-evento (X) \\[\\hat{F}(x) = \\dfrac{\\#(x_i \\leq x)}{m}\\] \\[\\hat{G}(w) = \\dfrac{\\#(w_i \\leq w)}{n}\\] Sabendo que: W - evento-evento X - ponto-evento # - contagem de pontos onde a condição acontece n - total de eventos m - total de pontos aleatórios Podemos dizer que ambas as funções podem ser representadas pelo número de (\\(x_i\\)) ou (\\(w_i\\)) cuja distância é menor ou igual ao evento (\\(x\\)) ou ponto (\\(w\\)), dividido pelo total de pontos (\\(m\\)) ou total de eventos (\\(n\\)) na região. O resultado desta função empírica é o histograma das distâncias para o vizinho mais próximo - cada classe do histograma é uma contagem de eventos que ocorrem até aquela distância. A plotagem dos resultados desta função de distribuição cumulativa empírica pode ser usada como um método exploratório para verificar se existe evidência de interação entre os eventos. Se esta plotagem apresentar um crescimento rápido para pequenos valores de distância, esta situação aponta para interação entre os eventos caracterizando agrupamentos nestas escalas. Se esta plotagem apresentar valores pequenos no seu início, e só crescer rapidamente para valores maiores de distância, esta situação aponta para uma distribuição mais regular. par(mfrow = c(1, 3)) plot(envelope(Y = alea_ppp, fun = Gest, nsim = 99), main = &quot;Aleatório&quot;) plot(envelope(Y = uni_ppp, fun = Gest, nsim = 99), main = &quot;Regular&quot;) plot(envelope(Y = clu_ppp, fun = Gest, nsim = 99), main = &quot;Agregado&quot;) Embora o método do vizinho mais próximo forneça uma indicação inicial da distribuição espacial, ele considera apenas escalas pequenas. Para se ter informação mais efetiva para o padrão espacial em escalas maiores, o melhor método a ser utilizado é o da função \\(K\\). 7.9 Propriedade de segunda ordem 7.9.1 Função K de Ripley (ou apenas função K) A função \\(K\\) permite analisar as propriedades de segunda ordem de um processo isotrópico. \\[\\lambda K(h) = E(\\#eventos)\\] Sendo: \\(#eventos\\) - o número de eventos esperados até distância \\(h\\) \\(\\lambda\\) - a intensidade ou número médio de eventos por unidade de área eventos contidos a uma distância h de um evento arbitrário\\()\\) A função \\(K(h)\\) é, para cada distância \\(h\\), o somatório do total de pares cuja distância é menor de que \\(h\\), vezes o inverso do total de pares ordenados existente na região \\(R\\). \\[K(h) = \\dfrac{1}{\\lambda^{2} R}\\sum \\sum\\limits_{i \\neq j} I_{h} (d_{ij})\\] Supondo: \\[I_{h}(d_{ij}) = \\begin{cases} 1 \\ \\ se\\ \\ d_{ij} \\leq h \\\\ 0 \\ \\ se\\ \\ d_{ij} &gt; h \\end{cases}\\] \\(I_{h}(d_{ij})\\) é uma função indicadora Esta função também necessita de correção do efeito de borda; A função K de Ripley conta quantos pontos há em círculos em torno de uma planta focal; Os círculos começam com um raio pequeno e vão até um raio que inclui toda a área de estudo; Faz-se uma média do número de pontos nas classes de distâncias em torno de todas as plantas focais da população. A distribuição é cumulativa e representa o no esperado de vizinhos em um círculo de raio r centrado em uma planta arbitrária dividido pela intensidade \\(\\lambda\\) do padrão dos pontos na área de estudo; Possíveis resultados: quando o processo é completamente aleatório, a curva se desvia relativamente pouco de \\(\\pi r²\\). A curva \\(K\\) permanece perto de o valor de referência \\(\\pi r²\\); no caso de um processo regular, obtemos: \\(\\hat{K}(r) &lt; K_{pois}(r)\\) porque se os pontos forem repulsivos, eles têm menos vizinhos em média em um raio \\(r\\) do que teriam baseado no pressuposto de uma distribuição aleatória de pontos. Graficamente, a curva K reflete isso repulsão: vemos que no gráfico à abaixo, no processo regular, a curva K está localizada abaixo da referência valor (\\(\\pi r²\\)); no caso de um processo agregado, há em média mais pontos em um raio \\(r\\) ao redor os pontos do que o número esperado sob uma distribuição aleatória: consequentemente, os pontos atraem um ao outro e \\(\\hat{K}(r) &gt; K_{pois}(r)\\). Graficamente, a curva \\(K\\) está neste momento localizada acima da valor de referência para todas as áreas de estudo. par(mfrow = c(1, 3)) plot(envelope(Y = alea_ppp, fun = Kest, nsim = 99), main = &quot;Aleatório&quot;) plot(envelope(Y = uni_ppp, fun = Kest, nsim = 99), main = &quot;Regular&quot;) plot(envelope(Y = clu_ppp, fun = Kest, nsim = 99), main = &quot;Agregado&quot;) 7.9.2 Função L A função \\(K(h)\\) tem uma distribuição teórica sob condições de aleatoriedade, quando a probabilidade de ocorrência de um evento em qualquer ponto de R é independente da ocorrência de outros eventos e igual em toda a superfície. Neste caso, o número de eventos a uma distância \\(h\\) será \\(\\pi \\lambda h^2\\): \\[ K(h) = \\pi h^2\\] No caso de distribuição regular, \\(K(h)\\) será menor que \\(\\pi h^2\\). Distribuição em cluster, K(h) será maior que \\(\\pi h^2\\). A função \\(L(h)\\) permite comparar a função \\(K(h)\\) e \\(\\pi h^2\\) \\[L(h) = \\sqrt {\\dfrac{K(h)}{\\pi}}\\] - Picos positivos indicam atração espacial - cluster - Vales negativos - repulsão espacial ou regularidade par(mfrow = c(1, 3)) plot(envelope(Y = alea_ppp, fun = Lest, nsim = 99), main = &quot;Aleatório&quot;) plot(envelope(Y = uni_ppp, fun = Lest, nsim = 99), main = &quot;Regular&quot;) plot(envelope(Y = clu_ppp, fun = Lest, nsim = 99), main = &quot;Agregado&quot;) Exemplo: Verificando o padrão espacial de segunda ordem dos casos de homicíodos em POA/RS: # vamos usar só a geometria para não incluir # aquela variável &#39;dentro&#39; homic_ppp2 &lt;- as.ppp(homic_sf2$geometry) plot(homic_ppp2, pch = 19, cex = 0.5) polymap(contorno.poa, add = T) Note que o restangulo envolvente (bbox) foi feito com a coordenadas dos dados, não do contorno de POA. Grafícos das Funções K , G e L dos homicíodos em POA/RS: par(mfrow = c(1, 3)) plot(envelope(Y = homic_ppp2, fun = Kest, nsim = 99), main = &quot;Funcao K&quot;) plot(envelope(Y = homic_ppp2, fun = Gest, nsim = 99), main = &quot;Funcao G&quot;) plot(envelope(Y = homic_ppp2, fun = Lest, nsim = 99), main = &quot;Funcao L&quot;) Comente esses resultados! 7.10 Detecção de cluster Definição (Knox): grupo de ocorrências geograficamente limitado em tamanho e concentração tais que seja improvável ocorrer por mero acaso. São causas de cluster: fonte comum, contagiosidade. Clusters são em geral espaço-temporais. É importante considerar: Demais fatores de risco – sexo, idade; Residência X outros locais; Latência. Dois tipos básicos de testes: Focados: testa-se a hipótese de excesso de casos ao redor de fonte suspeita, identificada antes de observar os dados; Genéricos: busca identificar áreas quentes, sem especificar quais e quantas. Hipóteses dos testes: \\(H_0\\): É ausência de cluster: completa aleatoriedade espacial (CSR). CSR Sendo: \\(n\\) são subdivisões da região do estudo, \\(y_i\\) número de casos observados e \\(E_i\\) esperados, \\(\\lambda\\) eventos por unidade de área (e tempo). Alternativas: Focados - \\(\\lambda\\) varia com a distância da fonte Genéricos - existe regiões onde \\(\\lambda\\) é mais elevado 7.10.1 Testes Genéricos de Cluster Knox: Testa um número acima do esperado de pares de casos excessivamente próximos (segundo critério pré-estabelecido) no espaço e no tempo. Mantel: Distância no tempo e distância no espaço, se \\(x\\) for 1 e \\(y\\) for 1, equivale ao teste de Knox. \\[\\sum \\sum\\limits_{i \\neq j} x_{ij} y_{ij} \\] Cuzick-Edwards: Caso-controle onde a coincidência de casos vizinhos aumenta o peso, e a junção controle-controle ou caso-controle tem peso zero; este teste permite considerar a variação populacional. 7.10.2 Fonte Específica Cluster ao redor de um ponto ou uma linha. Compara-se a ocorrência de no excessivo de “casos” em relação à população a partir de uma função de decaimento em relação à possível fonte. \\[\\lambda (s) = \\rho \\lambda&#39;(s)f(h;\\theta)\\] \\[f(h;\\theta) = 1 + \\theta_1 e^{\\theta_{2}h^2}\\] Sendo: \\(\\theta(s)\\) - estimativa do evento p/ unidade de área \\(\\rho\\) - parâmetro que indica a razão entre “casos” e “controles” \\(\\lambda&#39;(s)\\) - estimativa população p/ unidade de área \\(f\\) - função da distância para a fonte \\(θ\\) - parâmetros a estimar que descrevem como a incidência varia em torno da fonte 7.11 Exercícios Propostos Testar a CSR e explorar os Kernels para as causas de suicídio e acidentes de carro em POA/RS. Fazer a razão de kernel entre suicídios e acidentes de carro em POA/RS. Inspecionar o processo pontual de segunta ordem, utilizando as funções K, G e L para as causas de suicídio e acidentes de carro em POA/RS. 7.11.1 Exercícios Resolvidos Exercícios Padrões Pontuais 7.12 Bibliografia sugerida ASSUNÇÃO, Renato M. Estatística espacial com aplicações em epidemiologia, economia e sociologia. São Carlos: Associação Brasileira de Estatística, v. 131, 2001. DIGGLE, Peter J. et al. Statistical analysis of spatial point patterns. Academic press, 1983. GATRELL, Anthony C. et al. Spatial point pattern analysis and its application in geographical epidemiology. Transactions of the Institute of British geographers, p. 256-274, 1996. BADDELEY, Adrian , RUBAK, Ege, TURNER, Rolf Turner. Spatial Point Patterns: Methodology and Applications with R Chapman and Hall/CRC 810 Pages. 1st Edition 2015. BIVAND, Roger. et al. Applied Spatial Data Analysis with R, Springler 2008 "],["padrões-pontuais-ii.html", " 8 Padrões Pontuais II 8.1 Alguns usos em Epidemiologia Espacial 8.2 Exemplo com os dados de dengue em Dourados/MS 8.3 Modelos Aditivos Generalizados (GAM) 8.4 Modelos Espaciais Generalizados Aditivos 8.5 Bibliografia sugerida", " 8 Padrões Pontuais II 8.1 Alguns usos em Epidemiologia Espacial A partir de processos pontuais é possível se realizar análises tais quais: Estimar a variação do risco relativo no espaço Estudos Caso-Controle espaciais Regressões logísticas lineares e não lineares (GAM) no espaço Análises de processos pontuais marcados (marked point processes) 8.2 Exemplo com os dados de dengue em Dourados/MS Nesta aula serão utilizados os dados da monografia de Isis Rodrigues Reitman, apresentada ao Curso de Geografia da Faculdade de Ciências Humanas da Universidade Federal da Grande Douradosos/MS, em março de 2013. O título da monografia é “DISTRIBUIÇÃO ESPACIAL DOS CASOS DE DENGUE NO PERÍMETRO URBANO DE DOURADOS-MS E SUA RELAÇÃO COM OS FATORES SOCIOAMBIENTAIS E POLÍTICOS” Carregando pacotes: library(readr) library(tidyverse) library(sf) library(spatstat) Lendo a tabela da população por setor censitário e baixando os shapefiles do contorno e dos setores censitários de Dourados/MS: local &lt;- &quot;https://raw.githubusercontent.com/ogcruz/dados_eco_2023/main/dados/&quot; pop2010 &lt;- read_csv(paste0(local, &quot;pop2010.csv&quot;)) tmpdir &lt;- tempdir() download.file(paste0(local, &quot;setores_dourados.zip&quot;), destfile = paste0(tmpdir, &quot;/dourados.zip&quot;)) unzip(zipfile = paste0(tmpdir, &quot;/dourados.zip&quot;), exdir = tmpdir) dir(tmpdir) setor &lt;- read_sf(paste0(tmpdir, &quot;/Setor_UTM_SIRGAS.shp&quot;), crs = 31981) contorno &lt;- read_sf(paste0(tmpdir, &quot;/contorno.shp&quot;), crs = 31981) popsetor &lt;- setor %&gt;% mutate(idsetor = as.numeric(CD_GEOCODI)) %&gt;% left_join(pop2010, by = &quot;idsetor&quot;) Lendo os casos de dengue georreferenciados em Dourados/MS: casos &lt;- read_csv(paste0(local, &quot;dengue_dourados.csv&quot;)) casos.pt &lt;- casos %&gt;% st_as_sf(coords = c(&quot;X&quot;, &quot;Y&quot;), crs = 31981) Plotando os casos de dengue segundo o sexo: plot(casos.pt[&quot;CS_SEXO&quot;], pch = 19, cex = 0.5, main = &quot;Dengue por Sexo&quot;) Usando a ggplot() para fazer um gráfico do contorno e dos casos: Formatando os pontos que representam os casos de dengue na classe ppp (point pattern): cont.w &lt;- as.owin(contorno) dengue.ppp &lt;- ppp(x = casos$X, y = casos$Y, window = cont.w) plot(dengue.ppp, pch = 16, cex = 0.5, cols = &quot;red&quot;, main = &quot;casos dengue&quot;) Como vimos anteriormente podemos usar a técnica de quadrats para termos uma ideia da distribuição dos casos de dengue em Dourados/MS. Vamos usar nesse caso um grade de 5x5 (25 quadrados) , no entando pode-se ajustar esse valor e não é necessário que a grade seja simétrica. ## ## Conditional Monte Carlo test of CSR using quadrat counts ## Test statistic: Pearson X2 statistic ## ## data: ## X2 = 1927, p-value = 1e-04 ## alternative hypothesis: clustered ## ## Quadrats: 24 tiles (irregular windows) Uma vez que temos o objeto em formato ppp, visualizamos pelo método dos quadrats podemos verificar a melhor largura de banda sugerida por vários métodos disponíveis pela biblioteca spatstat para os casos de Dengue em Dourados/MS. Nome Comando R Resultado Diggle bw.diggle(dengue.ppp) 14.1335 Cronie and van Lieshout’s (CvL) bw.CvL(dengue.ppp) 1630.7311 Scoot bw.scott(dengue.ppp) 771.6202, 467.906 likelihood cross-validation bw.ppl(dengue.ppp) 178.0477 Existem ainda outros métodos para determinar automaticamente a largura de banda. É possível usá-los para ajudar a escolher o melhor valor, mas é preciso verificar se essa largura de banda apresenta plausibilidade dentro do contexto do estudo. Fazendo o mapa de kernel dos casos de dengue segundo várias larguras de banda. par(mfrow = c(2, 2), mar = c(1, 1, 1, 1)) plot(density(dengue.ppp, 250, diggle = TRUE), main = &quot;kernel 250 m&quot;, col = terrain.colors(64)) plot(density(dengue.ppp, 500, diggle = TRUE), main = &quot;kernel 500 m&quot;, col = terrain.colors(64)) plot(density(dengue.ppp, 750, diggle = TRUE), main = &quot;kernel 750 m&quot;, col = terrain.colors(64)) plot(density(dengue.ppp, 1000, diggle = TRUE), main = &quot;kernel 1000 m&quot;, col = terrain.colors(64)) Fazendo o kernel segundo sexo, criando padrões para cada sexo e em seguida gerando um kernel para cada categoria. masc &lt;- casos %&gt;% filter(CS_SEXO == &quot;M&quot;) masc.ppp &lt;- ppp(x = masc$X, y = masc$Y, window = cont.w) fem &lt;- casos %&gt;% filter(CS_SEXO == &quot;F&quot;) fem.ppp &lt;- ppp(x = fem$X, y = fem$Y, window = cont.w) D.masc &lt;- density(masc.ppp, 750, diggle = TRUE) D.fem &lt;- density(fem.ppp, 750, diggle = TRUE) par(mfrow = c(1, 2), mar = c(1, 0, 1, 2)) plot(D.masc, main = &quot;kernel Homens 750 m&quot;) plot(D.fem, main = &quot;kernel Mulheres 750 m&quot;) Fazendo a razão de kernel entre os sexos: D.res &lt;- D.masc/D.fem plot(D.res, main = &quot;Razão de Kernel H/M&quot;, box = FALSE) plot(contorno[1], add = T, col = NA) Podemos também fazer a proporção de homens, dividindo o kernel de homens pela soma do kernel de homens + kernel das mulheres. Vamos adicionar informações de contorno para visualizar melhor as regiões onde tem proporções iguais! cores &lt;- heat.colors(16, rev = TRUE) D.res &lt;- D.masc/(D.masc + D.fem) plot(D.res, main = &quot;Proporção de Homens&quot;, addcontour = TRUE, col = cores, box = FALSE) plot(contorno[1], add = T, col = NA) Como podemos observar no kernel acima, não foi detectada variabilidade espacial na razão entre os sexos. Observe o efeito de borda que ocorre no Norte, onde um único indivíduo do sexo masculido é responsável pelo efeito de borda. Extraindo os centróides dos setores censitários de Dourados/MS. centros &lt;- st_centroid(st_geometry(popsetor)) centros.tmp &lt;- centros %&gt;% st_coordinates() %&gt;% as_tibble() centros.ppp &lt;- ppp(x = centros.tmp$X, y = centros.tmp$Y, window = cont.w) plot(centros.ppp, pch = 19, cex = 0.5, box = FALSE) Fazendo o kernel dos pontos dos centróides dos setores censitários de Dourados/MS. Tal distribuição, pode se sugerida como uma proxy da verdadeira distribuição populacional de Dourados/MS. D.pop &lt;- density(centros.ppp, 500, weights = popsetor$pop, scalekernel = TRUE) plot(D.pop, box = FALSE) Gerando um kernel de atributo com a população de cada setor censitário. O parâmetro weights nos permite entrar o valor do atributo a ser ponderado. Desta forma é possível gerar um kernel de um valor especificado (atributo). ker_pop &lt;- density(centros.ppp, 750, weights = popsetor$pop, scalekernel = TRUE) plot(ker_pop, box = FALSE) Calculando a taxa média de casos (por 1.000 hab) de dengue do município de Dourados/MS nrow(casos.pt)/sum(popsetor$pop) * 1000 [1] 5.853 Gerando a razão de kernel (casos/população) x 1000: kcasos.b750 &lt;- density(dengue.ppp, 750, diggle = TRUE) razao &lt;- kcasos.b750 razao$v &lt;- (kcasos.b750$v/ker_pop$v) * 1000 plot(razao, main = &quot;Razão de kernel casos/população&quot;, box = FALSE) contour(razao, add = T, levels = seq(0, 25, by = 5)) Plotando a distribuição das taxas por dengue estimadas via razão de kernel. É possível verificar que a mediana das razões de kernel é bem próxima a taxa média de casos (por 1.000 hab) em Dourados/MS. boxplot(as.numeric(razao$v), col = &quot;green&quot;, main = &quot;Boxplot da razão de kernel&quot;) abline(h = mean(as.numeric(razao$v), na.rm = T), lty = 3, col = &quot;red&quot;) Sobrepondo a malha da população por setores censitários (dados de área) com os pontos de casos de dengue (padrões pontuais) ggplot(popsetor) + geom_sf(aes(fill = pop)) + geom_sf(data = casos.pt, color = &quot;white&quot;, size = 0.7) + theme_void() 8.3 Modelos Aditivos Generalizados (GAM) Um modelo aditivo generalizado (Hastie and Tibishirani, 1990) é um modelo linear generalizado com um preditor linear envolvendo a soma de funções suavizadas das covariáveis + os efeitos fixos das mesmas. \\[\\eta = \\sum X \\beta + f_1(x_{1i}) + f_2(x_{2i}) + \\ldots\\] 8.4 Modelos Espaciais Generalizados Aditivos São modelos aditivos generalizados tendo como um dos preditores o efeito suavizado das componentes espaciais. \\[\\eta = \\sum X \\beta + f_1(x_{1i}) + f_2(x_{2i}) + f_3(latitude_{i}, longitude_{i}) + \\ldots\\] Exemplo GAM Dourados - Tipo Caso/Controle Vamos ajustar um modelo GAM do tipo “caso/controle”, onde casos serão representados pelos casos de dengue confirmados e controles os casos não confirmados. casos.pt$X &lt;- casos$X casos.pt$Y &lt;- casos$Y grade &lt;- expand.grid(X = seq(720900.6, 734155.5, length.out = 150), Y = seq(7535267.6, 7544897.2, length.out = 100)) suppressMessages(library(mgcv, quietly = TRUE)) mod0 &lt;- gam(CLASSI_FIN == 1 ~ s(X, Y), data = casos.pt, family = binomial) summary(mod0) Family: binomial Link function: logit Formula: CLASSI_FIN == 1 ~ s(X, Y) Parametric coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.0069 0.0795 12.7 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Approximate significance of smooth terms: edf Ref.df Chi.sq p-value s(X,Y) 21.9 26.2 144 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 R-sq.(adj) = 0.161 Deviance explained = 14.7% UBRE = 0.085615 Scale est. = 1 n = 1017 Podemos observar que o modelo espacial vazio parace evidenciar que o componente espacial s(X,Y) é significativo, ou seja, existe indícios que o espaço geográfico está influenciando a variável de desfecho. Agora vamos verificar a saída gráfica original do modelo. vis.gam(mod0, main = &quot;Modelo Vazio&quot;, plot.type = &quot;contour&quot;, color = &quot;terrain&quot;, contour.col = &quot;black&quot;, lwd = 2) Essa saída não parece ser muito intuitiva, apesar ser possível observarmos as áreas que apresentam ‘pistas’ de haver um risco maior e as áreas que estão mais isentas de casos de dengue. Vamos agora tentar melhorar tal saída gráfica. library(splancs) library(fields) TAM &lt;- 400 caixa &lt;- st_bbox(contorno) grade &lt;- expand.grid(x = seq(caixa[1], caixa[3], length.out = TAM), y = seq(caixa[2], caixa[4], length.out = TAM)) contorno.xy &lt;- as.data.frame(slot(slot(slot(as_Spatial(contorno), &quot;polygons&quot;)[[1]], &quot;Polygons&quot;)[[1]], &quot;coords&quot;)) inside &lt;- in.out(as.matrix(contorno.xy), as.matrix(grade)) outside &lt;- list(x = seq(caixa[1], caixa[3], length.out = TAM), y = seq(caixa[2], caixa[4], length.out = TAM), z = matrix(rep(0, TAM^2), ncol = TAM)) outside$z[inside] &lt;- NA x &lt;- outside$x y &lt;- outside$y newgam &lt;- data.frame(X = grade[, 1], Y = grade[, 2]) gg.pred &lt;- predict(mod0, newdata = newgam, type = &quot;terms&quot;, terms = &quot;s(X,Y)&quot;, se.fit = T) gg.pred$fit[inside == F] &lt;- NA gg.pred$se.fit[inside == F] &lt;- NA z &lt;- exp(matrix(gg.pred$fit, TAM, TAM)) ## a very rough estimate of confidence intervals z.inf &lt;- exp(gg.pred$fit + (1.96 * gg.pred$se.fit)) z.sup &lt;- exp(gg.pred$fit - (1.96 * gg.pred$se.fit)) z.inf &lt;- matrix(z.inf, TAM, TAM) z.sup &lt;- matrix(z.sup, TAM, TAM) cores &lt;- c(&quot;#053061&quot;, &quot;#2166ac&quot;, &quot;#4393c3&quot;, &quot;#92c5de&quot;, &quot;#d1e5f0&quot;, &quot;#f7f7f7&quot;, &quot;#fddbc7&quot;, &quot;#f4a582&quot;, &quot;#d6604d&quot;, &quot;#b2182b&quot;, &quot;#67001f&quot;) invisible(split.screen(rbind(c(0, 0.8, 0, 1), c(0.8, 1, 0, 1)))) screen(1) image(x, y, z, zlim = range(z, na.rm = T), col = cores, asp = 1, xlab = &quot;&quot;, ylab = &quot;&quot;, main = &quot;&quot;, axes = F) # points(den$x_coord, den$y_coord, pch=19, # col=&#39;blue&#39;, cex=0.1) contour(x, y, z.inf, nlevels = 1, add = T, col = &quot;blue&quot;, lwd = 2, levels = 1, cex = 0.1, labels = &quot;&lt;1&quot;) contour(x, y, z.sup, nlevels = 1, add = T, col = &quot;red&quot;, lwd = 2, levels = 1, cex = 0.1, labels = &quot;&gt;1&quot;) splancs::polymap(contorno.xy, add = T, lwd = 2) screen(2) # The legend # range(z, na.rm=T) # to make a pretty legend # ticks &lt;- seq(0,0.5,by=0.2) ticks &lt;- quantile(na.omit(as.vector(z)), prob = seq(0, 1, by = 1/3)) ticks &lt;- seq(0, 5, by = 0.5) image.plot(zlim = range(z, na.rm = T), col = cores, axis.args = list(at = ticks, labels = ticks), legend.only = TRUE, smallplot = c(0.1, 0.25, 0.15, 0.85), legend.width = 3, legend.shrink = 0.8, horizontal = F) title(&quot;Modelo Vazio&quot;) Podemos também inspecionar a superfície do erro padrão do modelo. z &lt;- matrix(gg.pred$se.fit, TAM, TAM) invisible(split.screen(rbind(c(0, 0.8, 0, 1), c(0.8, 1, 0, 1)))) screen(1) image(x, y, z, zlim = range(z, na.rm = T), col = cores, asp = 1, xlab = &quot;&quot;, ylab = &quot;&quot;, main = &quot;&quot;, axes = F) splancs::polymap(contorno.xy, add = T, lwd = 2) screen(2) # The legend # range(z, na.rm=T) # to make a pretty legend # ticks &lt;- seq(0,0.5,by=0.2) ticks &lt;- quantile(na.omit(as.vector(z)), prob = seq(0, 1, by = 1/3)) ticks &lt;- seq(0, 10, by = 1) image.plot(zlim = range(z, na.rm = T), col = cores, axis.args = list(at = ticks, labels = ticks), legend.only = TRUE, smallplot = c(0.1, 0.25, 0.15, 0.85), legend.width = 3, legend.shrink = 0.8, horizontal = F) title(&quot;Erro Padrão - Modelo Vazio&quot;) Note que no centro, onde existe a maior quantidade de pontos, o erro e bem menor que nas áreas onde existem menos pontos e nas bordas ! Incluindo no modelo a variável sexo: mod1 &lt;- gam(CLASSI_FIN == 1 ~ CS_SEXO + factor(CS_RACA) + s(X, Y), data = casos.pt, family = binomial) summary(mod1) Family: binomial Link function: logit Formula: CLASSI_FIN == 1 ~ CS_SEXO + factor(CS_RACA) + s(X, Y) Parametric coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.9830 0.1097 8.96 &lt;2e-16 *** CS_SEXOM -0.0215 0.1530 -0.14 0.888 factor(CS_RACA)2 -0.6188 0.3746 -1.65 0.099 . factor(CS_RACA)3 -0.2041 0.9833 -0.21 0.836 factor(CS_RACA)4 0.2949 0.2099 1.41 0.160 factor(CS_RACA)5 -0.5668 1.1361 -0.50 0.618 factor(CS_RACA)9 0.8469 0.8517 0.99 0.320 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Approximate significance of smooth terms: edf Ref.df Chi.sq p-value s(X,Y) 21.8 26.2 143 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 R-sq.(adj) = 0.162 Deviance explained = 15.2% UBRE = 0.094684 Scale est. = 1 n = 1011 Como já visto anteriormente na análise exploratória espacial de pontos, a variável sexo não é significativa. newgam &lt;- data.frame(X = grade[, 1], Y = grade[, 2], CS_SEXO = &quot;F&quot;, CS_RACA = &quot;1&quot;) gg.pred &lt;- predict(mod1, newdata = newgam, type = &quot;terms&quot;, terms = &quot;s(X,Y)&quot;, se.fit = T) gg.pred$fit[inside == F] &lt;- NA gg.pred$se.fit[inside == F] &lt;- NA z &lt;- exp(matrix(gg.pred$fit, TAM, TAM)) ## a very rough estimate of confidence intervals z.inf &lt;- exp(gg.pred$fit + (1.96 * gg.pred$se.fit)) z.sup &lt;- exp(gg.pred$fit - (1.96 * gg.pred$se.fit)) z.inf &lt;- matrix(z.inf, TAM, TAM) z.sup &lt;- matrix(z.sup, TAM, TAM) invisible(split.screen(rbind(c(0, 0.8, 0, 1), c(0.8, 1, 0, 1)))) screen(1) image(x, y, z, zlim = range(z, na.rm = T), col = cores, asp = 1, xlab = &quot;&quot;, ylab = &quot;&quot;, main = &quot;&quot;, axes = F) # points(den$x_coord, den$y_coord, pch=19, # col=&#39;blue&#39;, cex=0.1) contour(x, y, z.inf, nlevels = 1, add = T, col = &quot;blue&quot;, lwd = 2, levels = 1, cex = 0.1, labels = &quot;&lt;1&quot;) contour(x, y, z.sup, nlevels = 1, add = T, col = &quot;red&quot;, lwd = 2, levels = 1, cex = 0.1, labels = &quot;&gt;1&quot;) splancs::polymap(contorno.xy, add = T, lwd = 2) screen(2) # The legend # range(z, na.rm=T) # to make a pretty legend # ticks &lt;- seq(0,0.5,by=0.2) ticks &lt;- quantile(na.omit(as.vector(z)), prob = seq(0, 1, by = 1/3)) ticks &lt;- seq(0, 5, by = 0.5) image.plot(zlim = range(z, na.rm = T), col = cores, axis.args = list(at = ticks, labels = ticks), legend.only = TRUE, smallplot = c(0.1, 0.25, 0.15, 0.85), legend.width = 3, legend.shrink = 0.8, horizontal = F) title(&quot;Modelo ajustado por Sexo e Raça&quot;) 8.5 Bibliografia sugerida Wood, S.N. (2017) Generalized Additive Models: an introduction with R (2nd edition), CRC. Handbook of Spatial Analysis,Insee - Eurostat 2018 "]]
