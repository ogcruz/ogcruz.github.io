j <- j + pd[[i]]$dim
}
}
else {
k <- 0
if (scale == -1 && is.null(ylim))
if (m > 0)
for (i in 1:m) {
if (pd[[i]]$dim == 1) {
if (k == 0) {
if (partial.resids)
ylim <- range(pd[[i]]$p.resid, na.rm = TRUE)
else ylim <- range(pd[[i]]$fit)
k <- 1
}
else {
if (partial.resids) {
if (min(pd[[i]]$p.resid) < ylim[1])
ylim[1] <- min(pd[[i]]$p.resid, na.rm = TRUE)
if (max(pd[[i]]$p.resid) > ylim[2])
ylim[2] <- max(pd[[i]]$p.resid, na.rm = TRUE)
}
else {
if (min(pd[[i]]$fit) < ylim[1])
ylim[1] <- min(pd[[i]]$fit)
if (max(pd[[i]]$fit) > ylim[2])
ylim[2] <- max(pd[[i]]$fit)
}
}
}
}
j <- 1
if (m > 0)
for (i in 1:m) {
if (is.null(select) || i == select) {
if (interactive() && pd[[i]]$dim < 3 && i >
1 && (i - 1)%%ppp == 0)
readline("Press return for next page....")
if (pd[[i]]$dim == 1) {
if (scale == 0 && is.null(ylim)) {
if (partial.resids)
ylimit <- range(pd[[i]]$p.resid, na.rm = TRUE)
else ylimit <- range(pd[[i]]$fit)
}
if (!is.null(ylim))
ylimit <- ylim
plot(pd[[i]]$x, trans(pd[[i]]$fit + shift),
type = "l", , xlab = pd[[i]]$xlab, ylab = pd[[i]]$ylab,
ylim = trans(ylimit + shift), xlim = xlim,
main = main, ...)
if (rug) {
if (jit)
rug(jitter(as.numeric(pd[[i]]$raw)),
...)
else rug(as.numeric(pd[[i]]$raw), ...)
}
if (partial.resids) {
if (is.null(list(...)[["pch"]]))
points(pd[[i]]$raw, trans(pd[[i]]$p.resid +
shift), pch = ".", ...)
else points(pd[[i]]$raw, trans(pd[[i]]$p.resid +
shift), ...)
}
}
else if (pd[[i]]$dim == 2) {
if (!is.null(main))
pd[[i]]$title <- main
if (pers) {
persp(pd[[i]]$xm, pd[[i]]$ym, matrix(trans(pd[[i]]$fit +
shift), n2, n2), xlab = pd[[i]]$xlab,
ylab = pd[[i]]$ylab, zlab = pd[[i]]$title,
theta = theta, phi = phi, xlim = pd[[i]]$xlim,
ylim = pd[[i]]$ylim, ...)
}
else {
image(pd[[i]]$xm, pd[[i]]$ym, matrix(trans(pd[[i]]$fit +
shift), n2, n2), xlab = pd[[i]]$xlab,
ylab = pd[[i]]$ylab, main = pd[[i]]$title,
xlim = pd[[i]]$xlim, ylim = pd[[i]]$ylim,
...)
if (rug) {
if (is.null(list(...)[["pch"]]))
points(pd[[i]]$raw$x, pd[[i]]$raw$y,
pch = ".", ...)
else points(pd[[i]]$raw$x, pd[[i]]$raw$y,
...)
}
}
}
else {
warning("no automatic plotting for smooths of more than one variable")
}
}
j <- j + pd[[i]]$dim
}
}
if (n.para > 0) {
class(x) <- c("gam", "glm", "lm")
if (is.null(select)) {
attr(x, "para.only") <- TRUE
if (interactive() && m && i%%ppp == 0)
readline("Press return for next page....")
# termplot(x, se = se, rug = rug, col.se = 1, col.term = 1)
}
else {
if (select > m) {
select <- select - m
term.labels <- attr(x$pterms, "term.labels")
term.labels <- term.labels[order == 1]
if (select <= length(term.labels)) {
if (interactive() && m && i%%ppp == 0)
readline("Press return for next page....")
# termplot(x, terms = term.labels[select], se = se,
# rug = rug, col.se = 1, col.term = 1)
}
}
}
}
if (pages > 0)
par(oldpar)
}
mod1 <- gam(CLASSI_FIN==1 ~ CS_SEXO + s(X,Y),data=casos.pt,family = binomial)
epiweek(now)
epiweek(now())
library(lubridate)
epiweek(now())
newgam <- data.frame(X=grade[,1],Y=grade[,2], CS_SEXO="F")
gg.pred <- predict(mod1,newdata=newgam, type="terms", terms="s(X,Y)",se.fit=T)
gg.pred$fit[inside==F]<-NA
suppressMessages(library(splancs,quietly = TRUE))
suppressMessages(library(fields,quietly = TRUE))
TAM <- 400
caixa <- st_bbox(contorno)
grade <- expand.grid(x=seq(caixa[1],caixa[3],length.out = TAM),y=seq(caixa[2],caixa[4],length.out = TAM))
contorno.xy <- as.data.frame(slot(slot(slot(as_Spatial(contorno),"polygons")[[1]],"Polygons")[[1]],"coords"))
inside <- in.out(as.matrix(contorno.xy),as.matrix(grade))
outside <- list(x=seq(caixa[1],caixa[3],length.out = TAM),
y=seq(caixa[2],caixa[4],length.out = TAM), z=matrix(rep(0,TAM^2),ncol=TAM) )
outside$z[inside] <- NA
x <-  outside$x
y <-  outside$y
newgam <- data.frame(X=grade[,1],Y=grade[,2])
gg.pred <- predict(mod0,newdata=newgam, type="terms", terms="s(X,Y)",se.fit=T)
gg.pred$fit[inside==F]<-NA
gg.pred$se.fit[inside==F]<-NA
z <- exp(matrix(gg.pred$fit,TAM,TAM))
## a very rough estimate of confidence intervals
z.inf <- exp(gg.pred$fit + (1.96 * gg.pred$se.fit))
z.sup <- exp(gg.pred$fit - (1.96 * gg.pred$se.fit))
z.inf <- matrix(z.inf,TAM,TAM)
z.sup <- matrix(z.sup,TAM,TAM)
cores <- c("#053061","#2166ac","#4393c3","#92c5de","#d1e5f0","#f7f7f7","#fddbc7","#f4a582","#d6604d",
"#b2182b","#67001f")
split.screen(rbind(c(0,.8,0,1), c(.8,1,0,1)))
screen(1)
image(x,y,z,zlim=range(z, na.rm=T), col=cores, asp=1, xlab="", ylab="", main="",axes=F)
#points(den$x_coord, den$y_coord, pch=19, col="blue", cex=0.1)
contour(x, y, z.inf, nlevels=1, add=T, col="blue", lwd=2, levels=1,cex=0.1,labels = '<1')
contour(x, y, z.sup, nlevels=1, add=T, col="red", lwd=2, levels=1,cex=0.1,labels = '>1')
splancs::polymap(contorno.xy,add=T,lwd=2)
screen(2) # The legend
#range(z, na.rm=T) # to make a pretty legend
#ticks <- seq(0,0.5,by=0.2)
ticks <- quantile(na.omit(as.vector(z)),prob=seq(0,1,by=1/3))
ticks <- seq(0,5,by=0.5)
image.plot(zlim=range(z, na.rm=T), col=cores, axis.args=list(at=ticks, labels=ticks),legend.only=TRUE, smallplot=c(.1,.25, .15,.85), legend.width=3, legend.shrink=.8, horizontal=F)
title("Modelo Vazio")
newgam <- data.frame(X=grade[,1],Y=grade[,2], CS_SEXO="F")
gg.pred <- predict(mod1,newdata=newgam, type="terms", terms="s(X,Y)",se.fit=T)
gg.pred$fit[inside==F]<-NA
gg.pred$se.fit[inside==F]<-NA
z <- exp(matrix(gg.pred$fit,TAM,TAM))
## a very rough estimate of confidence intervals
z.inf <- exp(gg.pred$fit + (1.96 * gg.pred$se.fit))
z.sup <- exp(gg.pred$fit - (1.96 * gg.pred$se.fit))
z.inf <- matrix(z.inf,TAM,TAM)
z.sup <- matrix(z.sup,TAM,TAM)
cores <- c("#053061","#2166ac","#4393c3","#92c5de","#d1e5f0","#f7f7f7","#fddbc7","#f4a582","#d6604d",
"#b2182b","#67001f")
split.screen(rbind(c(0,.8,0,1), c(.8,1,0,1)))
screen(1)
image(x,y,z,zlim=range(z, na.rm=T), col=cores, asp=1, xlab="", ylab="", main="",axes=F)
#points(den$x_coord, den$y_coord, pch=19, col="blue", cex=0.1)
contour(x, y, z.inf, nlevels=1, add=T, col="blue", lwd=2, levels=1,cex=0.1,labels = '<1')
contour(x, y, z.sup, nlevels=1, add=T, col="red", lwd=2, levels=1,cex=0.1,labels = '>1')
splancs::polymap(contorno.xy,add=T,lwd=2)
screen(2) # The legend
#range(z, na.rm=T) # to make a pretty legend
#ticks <- seq(0,0.5,by=0.2)
ticks <- quantile(na.omit(as.vector(z)),prob=seq(0,1,by=1/3))
ticks <- seq(0,5,by=0.5)
image.plot(zlim=range(z, na.rm=T), col=cores, axis.args=list(at=ticks, labels=ticks),legend.only=TRUE, smallplot=c(.1,.25, .15,.85), legend.width=3, legend.shrink=.8, horizontal=F)
title("Modelo Vazio")
newgam <- data.frame(X=grade[,1],Y=grade[,2], CS_SEXO="F")
gg.pred <- predict(mod1,newdata=newgam, type="terms", terms="s(X,Y)",se.fit=T)
gg.pred$fit[inside==F]<-NA
gg.pred$se.fit[inside==F]<-NA
z <- exp(matrix(gg.pred$fit,TAM,TAM))
## a very rough estimate of confidence intervals
z.inf <- exp(gg.pred$fit + (1.96 * gg.pred$se.fit))
z.sup <- exp(gg.pred$fit - (1.96 * gg.pred$se.fit))
z.inf <- matrix(z.inf,TAM,TAM)
z.sup <- matrix(z.sup,TAM,TAM)
cores <- c("#053061","#2166ac","#4393c3","#92c5de","#d1e5f0","#f7f7f7","#fddbc7","#f4a582","#d6604d",
"#b2182b","#67001f")
split.screen(rbind(c(0,.8,0,1), c(.8,1,0,1)))
screen(1)
image(x,y,z,zlim=range(z, na.rm=T), col=cores, asp=1, xlab="", ylab="", main="",axes=F)
#points(den$x_coord, den$y_coord, pch=19, col="blue", cex=0.1)
contour(x, y, z.inf, nlevels=1, add=T, col="blue", lwd=2, levels=1,cex=0.1,labels = '<1')
contour(x, y, z.sup, nlevels=1, add=T, col="red", lwd=2, levels=1,cex=0.1,labels = '>1')
splancs::polymap(contorno.xy,add=T,lwd=2)
screen(2) # The legend
#range(z, na.rm=T) # to make a pretty legend
#ticks <- seq(0,0.5,by=0.2)
ticks <- quantile(na.omit(as.vector(z)),prob=seq(0,1,by=1/3))
ticks <- seq(0,5,by=0.5)
image.plot(zlim=range(z, na.rm=T), col=cores, axis.args=list(at=ticks, labels=ticks),legend.only=TRUE, smallplot=c(.1,.25, .15,.85), legend.width=3, legend.shrink=.8, horizontal=F)
title("Modelo Sexo")
mod1 <- gam(CLASSI_FIN==1 ~ CS_SEXO + factor(CS_RACA) + s(X,Y),data=casos.pt,family = binomial)
newgam <- data.frame(X=grade[,1],Y=grade[,2], CS_SEXO="F",  CS_RACA="1")
gg.pred <- predict(mod1,newdata=newgam, type="terms", terms="s(X,Y)",se.fit=T)
gg.pred$fit[inside==F]<-NA
gg.pred$se.fit[inside==F]<-NA
z <- exp(matrix(gg.pred$fit,TAM,TAM))
## a very rough estimate of confidence intervals
z.inf <- exp(gg.pred$fit + (1.96 * gg.pred$se.fit))
z.sup <- exp(gg.pred$fit - (1.96 * gg.pred$se.fit))
z.inf <- matrix(z.inf,TAM,TAM)
z.sup <- matrix(z.sup,TAM,TAM)
cores <- c("#053061","#2166ac","#4393c3","#92c5de","#d1e5f0","#f7f7f7","#fddbc7","#f4a582","#d6604d",
"#b2182b","#67001f")
split.screen(rbind(c(0,.8,0,1), c(.8,1,0,1)))
screen(1)
image(x,y,z,zlim=range(z, na.rm=T), col=cores, asp=1, xlab="", ylab="", main="",axes=F)
#points(den$x_coord, den$y_coord, pch=19, col="blue", cex=0.1)
contour(x, y, z.inf, nlevels=1, add=T, col="blue", lwd=2, levels=1,cex=0.1,labels = '<1')
contour(x, y, z.sup, nlevels=1, add=T, col="red", lwd=2, levels=1,cex=0.1,labels = '>1')
splancs::polymap(contorno.xy,add=T,lwd=2)
screen(2) # The legend
#range(z, na.rm=T) # to make a pretty legend
#ticks <- seq(0,0.5,by=0.2)
ticks <- quantile(na.omit(as.vector(z)),prob=seq(0,1,by=1/3))
ticks <- seq(0,5,by=0.5)
image.plot(zlim=range(z, na.rm=T), col=cores, axis.args=list(at=ticks, labels=ticks),legend.only=TRUE, smallplot=c(.1,.25, .15,.85), legend.width=3, legend.shrink=.8, horizontal=F)
title("Modelo Sexo")
mod1 <- gam(CLASSI_FIN==1 ~ CS_SEXO + factor(CS_RACA) + s(X,Y),data=casos.pt,family = binomial)
summary(mod1)
summary(mod0)
tab_model(mod1, collapse.ci = TRUE)
tab_model(mod1)
tab_model(mod0, collapse.ci = TRUE)
tab_model(mod1, collapse.ci = TRUE)
tab_model(mod1, auto.label = FALSE, show.ci = FALSE)
tab_model(mod0, auto.label = FALSE, show.ci = FALSE)
tab_model(m0, show.se = TRUE, show.std = TRUE, show.stat = TRUE)
tab_model(mod0, show.se = TRUE, show.std = TRUE, show.stat = TRUE)
tab_model(mod1, show.se = TRUE, show.std = TRUE, show.stat = TRUE)
mod0
names(mod0)
summary(mod0)$coefficients
coefficients(mod0)
DT::datatable(mod0)
DT::datatable(summary(mod0))
DT::datatable(mod0)
summary(mod0)
xxx <- read_csv("https://github.com/parulnith/A-guide-to-Machine-Learning-in-R/blob/master/Part%205%20Logistic%20regression%20dataset/quality.csv")
View(xxx)
xxx <- read_csv("https://raw.githubusercontent.com/parulnith/A-guide-to-Machine-Learning-in-R/master/Part%205%20Logistic%20regression%20dataset/quality.csv")
View(xxx)
View(xxx)
tabela <- read_csv("https://raw.githubusercontent.com/parulnith/A-guide-to-Machine-Learning-in-R/master/Decision%20Trees/boston.csv")
View(tabela)
boston <- read_csv("https://raw.githubusercontent.com/parulnith/A-guide-to-Machine-Learning-in-R/master/Decision%20Trees/boston.csv")
points(boston$LON[boston$CHAS==1], boston$LAT[boston$CHAS==1], col="blue", pch=19)
plot(boston$LON, boston$LAT)
points(boston$LON[boston$CHAS==1], boston$LAT[boston$CHAS==1], col="blue", pch=19)
points(boston$LON[boston$NOX>=0.55], boston$LAT[boston$NOX>=0.55], col="green", pch=20)
points(boston$LON[boston$MEDV>=21.2], boston$LAT[boston$MEDV>=21.2], col="red", pch=20)
plot(boston$LAT, boston$MEDV)
plot(boston$LON, boston$MEDV)
latlonlm = lm(MEDV ~ LAT + LON, data=boston)
summary(latlonlm)
library(rpart)
install.packages("rpart.plot")
library(rpart.plot)
latlontree = rpart(MEDV ~ LAT + LON, data=boston)
# Plot the tree using prp command defined in rpart.plot package
prp(latlontree)
summary(boston$MEDV)
plot(boston$LON, boston$LAT)
points(boston$LON[boston$MEDV>=21.2],boston$LAT[boston$MEDV>=21.2], col="red", pch=20)
latlontree = rpart(MEDV ~ LAT + LON, data=boston, minbucket=50)
plot(latlontree)
text(latlontree)
tree = rpart(MEDV ~ LAT + LON + CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO, data=train)
library(caTools)
set.seed(123)
split = sample.split(boston$MEDV, SplitRatio = 0.7)
train = subset(boston, split==TRUE)
test = subset(boston, split==FALSE)
tree = rpart(MEDV ~ LAT + LON + CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO, data=train)
prp(tree)
olinda.map$tx <- olinda.map$CASES*10000/olinda.map$POP
library(tidyverse)
library(sf)
library(maptools)
library(spatstat)
olinda.map <- read_sf("dados/olinda.shp", crs = 5534)
DT::datatable(head(olinda.map))
olinda.map$tx <- olinda.map$CASES*10000/olinda.map$POP
olinda.map$tx <- olinda.map$CASES*10000/olinda.map$POP
plot(olinda.map['tx'])
olinda.map$tx <- olinda.map$CASES*10000/olinda.map$POP
# plot(olinda.map['tx'])
plot(st_geometry(olinda.map))
propSymbolsLayer(x = olinda.map, var = "tx",
legend.title.txt = "Taxa",
col = "#a7dfb4")
install.packages("cartography")
library(tidyverse)
library(sf)
library(maptools)
library(cartography)
#library(spatstat)
olinda.map <- read_sf("dados/olinda.shp", crs = 5534)
olinda.map$tx <- olinda.map$CASES*10000/olinda.map$POP
# plot(olinda.map['tx'])
plot(st_geometry(olinda.map))
propSymbolsLayer(x = olinda.map, var = "tx",
legend.title.txt = "Taxa",
col = "#a7dfb4")
plot(st_geometry(olinda.map))
propSymbolsLayer(x = olinda.map, var = "tx",  var2 = "cases",
legend.title.txt = "Taxa de Detecção",
col = "#a7dfb4")
plot(st_geometry(olinda.map))
propSymbolsLayer(x = olinda.map, var = "tx",
legend.title.txt = "Taxa de Detecção",
col = "#a7dfb4")
ggplot(olinda.map) + geom_sf(aes(fill=tx))
ggplot() +                                               # initialize ggplot object
geom_polygon(                                          # make a polygon
data = olinda.map,                                    # data frame
aes(x = long, y = lat, group = group,                # coordinates, and group them by polygons
fill = cut_number(tx, 5))) +                # variable to use for filling
scale_fill_brewer("Homicide Rate", palette = "OrRd") + # fill with brewer colors
ggtitle("Philadelphia homicide rate per 100,000") +    # add title
theme(line = element_blank(),                          # remove axis lines ..
axis.text=element_blank(),                       # .. tickmarks..
axis.title=element_blank(),                      # .. axis labels..
panel.background = element_blank()) +            # .. background gridlines
coord_equal()                                          # both axes the same scale
ggplot() +                                               # initialize ggplot object
geom_polygon(                                          # make a polygon
data = olinda.map,                                    # data frame
aes(fill = cut_number(tx, 5))) +                # variable to use for filling
scale_fill_brewer("Homicide Rate", palette = "OrRd") + # fill with brewer colors
ggtitle("Philadelphia homicide rate per 100,000") +    # add title
theme(line = element_blank(),                          # remove axis lines ..
axis.text=element_blank(),                       # .. tickmarks..
axis.title=element_blank(),                      # .. axis labels..
panel.background = element_blank()) +            # .. background gridlines
coord_equal()                                          # both axes the same scale
ggplot(olinda.map) +                                      # initialize ggplot object
geom_sf(aes(fill = cut_number(tx, 5))) +                # variable to use for filling
scale_fill_brewer("Taxa de Detecção", palette = "OrRd") + # fill with brewer colors
ggtitle("Taxa de Detecção de Hanseníase") +    # add title
theme(line = element_blank(),                          # remove axis lines ..
axis.text=element_blank(),                       # .. tickmarks..
axis.title=element_blank(),                      # .. axis labels..
panel.background = element_blank()) +            # .. background gridlines
coord_equal()                                          # both axes the same scale
ggplot(olinda.map) + geom_sf(aes(fill=tx))
ggplot(olinda.map) + geom_sf(aes(fill=cut_number(tx, 5)))
ggplot(olinda.map) +
geom_sf(aes(fill=cut_number(tx, 5))) +
scale_fill_brewer("Taxa de Detecção", palette = "OrRd") +
ggtitle("Taxa de Detecção de Hanseníase")
ggplot(olinda.map) +
geom_sf(aes(fill=cut_number(tx, 5))) +
scale_fill_brewer("Taxa de Detecção", palette = "OrRd") +
ggtitle("Taxa de Detecção de Hanseníase") +
theme(line = element_blank(),                          # remove axis lines ..
axis.text=element_blank(),                       # .. tickmarks..
axis.title=element_blank(),                      # .. axis labels..
panel.background = element_blank()) +            # .. background gridlines
coord_equal()
ggplot(olinda.map) +
geom_sf(aes(fill=cut_number(tx, 5))) +
scale_fill_brewer("Taxa de Detecção", palette = "OrRd") +
ggtitle("Taxa de Detecção de Hanseníase") +
theme_void()
ggplot(olinda.map) +
geom_sf(aes(fill=cut_number(tx, 5))) +
scale_fill_brewer("Taxa de Detecção", palette = "OrRd") +
ggtitle("Taxa de Detecção de Hanseníase") +
theme_bw()
ggplot(olinda.map) +
geom_sf(aes(fill=cut_number(tx, 5))) +
scale_fill_brewer("Taxa de Detecção", palette = "OrRd") +
ggtitle("Taxa de Detecção de Hanseníase") +
theme_void()
library(ggmap)
ph_basemap <- get_map(location="Philadelphia, PA", zoom=11, maptype = 'satellite')
ph_basemap <- get_map(location="Philadelphia, PA", zoom=11, maptype = 'satellite',  source = "osm")
ph_basemap <- get_map(location="Philadelphia, PA", zoom=11, maptype = 'satellite',  source = "osm")
ph_basemap <- get_map(location="Philadelphia, PA", zoom=11, maptype = 'satellite',  source = c("osm"))
ph_basemap <- get_map(location="Philadelphia, PA", zoom=11,   source = c("osm"))
ph_basemap <- get_map(location="Olinda, PE", zoom=24,   source = c("osm"))
map <- get_map("olinda, PE")
map <- get_map(location = "texas", zoom = 6, source = "stamen")
install.packages("osmdata")
library(osmdata)
map <- get_map("olinda, PE")
ph_basemap <- get_map(location="Olinda, PE", zoom=24,   source = c("osm"))
mad_map <- get_map(getbb("Madrid"),maptype = "toner-background")
mad_map
ggmap(mad_map)
mad_map <- get_map(getbb("olinda, PE"),maptype = "toner-background")
mad_map <- get_map(getbb("olinda, Brazil"),maptype = "satellite")
ggmap(mad_map)
olinda_osm <- get_map(getbb("olinda, Brazil"),maptype = "satellite")
ggmap(olinda_map)
olinda_osm <- get_map(getbb("olinda, Brazil"),maptype = "satellite")
olinda_osm <- get_map(getbb("olinda, Brazil"),maptype = "satellite")
ggmap(olinda_map)
ggmap(olinda_osm)
ggmap(olinda_osm) +
ggplot(olinda.map) +
geom_sf(aes(fill=cut_number(tx, 5))) +
scale_fill_brewer("Taxa de Detecção", palette = "OrRd") +
ggtitle("Taxa de Detecção de Hanseníase") +
theme_void()
ggmap(olinda_osm) +
geom_sf(aes(fill=cut_number(tx, 5))) +
scale_fill_brewer("Taxa de Detecção", palette = "OrRd") +
ggtitle("Taxa de Detecção de Hanseníase") +
theme_void()
ggmap(olinda_osm) +
ggplot(olinda.map) +
geom_sf(aes(fill=cut_number(tx, 5))) +
scale_fill_brewer("Taxa de Detecção", palette = "OrRd") +
ggtitle("Taxa de Detecção de Hanseníase") +
theme_void()
ggplot(olinda.map)
geom_sf(aes(fill=cut_number(tx, 5)))
ggmap(olinda_osm) +
ggplot(olinda.map) +
geom_sf(aes(fill=cut_number(tx, 5))) +
scale_fill_brewer("Taxa de Detecção", palette = "OrRd") +
ggtitle("Taxa de Detecção de Hanseníase") +
theme_void()
ggplot(olinda.map) +
geom_sf(aes(fill=cut_number(tx, 5))) +
scale_fill_brewer("Taxa de Detecção", palette = "OrRd") +
ggtitle("Taxa de Detecção de Hanseníase") +
theme_void()
ggplot(olinda.map) +
geom_sf(aes(fill=cut_number(tx, 5))) +
scale_fill_brewer("Taxa de Detecção", palette = "OrRd") +
ggtitle("Taxa de Detecção de Hanseníase") +
theme_void()
ggplot(olinda.map) +
geom_sf(aes(fill=cut_number(tx, 5))) +
scale_fill_brewer("Taxa de Detecção", palette = "OrRd") +
ggtitle("Taxa de Detecção de Hanseníase") +
theme_void()
ggplot(olinda.map) +
geom_sf(aes(fill=cut_number(tx, 5))) +
scale_fill_brewer("Taxa de Detecção", palette = "OrRd") +
ggtitle("Taxa de Detecção de Hanseníase") +
theme_void()
ggmap(olinda_osm) +
ggplot(olinda.map) +
geom_sf(aes(fill=cut_number(tx, 5))) +
scale_fill_brewer("Taxa de Detecção", palette = "OrRd") +
ggtitle("Taxa de Detecção de Hanseníase") +
theme_void()
library(leaflet)
# first try... ops what happened here
leaflet(philly) %>%
addPolygons()
# first try... ops what happened here
leaflet(olinda_osm)
library(tmap)
tm_shape(philly) +
tm_polygons("HOMIC_R", style="quantile", title="Philadelphia \nhomicide rate \nper 100,000")
library(tmap)
tm_shape(olinda.map) +
tm_polygons("tx", style="quantile", title="Philadelphia \nhomicide rate \nper 100,000")
library(tmap)
tm_shape(olinda.map) +
tm_polygons("tx", style="quantile", title="Taxa de Detecção de Hanseníase")
tmap_mode("view")
last_map()
plot(olinda.map['tx'])
library(tmap)
tm_shape(olinda.map) +
tm_polygons("tx", style="quantile", title="Taxa de Detecção de Hanseníase")
tmap_mode("view")
last_map()
tm_shape(olinda.map) +
tm_bubbles("tx")
tm_shape(olinda.map) +
tm_bubbles("tx")
tm_shape(olinda.map) +
tm_polygons("tx")
tm_shape(olinda.map) +
tm_bubbles("tx")
install.packages(c("cartography", "osmdata"))
install.packages("SpatialEpi")
