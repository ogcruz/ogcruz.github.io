[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ciência de Dados Aplicada à Epidemiologia - 2025",
    "section": "",
    "text": "Apresentação do Curso\n\nCiência de Dados Aplicada à Epidemiologia I - 2025\n\nSigla: ENSP.86.120.1\nCréditos: 2\nCarga Horária: 60\n\n\nOswaldo G Cruz & Laís Picinini Freitas\n\n\n\nLinks:\nlink Principal - Zoom ENSP\nlink alternativo - Zoom Oswaldo\n\n\nAulas Gravadas\nAulas Gravadas\n\n\nOBJETIVO\nCapacitar os estudantes a compreender e aplicar os fundamentos da Ciência de Dados no contexto da Epidemiologia com o apoio de ferramentas computacionais.\n\n\nCONTEÚDO\n\nFundamentos da Ciência de Dados aplicada à Saúde\n\nConceitos básicos de Ciência de Dados e sua relevância na Epidemiologia\nÉtica no uso de dados de saúde\nReprodutibilidade e transparência em pesquisas científicas\nApresentação de ferramentas (SQL, DuckDB, R, RStudio, RMarkdown, git, github)\n\nBases de Dados e Linguagem SQL\n\nConceitos de bancos de dados relacionais\nLinguagem SQL: criação, consulta e manipulação de dados\nUso de DuckDB para análise local de grandes volumes de dados\n\nAcesso e Consumo de Dados\n\nLeitura de dados locais (CSV, Excel, etc.)\nConsumo de dados remotos: APIs, FTP e web scraping\nIntegração do DuckDB (SQL) com R para manipulação de dados locais e em nuvem\n\nLimpeza, Transformação e Armazenamento de Dados\n\nLimpeza e padronização de bases de dados (uso do dplyr, tidyr, janitor, etc.)\nManipulação eficiente com data.frame, tibble, e data.table\nArmazenamento em bancos relacionais com SQL e DuckDB\n\nVisualização de dados e comunicação\n\nVisualização de dados com ggplot2\nIntrodução ao Rmarkdown para documentação e relatórios dinâmicos\nDashboards com Shiny\nVisualizações interativas com leaflet e plotly\n\n\n\n\nAVALIAÇÃO\nTrabalho final\n\nPrazo: 31 de outubro de 2025\nTrabalho individual ou em dupla\nFormato Quarto/Rmarkdown\n\nO trabalho deve ser em formato de relatório e conter:\n\ndescrição dos dados,\nprocedimentos de importação de dados externos,\npreparação/limpeza dos dados usando o tidyverse,\ncriação de novas variáveis,\ngeração de pelo menos um gráfico e uma tabela.\n\nDe preferência, use seus dados. Caso não seja possível, use dados públicos.",
    "crumbs": [
      "Apresentação do Curso"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introdução à Ciência de Dados em Epidemiologia",
    "section": "",
    "text": "1.1 O que é Ciência de Dados?\nA Ciência de Dados é uma área interdisciplinar que combina conhecimentos de diversas áreas com o principal objetivo de transformar dados em informações úteis para a tomada de decisões.\nA Ciência de Dados não é tão nova quanto se imagina. Suas raízes remontam à estatística e à análise de dados, já existindo desde a década de 1970 em computação cientifica em diversas instituições e universidades. Recententemente, ganhou muito destaque com o aumento da disponibilidade de grandes volumes de dados e avanços em tecnologias de computação.\nA Ciência de Dados é uma evolução resultante da integração de várias disciplinas, incluindo:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução à Ciência de Dados em Epidemiologia</span>"
    ]
  },
  {
    "objectID": "intro.html#o-que-é-ciência-de-dados",
    "href": "intro.html#o-que-é-ciência-de-dados",
    "title": "1  Introdução à Ciência de Dados em Epidemiologia",
    "section": "",
    "text": "Estatística (análise de dados tradicional)\nBanco de Dados (dados estruturados e não estruturados)\nData Mining (mineração de dados)\nMachine Learning (aprendizado de máquina)\nBig Data (grandes volumes de dados)\nInteligência Artificial (IA)\nVisualização de Dados (apresentação de dados)\nEngenharia de Dados (preparação e integração de dados)\n\n\n1.1.1 E na epidemiologia?\nVivemos um momento crítico em que o cenário epidemiológico é cada vez mais desafiador, com epidemias sobrepostas e mudanças climáticas. Ao mesmo tempo, temos uma enorme produção de dados administrativos e de vigilância, mas que ainda não são utilizados em todo o seu potencial.\nHá uma necessidade de respostas baseadas em evidência que sejam rápidas e robustas, e neste ponto a Ciência de Dados pode ser muito útil.\nNa epidemiologia, a Ciência de Dados combina com áreas como matemática, estatística, e ciências da computação para identificar padrões e extrair conhecimento a partir de dados.\n\n\n\nDS Diagrama de Venn\n\n\nAlguns exemplos de projetos que aplicam Ciência de Dados em Epidemiologia incluem:\n\nCentros de Inteligência Epidemiológica\nInfoGripe\nInfoDengue\nMosqlimate\nPCDaS",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução à Ciência de Dados em Epidemiologia</span>"
    ]
  },
  {
    "objectID": "intro.html#o-que-é-pipeline-de-dados",
    "href": "intro.html#o-que-é-pipeline-de-dados",
    "title": "1  Introdução à Ciência de Dados em Epidemiologia",
    "section": "1.2 O que é Pipeline de Dados?",
    "text": "1.2 O que é Pipeline de Dados?\nUm projeto de Ciência de Dados é organizado seguindo um Pipeline de Dados, que é um fluxo automatizado de etapas desde a coleta/importação dos dados, passando pela transformação e análise de dados, até a visualização e comunicação.\nEtapas comuns de um Pipeline de Dados incluem:\n\nColeta de Dados: Capturar dados de fontes diversas: arquivos csv, dbf, APIs, bancos de dados, etc.\nLimpeza de Dados: Remover dados faltantes, duplicados, inconsistências e outliers.\nTransformação de Dados: Normalizar, padronizar, converter formatos, criar novas variáveis.\nArmazenamento de Dados: Guardar os dados em bases de dados, data lakes / data warehouse , armazenamento em nuvem, etc.\nAnálise e Modelagem: Aplicar técnicas de análises estatísticas, machine learning, etc.\nVisualização e Comunicação: Criar relatórios, dashboards, infográficos e comunicar resultados.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução à Ciência de Dados em Epidemiologia</span>"
    ]
  },
  {
    "objectID": "intro.html#kdd",
    "href": "intro.html#kdd",
    "title": "1  Introdução à Ciência de Dados em Epidemiologia",
    "section": "1.3 KDD",
    "text": "1.3 KDD\nSignifica Descoberta de Conhecimento em Bancos de Dados (Knowledge Discovery in Databases), é um processo que envolve a extração de padrões e informações úteis e compreensíveis a partir de grandes volumes de dados.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução à Ciência de Dados em Epidemiologia</span>"
    ]
  },
  {
    "objectID": "intro.html#infraestrutura-necessária-para-ciência-de-dados",
    "href": "intro.html#infraestrutura-necessária-para-ciência-de-dados",
    "title": "1  Introdução à Ciência de Dados em Epidemiologia",
    "section": "1.4 Infraestrutura Necessária para Ciência de Dados",
    "text": "1.4 Infraestrutura Necessária para Ciência de Dados\nPara fazer Ciência de Dados, é essencial ter uma infraestrutura adequada, que permita o processamento e análise eficiente dos dados.\nUmas dos principais requesitos é um servidor que vem a ser um computador especializado que fornece recursos, serviços ou dados a outros dispositivos chamados clientes. As principais características de um servidor são:\n\nUso centralizado: Não é usado para tarefas individuais, mas para armazenar dados, executar aplicações e oferecer serviços a muitos usuários simultaneamente.\nHardware especializado: Tem componentes otimizados para processamento de tarefas em larga escala, como processadores de alto desempenho, memória RAM de alta capacidade e armazenamento em disco de alto desempenho (geralmente espelhados, uso de RAID).\nSem interface gráfica: Normalmente não tem monitor, teclado ou mouse. É controlado remotamente por meio de interfaces de linha de comando ou softwares de gerenciamento.\nSistemas Operacionais: Roda SO específicos para servidores, como Linux, Unix, e o lixo do Windows Server, diponibilizando serviços tais como servidor web, banco de dados, arquivos em rede etc.\n\n\n1.4.1 Componentes Principais de um Data Center\nAqui estão os principais elementos que compõem um Data Center:\n\nServidores: São computadores especializados que executam aplicações, armazenam dados e processam requisições. Podem ser físicos (servidores de rack) ou virtuais (em nuvem).\nRedes: Switches, roteadores, firewalls e cabeamento garantem a comunicação entre servidores e clientes. As redes são altamente redundantes para evitar falhas.\nArmazenamento: Discos rígidos (HDD), SSDs, não-voláteis, bases de dados, data lakes e dados que podem também pode ser armazenamento em nuvem.\n\n\nO que é RAID?\nRAID (Redundant Array of Independent Disks) é uma tecnologia que combina vários discos rígidos em um único array (conjunto de discos rígidos conectados em paralelo, trabalhando juntos) para:\n\nGarantir redundância (cópias de dados em diferentes discos).\nAumentar a velocidade de leitura e escrita.\nProteger contra falhas (se um disco falhar, os dados ainda estão disponíveis).\nMelhorar a tolerância a falhas (continuidade do serviço).\n\n\n\nSistemas de Refrigeração: Para manter os servidores refrigerados, pois eles geram muito calor durante o processamento.\nEnergia e Backup: Sistemas de energia (como geração de energia, UPS, baterias) garantem que o Data Center nunca desligue. Sistemas de backup (cópias de segurança) protegem os dados contra perda.\nSegurança Física e Cibernética: Câmeras, sensores, portões eletrônicos, biometria para proteger o local físico. Firewalls, criptografia, autenticação de usuários para proteger os dados.\n\n\n\n1.4.2 Centro de Dados no Local (On-Premise) vs Nuvem (Cloud)\nO centro de dados no local, também chamado de on-premise, é uma infraestrutura de TI que fica em um local físico dentro um departamento ou unidade de uma instituição, geralmente em uma sala dedicada.\nA nuvem é um modelo de armazenamento e processamento de dados em servidores remotos, geralmente gerenciados por provedores de nuvem como Amazon AWS, Microsoft Azure, Google Cloud etc… De onde os serviços são contratados e expandidos ou alterados conforme desejo (ou o bolso) do cliente.\n\n1.4.2.1 Comparação: On-Premise vs Nuvem\n\n\n\n\n\n\n\n\nCritério\nOn-Premise (Centro de Dados no Local)\nNuvem (Cloud)\n\n\n\n\nControle sobre a infraestrutura\nAlto (gerencia todos os recursos)\nBaixo (provedor de nuvem gerencia a infraestrutura)\n\n\nCusto inicial\nAlto (compra de hardware, instalação, energia, etc.)\nBaixo (não há investimento inicial significativo)\n\n\nCusto operacional\nAlto (manutenção, energia, pessoal especializado)\nBaixo (custos sob demanda, modelos de assinatura)\n\n\nEscalabilidade\nBaixa (requer aquisição de novos recursos físicos)\nAlta (escala automaticamente conforme necessidade)\n\n\nManutenção\nAlta (empresa é responsável pela manutenção)\nBaixa (provedor cuida da manutenção e atualizações)\n\n\nSegurança\nMaior controle, mas depende da implementação local\nDepende do provedor e de suas medidas de segurança\n\n\nLatência\nBaixa (serviços locais)\nPode ser alta (depende da qualidade da conexão à internet)\n\n\nAcesso aos dados\nAcesso local (requer estar no mesmo local)\nAcesso remoto (disponível a partir de qualquer lugar)\n\n\nFlexibilidade\nBaixa (configurações fixas e complexas)\nAlta (recursos podem ser ajustados rapidamente)\n\n\nAtualizações\nManuais, exigem planejamento e intervenção\nAutomáticas, gerenciadas pelo provedor\n\n\nDependência da internet\nBaixa (funciona offline)\nAlta (depende da conectividade e da nuvem)\n\n\nBackup e recuperação\nGerenciado internamente, pode ser complexo\nGerenciado pelo provedor, geralmente automatizado\n\n\nUso comum\nEmpresas grandes, regulamentadas, sistemas críticos\nEmpresas de médio e pequeno porte, startups, aplicações web\n\n\nExemplos de provedores\nNenhum (Local)\nAWS, Azure, Google Cloud, Oracle Cloud, IBM Cloud\n\n\n\n\n\n\n1.4.3 Como se parece um pequeno data center local.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução à Ciência de Dados em Epidemiologia</span>"
    ]
  },
  {
    "objectID": "intro.html#principais-softwares-e-linguagens-de-programação-utilizadas-na-ciência-de-dados",
    "href": "intro.html#principais-softwares-e-linguagens-de-programação-utilizadas-na-ciência-de-dados",
    "title": "1  Introdução à Ciência de Dados em Epidemiologia",
    "section": "1.5 Principais softwares e linguagens de programação utilizadas na Ciência de Dados",
    "text": "1.5 Principais softwares e linguagens de programação utilizadas na Ciência de Dados\n\nR (estatística, matemática, análise e visualização de dados)\nPython (matemática, machine learning, IA, análise de dados)\nSistema Gerenciador de Banco de Dados (SGBD)\nSQL (linguagem padrão para bancos de dados relacionais)\nJulia (alta performance, computação científica, estatística, matemática)\nJava / Scala (Apache Spark, Weka,etc…)\nSAS (old days mas ainda usado especialmente em setores regulamentados)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução à Ciência de Dados em Epidemiologia</span>"
    ]
  },
  {
    "objectID": "intro.html#principais-ides-em-ciência-de-dados",
    "href": "intro.html#principais-ides-em-ciência-de-dados",
    "title": "1  Introdução à Ciência de Dados em Epidemiologia",
    "section": "1.6 Principais IDEs em Ciência de Dados",
    "text": "1.6 Principais IDEs em Ciência de Dados\nUm IDE (Integrated Development Environment), ou Ambiente de Desenvolvimento Integrado, é um software que reúne em um só lugar as ferramentas essenciais para o desenvolvimento de códigos em uma determinada linguagem de programação.\n\n\nRstudio (R)\nPositron (R, Python)\nVisual Studio Code (R, Python. Julia, SQL, etc…)\nPyCharm (python)\nJupyter Notebook (Python, R, Julia)\nDBeaver (SQL)\nColab (Google Colab)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução à Ciência de Dados em Epidemiologia</span>"
    ]
  },
  {
    "objectID": "intro.html#bases-de-dados-armazenando-informações",
    "href": "intro.html#bases-de-dados-armazenando-informações",
    "title": "1  Introdução à Ciência de Dados em Epidemiologia",
    "section": "1.7 Bases de Dados: Armazenando Informações",
    "text": "1.7 Bases de Dados: Armazenando Informações\nUma Base de Dados é um conjunto organizado de dados, estruturado para facilitar o armazenamento, recuperação e manipulação de informações.\nTipos de Bases de Dados:\n• Relacional (ex: MySQL, PostgreSQL, Oracle)\n• NoSQL (ex: MongoDB, Cassandra)\nO que é uma Base de Dados Relacional?\n• Dados organizados em tabelas. • Relacionamentos entre tabelas são definidos por chaves primárias e estrangeiras. • Exemplo: Tabela de pacientes, tabela de internações, etc.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução à Ciência de Dados em Epidemiologia</span>"
    ]
  },
  {
    "objectID": "intro.html#sgbd",
    "href": "intro.html#sgbd",
    "title": "1  Introdução à Ciência de Dados em Epidemiologia",
    "section": "1.8 SGBD",
    "text": "1.8 SGBD\nO que é um Sistema Gerenciador de Banco de Dados (SGBD)?\nUm Sistema Gerenciador de Banco de Dados (SGBD) é um software que permite criar, gerenciar, armazenar e recuperar dados de forma organizada, eficiente e segura.\nO SGBD atua como um intermediário entre o usuário e o armazenamento físico dos dados (banco de dados), facilitando a criação, manipulação e consulta de dados.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução à Ciência de Dados em Epidemiologia</span>"
    ]
  },
  {
    "objectID": "intro.html#big-data-os-3vs-e-banco-de-dados",
    "href": "intro.html#big-data-os-3vs-e-banco-de-dados",
    "title": "1  Introdução à Ciência de Dados em Epidemiologia",
    "section": "1.9 Big Data os 3Vs e Banco de dados",
    "text": "1.9 Big Data os 3Vs e Banco de dados\nBig Data (Dados em Grande Escala) refere-se a conjuntos de dados extremamente grandes e complexos que excedem a capacidade de processamento e armazenamento tradicionais de sistemas de informação. Esses dados podem ser estruturados (como tabelas em bancos de dados), semi-estruturados (como XML ou JSON) ou não estruturados (como textos, imagens, vídeos e áudios).\nO termo Big Data surgiu com o aumento exponencial da geração de dados no mundo digital, impulsionado por tecnologias como redes sociais, sensores, dispositivos móveis, Internet das Coisas (IoT), e outras fontes de dados em tempo real.\nNo processamento de Big Data são necessárias o uso de ferramentas distribuídas, linguagens de programação e bibliotecas especializadas, bem diferentes do que estamos acostimados\n\nOs 3Vs do Big Data\nOs 3Vs são os três principais características que definem o Big Data. Eles foram introduzidos por Doug Laney, da Gartner, em 2001. Com o tempo, outros “V” foram adicionados, mas os três principais são:\n\nVolume (Volume) • Refere-se à quantidade de dados gerados e acumulados. • Exemplo: Empresas geram petabytes de dados diariamente.\nVelocidade (Velocity) • Refere-se à rapidez com que os dados são gerados e processados. • Exemplo: Dados de sensores, transações em tempo real, redes sociais.\nVariedade (Variety) • Refere-se à diversidade de formatos e tipos de dados. • Exemplo: Dados estruturados (tabelas), semi-estruturados (JSON), e não estruturados (texto, imagens, vídeos).\n\nOutros “3 Vs” (adicionais)\n• Verdade (Veracity): Qualidade e confiabilidade dos dados.\n• Valor (Value): A utilidade dos dados para a tomada de decisões.\n• Complexidade (Complexity): A dificuldade de processar e analisar os dados\n\n1.9.1 Quanto é um grande volume?\n\n\n\n\n\n\n\n\nTamanho\nQuantidade de filmes\nDuração total (1 hora/filme)\n\n\n\n\n1 GB\n1 filme\n~1 hora\n\n\n1 TB\n250 filmes\n~10 dias\n\n\n1 PB\n250.000 filmes\n~28,5 anos\n\n\n\n\n\n1.9.2",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução à Ciência de Dados em Epidemiologia</span>"
    ]
  },
  {
    "objectID": "intro.html#resultadosaplicações",
    "href": "intro.html#resultadosaplicações",
    "title": "1  Introdução à Ciência de Dados em Epidemiologia",
    "section": "1.10 Resultados/Aplicações",
    "text": "1.10 Resultados/Aplicações\n\n1.10.1 o que tem sido publicado?\n\n~600 artigos nos últimos 10 anos buscando pelo termo ‘DATA SCIENCE’[TIAB] AND EPIDEMIOLOGY no pubmed.\n\n \n\n\n\n1.10.2 Tese Mineração de dados na identificação de padrões de mortalidade no Brasil de 1979 a 2013:\nAluno: Davi Barroso Alves\nOrientador: Oswaldo G. Cruz\nBanco em postgresSQL criado a partit do SIM Brasil de 1979 a 2014\nLinhas: ~ 35 milhões colunas:20, espaço em memoria ~ 4GB",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução à Ciência de Dados em Epidemiologia</span>"
    ]
  },
  {
    "objectID": "intro.html#considerações-finais",
    "href": "intro.html#considerações-finais",
    "title": "1  Introdução à Ciência de Dados em Epidemiologia",
    "section": "1.11 Considerações finais",
    "text": "1.11 Considerações finais\n\nTornar-se um cientista de dados é um processo longo e exige dedicação e constante atualização.\nUso de grandes bancos de dados exige uma combinação de software e hardware muito diferente do que os usuários estão acostumados a usar.\nNecessário conhecimento de linguagens de programação e bancos de dados SQL (ou NoSQL).\nNenhum pacote estatístico por si só vai dar conta de criar, gerenciar e analisar dados.\nNecessário conhecimento aprofundado de estatística e ML.\n\n\n“…if the quantity of information is increasing by 2.5 quintillion bytes per day, the amount of useful information almost certainly isn’t. Most of it is just noise, and the noise is increasing faster than the signal. There are so many hypotheses to test, so many data sets to mine but a relatively constant amount of objective truth.”\n📚 Nate Silver, The Signal and the Noise: Why Most Predictions Fail – But Some Don’t\nPenguin Press/Classics 2013",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução à Ciência de Dados em Epidemiologia</span>"
    ]
  },
  {
    "objectID": "sql.html",
    "href": "sql.html",
    "title": "2  Banco de Dados & SQL",
    "section": "",
    "text": "2.1 Introdução\nVocê já parou para pensar na quantidade de dados que geramos no dia-a-dias? Quando postamos uma foto em uma rede social, fazemos uma compra online, assistimos a um vídeo, ou usamos um aplicativo de transporte — tudo isso envolve um banco de dados por trás.\nBancos de dados são essenciais em praticamente todas as áreas onde computadores são utilizados: negócios, engenharia, educação, medicina, e a lista continua. Eles não são apenas um amontoado de informações — para ser um banco de dados, é preciso que esses dados estejam organizados, estruturados e prontos para serem consultados.\nPor isso, um arranjo aleatório de informações não é um banco de dados. E, embora úteis, planilhas não oferecem a mesma estrutura e funcionalidades de um sistema de gerenciamento de banco de dados (SGBD).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Banco de Dados & SQL</span>"
    ]
  },
  {
    "objectID": "sql.html#sistema-gerenciador-de-banco-de-dados",
    "href": "sql.html#sistema-gerenciador-de-banco-de-dados",
    "title": "2  Banco de Dados & SQL",
    "section": "2.2 Sistema Gerenciador de Banco de Dados",
    "text": "2.2 Sistema Gerenciador de Banco de Dados\nUm Sistema Gerenciador de Banco de Dados (SGBD) é uma coleção de programas que habilitam usuários a criar e manter um banco de dados.\nO SGBD é um software de propósito geral, que facilita o processo de definição, construção e manipulação de um banco de dados.\n\nExemplos de SGBDs populares:\n\nMySQL (muito usado em aplicações web)\nPostgreSQL (foco em robustez e recursos avançados)\nSQLite (leve e ideal para aplicações locais ou mobile)\nOracle Database ($$$, amplamente utilizado em grandes corporações)\nDuckdb (moderno, usa filosofia OLAP, crescendo rapidamente)\nSQL Server (desenvolvido pela Microsoft) 🤮🤮🤮",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Banco de Dados & SQL</span>"
    ]
  },
  {
    "objectID": "sql.html#terminologia-básica",
    "href": "sql.html#terminologia-básica",
    "title": "2  Banco de Dados & SQL",
    "section": "2.3 Terminologia Básica",
    "text": "2.3 Terminologia Básica\nAntes de trabalhar com SQL, é importante entender a linguagem básica dos bancos de dados. Esses termos descrevem como as informações são organizadas e manipuladas dentro de um SGBD.\n\n\nCampo: unidade básica de informação mínima com significado (coluna).\nRegistro: conjunto de campos (linha).\nAtributo: área que pode conter um tipo de dados. É a interseção de uma linha com uma coluna.\nTabela: conjunto de registros.\nBanco de Dados (BD): conjunto de tabelas e as formas de manipulação (relacionamentos).\nModelo de Dados: conjunto de conceitos utilizados para descrever a estrutura de um BD, ou seja, os tipos de dados, relacionamentos e restrições sobre estes dados.\nModelo Relacional: representa os dados em um BD por meio de um conjunto de relações.\nChave Primária: coluna que identifica com exclusividade cada linha de um tabela.\nChave Estrangeira: coluna ou conjunto de colunas referente a uma chave primária de uma outra tabela. A partir da chave estrangeira podemos relacionar tabelas (join).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Banco de Dados & SQL</span>"
    ]
  },
  {
    "objectID": "sql.html#modelo-de-dados",
    "href": "sql.html#modelo-de-dados",
    "title": "2  Banco de Dados & SQL",
    "section": "2.4 Modelo de Dados",
    "text": "2.4 Modelo de Dados\nUm Modelo de Dados consiste de um conjunto de conceitos utilizados para descrever a estrutura de um BD, ou seja, os tipos de dados, relacionamentos e restrições sobre estes dados.\nNo Modelo Relacional representa-se os dados em um BD por meio de um conjunto de relações.\nExistem diferentes técnicas e tipos de representação de um modelo de dados, como:\n\nModelo Externo (visões específicas para determinados usuários ou aplicações)\nModelo Conceitual (representação abstrata e independente de tecnologia)\nModelo Entidade-Relacionamento (ER) (foco em entidades, atributos e relacionamentos)\n\nAbaixo temos um exemplo de um diagrama de entidade-relacionamento (ER) para um sistema de prontuário eletrônico:\n\nNo diagrama acima, podemos identificar três elementos principais: entidades, atributos e relações:\n\nEntidades: Representam os objetos principais do sistema (ex: Paciente, Médico, Prontuário Eletrônico).\nAtributos: Informações associadas a cada entidade (ex: ID, sexo, dt nasc para Paciente).\nRelações: Indicam como as entidades estão conectadas (ex: um Paciente pode ter vários registros nos prontuários eletrônicos — relação 1:N; um Médico realiza diversas Consultas (1:N)).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Banco de Dados & SQL</span>"
    ]
  },
  {
    "objectID": "sql.html#introdução-ao-sql",
    "href": "sql.html#introdução-ao-sql",
    "title": "2  Banco de Dados & SQL",
    "section": "3.1 Introdução ao SQL",
    "text": "3.1 Introdução ao SQL\nO SQL (Structured Query Language) foi criado em 1974 por Donald D. Chamberlin e Raymond F. Boyce, pesquisadores da IBM. Originalmente chamado SEQUEL (Structured English Query Language), ele foi desenvolvido para gerenciar e manipular dados de diferentes bancos de dados relacionais existentes na época.\nDesde a década de 1990 tornou-se a linguagem mais popular para acesso a bancos de dados, associado à difusão dos SGBDs relacionais. Neste mesmo ano, a Oracle lançou seu primeiro SGBD totalmente em SQL.\nEm inglês SQL é pronunciado como S-Q-L ou SEQUEL.\nAtualmente SQL é a linguagem padrão para bancos de dados relacionais. Os principais Bancos de Dados Relacionais operam com alguma forma de SQL.\nExistem dois tipos básicos de linguagem:\n\nLinguagem procedural: fornece uma descrição detalhada de COMO uma tarefa é realizada, operando sobre um registro ou uma unidade de dados a cada vez. Ex: C, Java etc…\nLinguagem não procedural: é uma descrição de O QUE se deseja, onde o sistema deverá determinar a forma de fazer. Ex: SQL, HTML.\n\nSQL é uma linguagem não procedural, pois permite expressar consultas e operações em bancos de dados (como seleção, inserção e atualização de dados) sem especificar como o sistema deve executá-las. O usuário declara o que deseja, e o SGBD (Sistema Gerenciador de Banco de Dados) determina como atender a solicitação.\nPara efeitos didáticos, podemos organizar os módulos do SQL nas seguintes categorias:\n\nDefinição (DDL): criação do esquema (tabelas e relacionamentos) que atenderá as necessidades no BD;\nManipulação (DML): inclusão, deleção e modificação dos dados no BD;\nConsulta (DQL): realização de consultas no BD;\nAcessos (DCL): Controlar permissões e acesso definindo quem pode fazer o que com os tabelas e dados;\n\nA tabela abaixo resume as principais subcategorias dos comandos em SQL.\n\n\n\n\n\n\n\n\n\n\nAbreviação\nNome completo\nPropósito\nExemplos de comandos\nDuckDB?\n\n\n\n\nDDL\nData Definition Language\nCriar, modificar ou excluir estruturas de dados\nCREATE, ALTER, DROP\n✅ Sim\n\n\nDML\nData Manipulation Language\nInserir, atualizar, excluir ou selecionar dados\nSELECT, INSERT, UPDATE, DELETE\n✅ Sim\n\n\nDQL\nData Query Language\nRecuperar dados (subcategoria do DML)\nSELECT, FROM, WHERE, JOIN\n✅ Sim\n\n\nDCL\nData Control Language\nControlar permissões e acesso aos dados\nGRANT, REVOKE\n❌ Não",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Banco de Dados & SQL</span>"
    ]
  },
  {
    "objectID": "sql.html#exemplo-de-uma-consulta-dql",
    "href": "sql.html#exemplo-de-uma-consulta-dql",
    "title": "2  Banco de Dados & SQL",
    "section": "3.2 Exemplo de uma consulta (DQL)",
    "text": "3.2 Exemplo de uma consulta (DQL)\nselect nome,idade from pacientes ; \n ​",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Banco de Dados & SQL</span>"
    ]
  },
  {
    "objectID": "sql.html#definição-e-manipulação-ddl-dml",
    "href": "sql.html#definição-e-manipulação-ddl-dml",
    "title": "2  Banco de Dados & SQL",
    "section": "3.3 Definição e Manipulação (DDL & DML)",
    "text": "3.3 Definição e Manipulação (DDL & DML)\nPara iniciar a definição de um BD é necessário conhecer os tipos de dados que o BD manipula. Existe uma grande quantidade de diferentes tipos, mas, de uma maneira geral, os tipos mais frequentes são:\n\nNuméricos: INT, BIGINT, DECIMAL, FLOAT, DOUBLE\nData/Hora: DATE, TIME, DATETIME, TIMESTAMP\nTexto: CHAR, VARCHAR, TEXT, NCHAR, NVARCHAR\nBooleanos: BOOLEAN, BIT\nBinários: BINARY, VARBINARY, BLOB\nIntervalos: INTERVAL (representa um intervalo de tempo)\n\nObs: O dado faltante é chamado NULL.\n\n3.3.1 Criando uma tabela\nPara criar uma tabela será usada a declaração SQL (DDL) create table. De uma maneira geral:\ncreate table &lt;nome_tabela&gt; (\n    campo_1 &lt;tipo de dado&gt;,\n    campo_2 &lt;tipo de dado&gt;, \n    campo_3 &lt;tipo de dados&gt; \n    ) ;\nO ponto e vírgula encerra uma instrução deixando-a pronta para ser executada após o .\nExemplo\nComo seria a criação de uma tabela que armazenaria os nomes e siglas de todas a UF?\n​ CREATE TABLE estados (\n    uf integer,\n    sigla char(2),\n    regiao char(2),\n    nome varchar(50)\n) ; \n\n\n3.3.2 Inserindo dados (populando a tabela)\nInstrução INSERT é usada para inserir dados na tabela.\nINSERT INTO &lt;tabela&gt;[(coluna_1,...,coluna_n)] VALUES (valor_1,...,valor_n);\nExemplo:\nINSERT INTO estados(uf, sigla, regiao, nome)\nVALUES (33, 'RJ', 'SE', 'Rio de Janeiro');\n\nINSERT INTO estados\nVALUES (31, 'MG', 'SE', 'Minas Gerais');\n\nINSERT INTO estados(sigla, nome, uf, regiao)\nVALUES ('SP', 'São Paulo', 35, 'SE');\n\nINSERT INTO estados(nome, regiao, uf, sigla)\nVALUES ('Espirito Santo', 'SE', 32, 'ES');\n\nINSERT INTO estados(sigla, regiao, nome)\nVALUES('GO', 'CO', 'Goias'); -- uf vai receber NULL\nA modificação dos registros é feita usando o comando UPDATE. Por exemplo:\nUPDATE estado SET nome = 'Goiás', uf = 52 WHERE sigla = 'GO';  \n\n\n3.3.3 Apagando os Dados\nInstrução DELETE é usada para remover uma ou registros da tabela, possuindo duas formas básicas:\nDELETE FROM &lt;tabela&gt;;\\\n-- ou \nDELETE FROM &lt;tabela&gt; WHERE &lt;condição&gt;;\nA primeira forma é obrigatória e apaga todos os dados da tabela, enquanto que a segunda possui uma parte opcional, a partir do WHERE, que apaga somente os dados da tabela que atendem a uma condição (ou condições) imposta pela cláusula WHERE. Exemplo:\nDELETE FROM ESTADOS WHERE SIGLA = 'SP'; -- 1 linha deletada\n\nDELETE FROM ESTADOS WHERE NOME = 'ACRE';  -- nenhuma linha encontrada\n\nDELETE FROM ESTADOS; -- todas as  linhas deletadas  ATENÇÃO. \n\n\n3.3.4 Remover uma Tabela\nPara se remover uma tabela deve-se usar o comando DROP TABLE.\nDROP TABLE &lt;nome_da_tabela&gt;;\nPor meio deste comando a tabela deixará de existir neste banco de dados, sendo todas as informações contidas nela vão ser TOTALMENTE apagadas (não tem como desfazer).\nExemplo:\nDROP TABLE ESTADOS; -- Tabela eliminada",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Banco de Dados & SQL</span>"
    ]
  },
  {
    "objectID": "sql.html#consulta-dql",
    "href": "sql.html#consulta-dql",
    "title": "2  Banco de Dados & SQL",
    "section": "3.4 Consulta (DQL)",
    "text": "3.4 Consulta (DQL)\nInstrução SELECT é a essência da linguagem SQL. É por meio dela que se recupera dados de um banco de dados. De modo simples, forma declarativa, está se dizendo ao BD quais informações foram selecionadas para serem recuperadas.\nPode-se dividir esta instrução em quatro partes básicas:\n\nSELECT seguido dos atributos que se deseja ver (obrigatório)\nFROM seguido de onde se obterão os dados (obrigatório)\nWHERE seguido das restrições de recuperação (opcional)\nORDER BY seguido da forma como os dados serão classificados (opcional)\nGROUP BY serve para agregar os dados (opcional)\n\nO símbolo asterisco * significa que todos atributos da relação informada deverão ser recuperados.\nExemplo:\nSELECT * from ESTADO;\n\nselect * FROM estado WHERE SIGLA='GO' OR NOME='Acre' ; \nSELECT será a instrução mais comumente usada na linguagem SQL. Repare que o SQL não é case sensitive como a maioria das linguagens!\nNa cláusula WHERE serão utilizados alguns operadores de comparação e lógicos para que a condição seja especificada. Os operadores logicos usados em SQL são:\n\n\n3.4.1 Criando novas tabelas a partir de tabelas existentes\nÉ muito simples a criação de novas tabelas a partir de outras, basta usar o CREATE TABLE com o resultado de um SELECT.\nCREATE TABLE sudeste AS (SELECT * FROM estados WHERE regiao = 'SE')  ;\n\n\n3.4.2 Joins (Relacionando as Tabelas)\nExistem diversas maneiras de se fazer o relacionamento entre duas tabelas. Os mais comuns são:\n\nINNER JOIN: Este é simples e comumente empregado. Esta query retornará todos os registros da tabela A (esquerda) que têm correspondência com a tabela B (direita). Ou seja, o que existe de comum entre A e B. Podemos escrever este JOIN da seguinte forma:\n\nSELECT * FROM A INNER JOIN B ON A.chave = B.chave\n\nLEFT JOIN: Esta consulta retorna todos os registros da tabela A (esquerda) e o que existir em comum com a tabela B (direita). O código fica da seguinte forma:\n\n    SELECT * FROM A LEFT JOIN B ON A.chave = B.chave\n\nOUTER JOIN: Este relacionamento é conhecido também como FULL OUTER JOIN ou FULL JOIN. Esta consulta retornará todos os registros das duas tabelas e juntando também os registros correspondentes entre as duas tabelas. O que for diferente nas duas tabelas ficara com o valor NULL. O código ficará da seguinte forma:\n\nSELECT * FROM A FULL OUTER JOIN B ON A.chave = B.chave\n Fonte da figura",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Banco de Dados & SQL</span>"
    ]
  },
  {
    "objectID": "sql.html#principais-funções-usadas-em-sql",
    "href": "sql.html#principais-funções-usadas-em-sql",
    "title": "2  Banco de Dados & SQL",
    "section": "3.5 Principais funções usadas em SQL",
    "text": "3.5 Principais funções usadas em SQL\nSeguem algumas funções comumente usadas em SQL, lembre-se que cada versão e SGDB usa dialetos diferentes.\nFunções matemáticas (mais comumente usadas):\n\nABS(n): Devolve o valor absoluto de n.\nCEIL(n): Obtém o valor inteiro imediatamente superior ou igual a n.\nFLOOR(n): Devolve o valor inteiro imediatamente inferior ou igual a n.\nMOD(m, n): Devolve o resto resultante de dividir m entre n.\nPOWER(m, exponente): Calcula a potência de um número.\nROUND(número [, m]): Arredonda números com o número de dígitos de precisão indicados.\nSIGN(valor): Indica o sinal do valor.\nSQRT(n): Devolve a raiz quadrada de n.\nTRUNC(número, [m]): Trunca números para que tenham uma certa quantidade de dígitos de precisão.\n\nFunções Agregadas :\n\nCOUNT ( * | Expressão): Conta o número de ocorrências (A opção “*” conta todos os registros selecionados).\nAVG (n): Calcula o valor médio de n ignorando os valores nulos.\nMAX (expressão): Calcula o máximo.\nMIN (expressão): Calcula o mínimo.\nSUM (expressão): Obtém a soma dos valores da expressão (se houver NULL retorna NULL).\nTOTAL (expressão): soma ignorando NULL.\n\nFunções de caracteres:\n\nCONCAT (str1, str2): Devolve “str1” concatenada com “str2”.\nLOWER (str): Devolve a string em minúsculas.\nUPPER (str): Devolve a string em maiúsculas.\nSUBSTR (str, m [,n]): Obtém parte de uma string.\nLENGTH (str): Devolve o número de caracteres de str.\nREPLACE (str, cadeia_busca [, cadeia_substituição]): Substitui um caractere ou caracteres de uma cadeia com 0 ou mais caracteres.\n\nFunções de datas:\n\nDATE(): Retorna a data ou transforma em data.\nTIME(): Retorna tempo.\nSTRFTIME(fmt, data): Formata a data.\n\nExemplos:\nSELECT count(*) from Estados;\nSELECT now() ; \nSELECT round(355.0 / 113.0,6) ;  -- que número é esse? Teste sem o .0\n\nPara saber mais sobre os tipos de dados e funções do duckDB consulte o Documentação online do duckDB.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Banco de Dados & SQL</span>"
    ]
  },
  {
    "objectID": "sql.html#exercícios-e-prática",
    "href": "sql.html#exercícios-e-prática",
    "title": "2  Banco de Dados & SQL",
    "section": "3.6 Exercícios e prática",
    "text": "3.6 Exercícios e prática\nUsando o duckDB (instalação feita aqui), faça os exercícios a seguir.\nCrie a tabela pessoas:\n-- Criar tabela pessoas\nCREATE TABLE pessoas (\n    cpf BIGINT PRIMARY KEY,\n    nome VARCHAR(100),\n    idade INT,\n    sexo VARCHAR(10),\n    telefone VARCHAR(20),\n    uf int\n);\nInsira dados na tabela pessoas:\n-- Inserir dados de pessoas \nINSERT INTO pessoas (cpf, nome, idade, sexo, telefone,uf)\nVALUES\n(12345678901, 'João Silva', 35, 'Masculino', '(21) 98765-4321',33),\n(23456789012, 'Maria Oliveira', 28, 'Feminino', '(21) 99565-0987',33),\n(34567890123, 'Pedro Almeida', 42, 'Masculino', '(31) 78765-4323', 31),\n(45678901234, 'Ana Costa', 31, 'Feminino', '(11) 98765-4324',35),\n(56789012345, 'Lucas Ferreira', 25, 'Masculino', '(21) 98765-4325',31),\n(67890123456, 'Julia Santos', 29, 'Feminino', '(22) 98765-4326',32),\n(78901234567, 'Ricardo Lima', 50, 'Masculino', '(11) 98765-4327',35),\n(89012345678, 'Fernanda Souza', 33, 'Feminino', '(65) 98765-4328', 52),\n(90123456789, 'Mateus Pereira', 40, 'Masculino', '(21) 98765-4329',33),\n(10234567890, 'Isabela Lima', 27, 'Feminino', '(68) 98765-4330',11);\nCrie a tabela estados:\nCREATE TABLE estados (\n    uf INT PRIMARY KEY,\n    sigla VARCHAR(2),\n    nome VARCHAR(50),\n    regiao VARCHAR(20)\n    );\nInsira dados na tabela estados:\nINSERT INTO estados (uf,sigla,nome,regiao)\nVALUES\n(12,'AC','Acre','Norte'),\n(13,'AM','Amazonas','Norte'),\n(14,'RR','Roraima','Norte'),\n(15,'PA','Pará','Norte'),\n(16,'AP','Amapá','Norte'),\n(17,'TO','Tocantins','Norte'),\n(21,'MA','Maranhão','Nordeste'),\n(22,'PI','Piauí','Nordeste'),\n(23,'CE','Ceará','Nordeste'),\n(24,'RN','Rio Grande do Norte','Nordeste'),\n(25,'PB','Paraíba','Nordeste'),\n(26,'PE','Pernambuco','Nordeste'),\n(27,'AL','Alagoas','Nordeste'),\n(28,'SE','Sergipe','Nordeste'),\n(29,'BA','Bahia','Nordeste'),\n(31,'MG','Minas Gerais','Sudeste'),\n(32,'ES','Espírito Santo','Sudeste'),\n(33,'RJ','Rio de Janeiro','Sudeste'),\n(35,'SP','São Paulo','Sudeste'),\n(41,'PR','Paraná','Sul'),\n(42,'SC','Santa Catarina','Sul'),\n(43,'RS','Rio Grande do Sul','Sul'),\n(50,'MS','Mato Grosso do Sul','Centro-Oeste'),\n(51,'MT','Mato Grosso','Centro-Oeste'),\n(52,'GO','Goiás','Centro-Oeste'),\n(53,'DF','Distrito Federal','Centro-Oeste');\nSiga os passos abaixo:\n\nCertifique-se que as tabelas foram criadas listando o conteúdo de cada uma delas (dica: use SELECT).\nInsira mais um registro a tabela pessoas usando o comando abaixo:\n\ninsert into pessoas values (53565635794, 'Joana da Silva', 33, 'Feminino', '', NULL) ;\n\nO registro foi inserido?\nNULL é igual a ’’ ?\nQual é a idade média das pessoas?\n\n\nEm seguida, vamos adicionar mais um registro:\n\ninsert into pessoas values (78901234567, 'Carlos Valente',53,'Masculino','(21)9765-4320',26) ;\n\nO registro foi inserido?\nEm caso negativo, alguma ideia do motivo ?\nQue campo você deve modificar para poder incluir esse registro?\n\n\nVocê quer saber qual a frequência por sexo na tabela pessoas e usou o código abaixo:\n\nselect sexo, count(sexo) from pessoas \n\nFuncionou? (lembre-se do terminador)\nFuncionou quando adicionou o ‘;’ ? (dica: leia as mensagens de erro com atenção)\n\n\nFaça um join entre as tabelas pessoas e estados.\n\nselect * from pessoas \nleft join estados on pessoas.uf = estados.uf ;\n\nPor que a Isabela Lima e a Joana da Silva ficaram sem UF?\n\n\nFaça agora um INNER JOIN e responda o que muda em relação ao left join.\n\nselect * from pessoas \ninner join estados on pessoas.uf = estados.uf ;\n\nVamos corrigir os problemas nas nossas tabelas:\n\nInsira na tabela estados uma linha para Rondônia (RO) que tem o código 11 e está na região norte.\nModifique o registo de Joana da Silva para a UF 29 (lembre-se de usar where para especificar o que vai ser alterado).\n\n\nSelecione com o mouse para ver a resposta no box abaixo!\n\n\nINSERT INTO estados VALUES (11,‘RO’,‘Rondônia’,‘Norte’) ;\nUPDATE pessoas set uf = 29 where cpf = ‘53565635794’;\n\n\nAgora que nosso dado está devidamente corrigido, vamos criar uma nova tabela chama pessoa_uf que recebe os dados unidos de ambas as tabelas.\ncreate or replace table pessoa_uf as (\nselect * from pessoas as p \nleft join estados as e on p.uf = e.uf );\n-- Aqui poderíamos usar o INNER JOIN pois ficariam iguais!\nAqui um exemplo de como qualquer tabela pode ser facilmente exportada para csv (neste caso csv2, com delimitador “;” , o default é , .\nCOPY pessoa_uf TO 'pessoas.csv' (HEADER, DELIMITER ';');\n\n3.6.1 Importando arquivos remotos\n\n3.6.1.1 Importando CSV com a população do TCU de 2024\n-- lendo a população do TCU por muni do github\ncreate table tcu as (\nselect * from \n'https://raw.githubusercontent.com/ogcruz/curso_DS2025/refs/heads/main/sql/poptcu_2024.csv') ; \n-- cria a a coluna numérica UF \ncreate or replace table tcu as (\n    select substr(MUNI_RES::CHAR,1,2) as uf,*, from tcu) ;\n-- soma a população por UF\ncreate table pop_tcu as (\n    select uf,sum(POPULACAO) as pop2024 from tcu group by uf order by pop2024 desc ) ;\n-- atualiza a tabela estados com a pop do TCU de 2024\ncreate table estados as (\nselect *  from estados e \nleft join pop_tcu p on e.uf = p.uf \norder by e.uf ) ;\n\n\n3.6.1.2 Importando um parquet com ESUS do RJ de 2020\nVamos importar um arquivo paquet com cerca de ~2.56 milhões de registos direto da nuvem da Fiocruz. O tamanho do arquivo em CSV é de cerca de 900MB, já em parquet 140MB (15% do CSV).\n\nParquet é um formato de armazenamento colunar , amplamente utilizado em ecossistemas de big data por ser muito eficiente tanto na leitura quanto na escrita, suporta diferentes algoritmos de compressão de dados e é altamente portável é suportado por ferramentas de Big Data e também em Python, R, Julia, DuckDB entre outros.\n\nPara ler arquivos externos em geral o duckDB já vem com a extensão httpfs pré-instaladas por default\nEssa extensão é a responsável por ler e escrever remotamente, ela dá suporte a leitura por http e https e leitura e escrita usando S3 (protocolo da Amazon Clould)\nCaso você tenha algum problema aqui está o código para instalar, mas não deve ser necessário fazer essa instalação.\nINSTALL httpfs;\nLOAD httpfs;\nAntes de começar um projeto maior no duckDB é recomendável ajustar o tamanho da memória e número de núcleos da CPU que o duckDB poderá usar.\nVocê deve ajustar esses parâmetros conforme a quantidade de RAM do seu sistema e a quantidade de núcleos (cores) da sua CPU. Nas CPU modernas um núcleo roda 2 tarefas (Threads) por núcleo. Para a quantidade de memoria um parâmetro 4GB é razoável, mas a depender do dado pode ser necessário aumentar esse valor. Já para a CPU aposta segura é que toda a maquina é no mínimo um dual core atualmente.\nMas lembre-se que se você for usar também o R ou alguma outra linguagem / ‘software’ simultaneamente você deve ajustar esses valores.\nSET memory_limit = '4GB'; -- ajuste de acordo \nSET threads TO 4; -- toda cpu moderna tem no minimo 4 threads\nCREATE TABLE esus as (\n    SELECT  * \nFROM 'https://own.procc.fiocruz.br/DS2025/esus_rj.parquet' \n); \n-- cerca de 15 segundos \nDESCRIBE esus ; \nNote que a maioria das variáveis foi importada como VARCHAR somente index e testeSorologico são inteiros e idade é um double\nselect * from esus limit 100; \n\n\n\n3.6.2 Limpando a tabela do ESUS\nVamos proceder algumas transformações para transformar os campos dataNotificacao, dataInicioSintomas e dataTeste de texto para data. Repare que eles são, na verdade, do tipo timestamps assim temos de fazer uma dupla transformação, de texto para timestamp e depois de timestamp para date. Vamos aproveitar e colocar um filtro de data de primeiro sintomas maior ou igual a 1 de março de 2020 e salvar em uma tabela esus2.\ncreate or replace table esus2 as ( \nselect  index, \n        cast(dataNotificacao::TIMESTAMP as date) as dt_not , --converte pra data\n        cast(dataInicioSintomas::TIMESTAMP as date) as dt_sint,\n        cast(dataTeste::TIMESTAMP as date) as dt_test,\n        * EXCLUDE (dataNotificacao,dataInicioSintomas,dataTeste,index_1) --não há virgula!!!\n        from esus where dt_sint &gt;= '2020-03-01'\n        ) ;\nPodemos visualizar a nova tabela usando o comando select … como já fizemos antes no, entretanto existe um atalho para isso. use o painel a direita do duckDB UI para explorar a nova tabela.\nfrom esus2;\n\n\n3.6.3 Listando\nComo fazer uma listagem dos nomes dos município do estado do Rio de Janeiro que constam da tabela esus2 ?\nselect distinct municipio \nfrom esus2 \nwhere estado = 'RIO DE JANEIRO' \norder by municipio;\nNote que no comando acima temo o uso do DISTINCT é usado para eliminar registros duplicados de um resultado da consulta. O uso de DISTINCT pode impactar a performance, especialmente em tabelas grandes, pois o SQL precisa ordenar e remover duplicados.\ncaso eu queira saber quantos são:\nselect count(distinct municipio) \nfrom esus2;",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Banco de Dados & SQL</span>"
    ]
  },
  {
    "objectID": "sql.html#exemplos-de-tabulações",
    "href": "sql.html#exemplos-de-tabulações",
    "title": "2  Banco de Dados & SQL",
    "section": "3.7 Exemplos de tabulações",
    "text": "3.7 Exemplos de tabulações\n\n3.7.1 Tabulações simples\nTabulações simples podem ser obtidas usando alguma função de agregação como count,sum,avg e combinadas com group by e order by\nselect racaCor,count(racaCor) as freq\nfrom esus2\ngroup by racaCor \norder by freq desc;\n\n\n3.7.2 Tabulações avançadas\nTabulações avançadas como tabelas cruzadas necessitam de códigos mais complexos usando CASE WHEN , PIVOT , WITH , subquery etc…\nAbaixo um exemplo geral de como fazer uma tabela cruzada num SQL qualquer(a versão ISO-92 foi uma das grandes revisões, a mais atual é a ISO-2023).\nSELECT \n    classificacaoFinal,\n    SUM(CASE WHEN sexo = 'Masculino' THEN 1 ELSE 0 END) AS Masculino,\n    SUM(CASE WHEN sexo = 'Feminino' THEN 1 ELSE 0 END) AS Feminino,\n    Masculino+Feminino as Total\nFROM esus2\nGROUP BY 1\nORDER BY 2 ;\nUsando o dialeto SQL do duckDB (o PostgreSQL é 98% semelhante ) podemos usar o comando PIVOT que reorganiza dados de uma tabela, transformando valores de uma coluna em nomes de colunas e aplicando uma função de agregação para calcular os valores correspondentes.\nNo uso a seguir não temos a categoria Total que teria de ser implementada de outra forma. Por outro lado, o comando é mais enxuto e objetivo.\nSELECT * FROM (\n    SELECT classificacaoFinal, sexo\n    FROM esus2\n) PIVOT (COUNT(*) FOR sexo IN ('Masculino', 'Feminino'))\nORDER BY 2 ;\nE se quiséssemos adicionar uma linha final totalizando todas as classificações?\nSELECT \n    'Total' AS classificacaoFinal,\n    SUM(CASE WHEN sexo = 'Masculino' THEN 1 ELSE 0 END) AS Masculino,\n    SUM(CASE WHEN sexo = 'Feminino' THEN 1 ELSE 0 END) AS Feminino,\n    Masculino + Feminino AS Total\nFROM esus2\nORDER BY   Total\nE como colocar tudo isso junto numa mesma query?\nSELECT \n    classificacaoFinal,\n    SUM(CASE WHEN sexo = 'Masculino' THEN 1 ELSE 0 END) AS Masculino,\n    SUM(CASE WHEN sexo = 'Feminino' THEN 1 ELSE 0 END) AS Feminino,\n     Masculino + Feminino AS Total\nFROM esus2\nGROUP BY 1\nUNION ALL\nSELECT \n    'Total' AS classificacaoFinal,\n    SUM(CASE WHEN sexo = 'Masculino' THEN 1 ELSE 0 END) AS Masculino,\n    SUM(CASE WHEN sexo = 'Feminino' THEN 1 ELSE 0 END) AS Feminino,\n    Masculino + Feminino AS Total\nFROM esus2\nORDER BY   Total\nSe você fez tudo corretamente deve chegar a uma tabela como essa:\n\n\n3.7.2.1 Common Table Expression (CTE)\nUma CTE é uma subconsulta que pode ser nomeada e usada dentro de uma consulta SQL subsequente. Ela é definida com a cláusula WITH e serve para organizar e simplificar consultas complexas, tornando o código mais legível e fácil de manter.\nWITH filtro_obito as (\n   select * \n   from esus2\n   where evolucaoCaso = 'Óbito' and regexp_matches(classificacaoFinal,'^Confirmado','i') \n) \nSELECT sexo, \n    count(*) as freq , \n    ROUND((freq * 100.0 / (SELECT COUNT(*) FROM filtro_obito )), 2) AS percentual \nfrom filtro_obito \ngroup by 1;\nNote o uso de uma das funções de expressão regular (regexp_matches) para selecionar todos que começam pela palavra “Confirmado” . Existem muitas funções que lidam com textos e o uso delas em especial das expressões regulares nos possibilita uma enorme gama de soluções rápidas e simples.\nVamos ver mais sobre esse assunto na próxima aula.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Banco de Dados & SQL</span>"
    ]
  },
  {
    "objectID": "sql.html#conclusão",
    "href": "sql.html#conclusão",
    "title": "2  Banco de Dados & SQL",
    "section": "3.8 Conclusão",
    "text": "3.8 Conclusão\n\nConclusões da aula de SQL\nEsses são apenas os primeiros passos no SQL há muito mais a ser aprendido como vocês viram ele é flexível e relativamente simples ainda que tenhamos de escrever um código um tanto mais longo se comparado ao R.\n\nSQL é linguagem nativa para manipulação e consulta de dados em bancos relacionais, enquanto R e Python são linguagens gerais\nSQL é otimizado para consultas em grandes volumes de dados armazenados em bancos, com suporte a índices, otimização de consultas e ACID (atomicidade, consistência, isolamento, duração).\nSQL é padrão para interagir com bancos de dados, facilitando a manipulação de dados diretamente no armazenamento, sem precisar carregar todos os dados em memória (como em R/Python)\nSQL é mais eficiente para consultas distribuídas (ex: clusters, bancos em nuvem)\n\nOnde aprender mais sobre SQL:\nExistem muitos tutoriais em plataformas de cursos (a maioria não é gratuita) , livros, capítulos de livros , apostilas e vídeos no youtube.\nAbaixo alguns recursos gratuitos:\n\nTutorial online W3 - com exercícios interativos\nSQL Tutorial in PDF\nDocumentação online do duckDB",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Banco de Dados & SQL</span>"
    ]
  },
  {
    "objectID": "tidy_import.html",
    "href": "tidy_import.html",
    "title": "3  Importando e arrumando dados",
    "section": "",
    "text": "3.1 O universo tidy: tidyverse\nO tidyverse revolucionou como os cientistas de dados trabalham usando o R. Ele promove uma filosofia de programação clara, consistente e fácil de aprender, o que o tornou extremamente popular e influente no ecossistema do R e na ciência de dados.\nO tidyverse não é um único pacote, mas sim um metapacote, ou seja, uma coleção de pacotes que compartilham uma filosofia comum, formando um framework completo para a ciência de dados em R construído sobre três pilares essenciais:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Importando e arrumando dados</span>"
    ]
  },
  {
    "objectID": "tidy_import.html#o-universo-tidy-tidyverse",
    "href": "tidy_import.html#o-universo-tidy-tidyverse",
    "title": "3  Importando e arrumando dados",
    "section": "",
    "text": "a filosofia dos dados arrumados,\no poder da programação funcional e\na clareza do operador pipe.\n\n\n3.1.1 A filosofia dos dados arrumados (tidy data)\nNo coração do tidyverse está o conceito de tidy data, que defende uma estrutura de dados consistente e universal:\n\nCada variável deve ter sua própria coluna.\nCada observação (ou caso) deve ter sua própria linha.\nCada unidade de medida deve ter sua própria tabela. (Se você tem dados sobre diferentes tipos de coisas, essas coisas deveriam estar em tabelas separadas.)\n\nAo organizar os dados dessa forma, o tidyverse garante que todas as suas ferramentas funcionem perfeitamente juntas, simplificando tarefas de manipulação e análise que seriam complexas de outra forma.\n\n\n3.1.2 O poder da programação funcional\nO tidyverse faz uso extensivo de funções que executam uma única tarefa bem definida. Essa abordagem, conhecida como programação funcional, promove a reutilização do código e o torna mais fácil de ler e entender. Em vez de escrever loops complexos, você pode usar funções concisas para aplicar operações a grupos de dados, o que torna seu código mais robusto e menos propenso a erros.\n\n\n3.1.3 A clareza do operador pipe (%&gt;% ou |&gt; no R 4.1+)\nO operador pipe é o que realmente torna o código tidyverse tão intuitivo. Ele permite que você encaminhe o resultado de uma função diretamente para a próxima, criando uma sequência lógica de operações. Isso melhora drasticamente a legibilidade e a manutenção do seu código.\ndados |&gt;                        # Começa com o objeto 'dados'\n  select(coluna1, coluna2) |&gt;   # Usa select() para manter apenas as variáveis coluna1 e coluna2\n  filter(coluna1 == 'exemplo')  # Depois, usa filter() para manter apenas as linhas onde coluna1 == 'exemplo'\nO operador pipe pega o resultado da linha anterior e passa como primeiro argumento para a função da próxima linha. Assim:\nfilter(select(dados, coluna1, coluna2), coluna1 == 'exemplo')\nMas com o pipe, fica mais legível e mais próximo da lógica humana: “pegue os dados → selecione colunas → filtre as linhas”.\nJuntos, esses três pilares criam uma abordagem de ciência de dados que é consistente, poderosa e, acima de tudo, agradável de se trabalhar.\n\n\n3.1.4 Principais pacotes do tidyverse\n\n\nreadr: Para importação de dados. O readr oferece uma maneira rápida e robusta de ler arquivos de texto retangular, como CSV e TSV. Ele é mais rápido que as funções base do R e importa os dados diretamente no formato tibble, facilitando a próxima etapa de análise.\ntibble: Para criar (tibble()) e transformar (as_tibble()) data frames no formato tibble, um formato mais moderno, eficiente e legível. Melhora a exibição dos dados (glimpse()) e mantém tipos de dados consistentes durante operações.\ndplyr: Para manipulação de dados. Com ele, você pode filtrar linhas (filter()), selecionar colunas (select()), reorganizar dados (arrange()), e resumir dados (summarise()), tudo de maneira clara e legível. Ele substitui a necessidade de escrever códigos complexos e muitas vezes confusos para essas tarefas.\ntidyr: Para arrumar dados. Este pacote é fundamental para transformar dados “bagunçados” em “arrumados” (tidy data). Funções como pivot_wider() e pivot_longer() são essenciais para remodelar tabelas de forma eficiente, uma tarefa comum e muitas vezes difícil.\nggplot2: Para visualização de dados. Considerado um dos melhores pacotes de gráficos em R, o ggplot2 permite criar gráficos impressionantes e personalizados. Ele se baseia na “Gramática dos Gráficos”, onde você constrói seu gráfico camada por camada, dando controle total sobre cada elemento visual.\npurrr: Para programação funcional. O purrr ajuda a trabalhar com listas e vetores, substituindo loops for por funções mais concisas e expressivas como map(), tornando seu código mais fácil de ler e menos propenso a erros.\nstringr: Para manipulação de strings. Este pacote simplifica as operações com texto, como detecção, extração e substituição de padrões.\nlubridate: Para manipulação de datas e horas de forma mais acessível e eficiente.\nforcats: Para trabalhar com variáveis categóricas (fatores).\n\n\n\n3.1.5 Importância no ecossistema R\n\nSimplificação da manipulação e análises de dados: o tidyverse reduziu drasticamente a complexidade da manipulação e análises de dados no R. Antes do tidyverse, muitas tarefas exigiam códigos mais longos, complexos e difíceis de entender.\nMelhoria da legibilidade do código: A filosofia do tidyverse enfatiza a legibilidade do código, o que facilita a colaboração e a manutenção de projetos.\nAumento da produtividade: A sintaxe clara e consistente do tidyverse permite que os usuários realizem tarefas mais rapidamente.\nAmpliação da acessibilidade: O tidyverse tornou a manipulação e análise de dados em R mais acessíveis para pessoas com diferentes níveis de experiência.\nInfluência em outras bibliotecas: A filosofia do tidyverse influenciou o design de outras bibliotecas R, promovendo a consistência e a clareza.\nComunidade ativa: O tidyverse possui uma comunidade grande e ativa que oferece suporte, compartilha conhecimento e desenvolve novas ferramentas.\n\n\n\n3.1.6 A criação do tidyverse\nA criação do tidyverse foi um processo contínuo de desenvolvimento, não um evento único. Sua filosofia central sempre foi tornar a manipulação e visualização de dados mais organizada, padronizada e acessível.\nO projeto foi iniciado por Hadley Wickham, que, na época, era professor adjunto de estatística na Universidade de Auckland, Nova Zelândia. Ele já estava desenvolvendo pacotes influentes como o ggplot2 para visualização e o dplyr para manipulação de dados.\nMais tarde, Wickham se juntou à empresa RStudio como Cientista chefe. O empresa RStudio posteriormente mudou de nome para Posit e se tornou o lar do tidyverse, apoiando e impulsionando seu desenvolvimento e popularidade na comunidade de R.\n\nVamos falar mais sobre o tidyverse e ver exemplos ao logo do curso. Algumas vezes vamos usar funções do módulo base do R, pois podem ser mais rápidas ou para ilustrar algum ponto. Lembre-se que para você usar plenamente o R não basta conhecer o tidyverse, também precisa saber usar bem o módulo base!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Importando e arrumando dados</span>"
    ]
  },
  {
    "objectID": "tidy_import.html#importando-dados",
    "href": "tidy_import.html#importando-dados",
    "title": "3  Importando e arrumando dados",
    "section": "3.2 Importando dados",
    "text": "3.2 Importando dados\nA importação de dados é um dos primeiros e mais cruciais passos em qualquer projeto de Ciência de Dados. A qualidade e a eficiência desse processo impactam diretamente a qualidade da análise e dos resultados. Aqui vamos falar da importação de dados, cobrindo principais formatos, funções de leitura, melhores práticas e considerações e observações:\nPodemos ver no diagrama abaixo (adaptado de Wickham & Grolemund, 2017), as principais etapas que veremos nesse curso e como tudo começa com a Importação dos dados.\n\nExistem diversas maneiras de importar o dados em R. As principais são:\n\nImportando de arquivos locais,\nImportação via WEB (APIs, http,ftp, Scraping),\nAcesso a Banco de Dados.\n\nVamos olhar cada uma delas.\n\n3.2.1 Importando de arquivos locais\nEssa é a maneira que todos estão mais acostumados. Em geral se importa um arquivo texto do tipo CSV ou ainda TXT com algum tipo de delimitador, que pode ser espaço,  ou qualquer outro.\nComumente se utiliza a função read.table() ou alguma de suas variantes como read.csv() ou read.csv2().\nAlguns exemplos:\ndados &lt;- read.csv2(\"planilha.csv\")  \n# ou \ndados2  &lt;- read.table(\"arquivo.txt\", header=F, sep=\";\")\n\n3.2.1.1 Benchmarks e cuidados na importação\nUm rápido benchmark compara o tempo que cada uma das funções em R (em diferentes pacotes), Python e duckDB demorou para ler um arquivo CSV de 1.5 Gb contendo 3.297.660 linhas e 123 colunas.\n\n\n\nfunção\npacote\ntempo\ntam\nstatus\n\n\n\n\nread.csv\nR base\n28.95s\n1.592.365\nwarning\n\n\nread_csv\npandas (python)\n21.40s\n3.254.452\nwarning\n\n\nread_csv\nreadr (tidyverse)\n16.34s\n3.253.190\nwarning\n\n\nvroom\nvroom\n8.76s\n3.253.190\nwarning\n\n\nfread\ndata.table\n4.28s\n—\nErro\n\n\nread_csv\nduckDB\n1.87s\n1.620.028\nwarning\n\n\n\nNote que nenhuma das funções leu todo o arquivo! A read_csv do pandas, a read_csv do readr e a vroom do pacote de mesmo nome foram as que mais se aproximaram de ler todo o arquivo. A função read.csv do módulo base não só foi a mais lenta como leu cerca da metade do arquivo demorando quase 29 segundos, já a função read_csv do duckDB também leu apenas ~1.6 milhões de registos em menos de 2 segundos.\n\nObservação importante:\nComo vimos, nem sempre a importação ocorre da maneira que imaginamos e poucos se dão ao trabalho de ler as mensagens de aviso. o formato CSV, que é apenas um texto separado por um caractere (em geral uma “,” ou “;”), pode facilmente ser comprometido. Basta qualquer linha que tenha um desses caracteres, (por exemplo, um endereço ter um desses delimitadores e não estar entre aspas) para gerar problemas.\nAo exportar um arquivo para CSV lembre-se sempre de configurar para que os campos textos sejam incluídos entre aspas! (Nem sempre resolve, pois pode ter uma ou mais aspas soltas, o que também vai corromper o arquivo CSV).\n\nTambém algumas vezes podemos importar algum formato oriundo de outro software, como o SPSS, STATA, SAS, ou até mesmo o extinto formato DBF. No R é necessário chamar o pacote haven, parte do tidyverse, ou a foreign do R base.\nNo pacote haven você encontra as funções read_sas(), read_sav() e read_dta() para importar respectivamente do SAS, SPSS e STATA.\nPor conta da influência dos sistemas do DATASUS na saúde, até hoje temos de lidar com um formato legado chamado DBASE que possui a extensão .DBF ou ainda .DBC (DBase compactado, formato criado pelo DATASUS).\nPara importar um DBF com o pacote foreign:\nlibrary(foreign)\ndados &lt;- read.dbf(\"datasus.dbf\", as.is=TRUE) \n## as.is = TRUE não transforma as strings em fatores\nJá para o DBC vamos precisar do pacote read.dbc, desenvolvido por Daniela Petruzalek. Não está disponível no CRAN (o repositório oficial do R) no momento deste curso. Por isso, para instalá-lo, você precisará seguir um procedimento alternativo. Vamos instalar do repositório no github da autora e, para isso, precisamos do pacote devtools instalado.\ndevtools::install_github(\"danicat/read.dbc\")\nlibrary(read.dbc)\ndados &lt;- read.dbc(\"datasus.DBC\", as.is=TRUE)  ## DBC é um DBF mas compactado\n\n\n\n3.2.2 Importando via WEB\nUma das facilidades do R que é pouco explorada é a possibilidade de ler arquivo através da internet. Algumas das funções podem acessar diretamente e referenciar um link remoto (URL).\nVeja o exemplo abaixo:\nlibrary(tidyverse)\ntab &lt;- read_csv('https://raw.githubusercontent.com/wcota/covid19br/refs/heads/master/cases-brazil-cities.csv')\nhead(tab)\n\nTendo o endereço da URL, em apenas um comando podemos importar o conteúdo de uma tabela ou dados localizados numa página.\nEssa página pode ser num local web usando o protocolo https, http ou, por exemplo, ftp.\npaises &lt;- read_csv('http://157.86.198.20/dados/paises.csv')\nMas nem sempre é tão simples assim. A segurança na web evoluiu significativamente, e os navegadores modernos e até mesmo as ferramentas que fazem requisições HTTP estão se tornando cada vez mais rigorosas na validação de certificados SSL/TLS.\nVamos baixar uma lista de preços de medicamentos do site da ANVISA:\nurl &lt;- \"https://dados.anvisa.gov.br/dados/TA_PRECOS_MEDICAMENTOS.csv\"\nread_csv(url)\n\nAinda é possível acessar a tabela, mas vamos precisar de um pacote chamado httr2, usado para “raspagem” (scrapper) de dados da web. Nesse pacote podemos dizer para que a autenticação seja ignorada desativando as opções (ssl_verifypeer e ssl_verifyhost). Assim, nem a conexão e nem o site serão validados.\nlibrary(httr2)\n\nreq &lt;- request(url) |&gt;\n  req_options(ssl_verifypeer = 0) |&gt;\n  req_options(ssl_verifyhost = 0)\no comando acima montou uma requisição “req” que vai ser executada a seguir pela função req_perform(). Somente o dado será acessado.\nA função resp_status() vai nos retornar o código web resultante da operação. Caso seja 200, a operação foi realizada com sucesso. Caso a página ou a URL esteja mal formatada, teremos como resultado o código 404, também conhecido como “Página Não Encontrada”.\nresposta &lt;- req_perform(req)\n\nresp_status(resposta)  ## ou resposta$status_code \n# [1] 200\nRecebemos o código 200, indicando que a função foi bem sucedida, mas não termina aqui ainda. É necessário extrair o dado do objeto resposta para poder importá-lo usando uma função como read_csv(). Nesse caso vamos usar read_delim().\nA função resp_body_raw() é a que vai extrair os dados do objeto resposta e passar a função que vai realizar a importação. Vamos informar a função read_delim() que o delimitador é “;” e também que o code_page usados para a acentuação é o Latin-1 (o mesmo que ISO-8859-1 / Windows-1252), usado, em geral, pelo Windows. Já o Linux e o MAC utilizam UTF-8 . Assim, se o seu computador roda Linux, é preciso informar isso. Caso você use Windows, não é necessário, mas também não retorna erro se usar.\nanvisa &lt;- read_delim(file=resp_body_raw(resposta), delim = ';',\nlocale=locale(encoding='latin1'))\n\nanvisa\n\nComo podem ver, obtivemos 53436 linhas e 16 colunas.\nPodemos agora perguntar, por exemplo, quantos medicamentos estão registados que usam acido acetilsalicílico nesta base.\nanvisa |&gt; \n  filter(str_detect(DS_SUBSTANCIA, 'acetilsalicílico')) |&gt;\n  nrow()\n# [1] 281\nNão se preocupe que vamos ver em maiores detalhes essas funções acima nas próximas aulas.\n\n\n3.2.3 Acessando o feed de notícias RSS do Ministério da Saúde\nVamos acessar o RSS que é o serviço de notícias do MS. O formato dos dados está em XML.\nFormatos XML e JSON.\nlibrary(xml2)\n\n# URL do feed RSS\nrss_url &lt;- \"https://www.gov.br/aids/pt-br/assuntos/noticias/site-feed/RSS\"\n# Leia o conteúdo do XML da URL\nxml_data &lt;- read_xml(rss_url)\n# Defina os namespaces usados no arquivo XML\nnamespaces &lt;- c(\n  rdf = \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\",\n  d1 = \"http://purl.org/rss/1.0/\"\n)\n# Encontre todos os nós &lt;item&gt; usando o namespace padrão\nitems &lt;- xml_find_all(xml_data, \"//d1:item\", ns = namespaces)\n# Extraia os títulos de cada &lt;item&gt;\n# O título está na tag 'title'\ntitulo &lt;- xml_text(xml_find_all(items, \"d1:title\", ns = namespaces))\n# Extraia os links de cada &lt;item&gt;\n# O link da notícia está na tag 'link'\nlinks &lt;- xml_text(xml_find_all(items, \"d1:link\", ns = namespaces))\n# o mesmo para o corpo\ncorpo &lt;- xml_text(xml_find_all(items, \"d1:description\", ns = namespaces))\n# Crie um data frame para organizar as informações\nnoticias_df &lt;- data.frame(\n  Titulo = titulo,\n  Corpo = corpo,\n  Link = links,\n  stringsAsFactors = FALSE\n)\nprint(noticias_df)\n\n\n3.2.4 Usando uma API para acessar Dados\nO que é um API?\nUma Application Programming Interface (API), em português, Interface de Programação de Aplicações, é um conjunto de definições, protocolos e ferramentas para a criação de software e acesso a dados. Pense nela como um meio que permite a comunicação entre diferentes programas, ou mesmo entre um software e um hardware. As APIs são amplamente utilizadas no desenvolvimento de software para integrar sistemas e funcionalidades.\nNeste curso, vamos nos concentrar no uso de APIs Web, também conhecidas como Web Services ou Web APIs. Como você pode ver no gráfico abaixo, o uso delas tem crescido significativamente, impulsionado pela popularização de repositórios de dados e pela crescente adoção de políticas de dados abertos por governos e instituições.\n\nPara entendermos melhor como funciona uma API, imagine que você é o cliente (software) e vai a um restaurante, escolhe um prato (dados) que consta de um menu (lista de dados disponíveis) faz o pedido ao garçon (API) que leva o seu pedido a cozinha (servidor) onde o chefe (SGBD) vai preparar o seu prato. Uma vez pronto o seu prato é levado a você pelo garçon (API). Em alguns casos o menu pode deixar você customizar alguns aspetos do prato(dado), por exemplo, o ponto do filé, que deve ser especificado ao garçon (API) e preparado pelo chefe dentro de parâmetros pre estabelecidos.\n\n\n3.2.5 API do INFODENGUE\nO InfoDengue é um sistema de alerta para arboviroses baseado em dados híbridos gerados por meio da análise integrada de dados minerados a partir da web social e de dados climáticos e epidemiológicos.\nEm 2021, o sistema ganhou amplitude nacional com o apoio do Ministério da Saúde realizando análises em nível estadual. Com isso, mais secretarias passaram a receber semanalmente os boletins do InfoDengue. Implementado em 2015, o sistema foi desenvolvido por pesquisadores do Programa de Computação Científica (Fundação Oswaldo Cruz, RJ) e da Escola de Matemática Aplicada (Fundação Getúlio Vargas) com a forte colaboração da Secretaria Municipal de Saúde do Rio de Janeiro\nAs tabelas geradas pelo Infodengue contem dados agregados por semana provenientes de diferentes fontes. Elas podem ser consultadas via formulário, ou diretamente do R, por meio de uma consulta à API.\nEssa funcionalidade está disponível por meio da URL:\nhttps://info.dengue.mat.br/api/alertcity?PARAMETROS_DA_CONSULTA\nOnde PARAMETROS_DA_CONSULTA deve conter os parâmetros:\ngeocode: código IBGE da cidade\ndisease: tipo de doença a ser consultado (dengue|chikungunya|zika)\nformat: formato de saída dos dados (str:json|csv)\new_start: semana epidemiológica de início da consulta (int:1-53)\new_end: semana epidemiológica de término da consulta (int:1-53)\ney_start: ano de início da consulta (int:0-9999)\ney_end: ano de término da consulta (int:0-9999)\nATENÇÃO Todos os parâmetros acima mencionados são obrigatórios para a consulta.\nRetorno da API do Infodengue\nurl &lt;- \"https://info.dengue.mat.br/api/alertcity?\"\ngeocode &lt;- 4108304\ndisease &lt;- \"dengue\"\nformat &lt;- \"csv\"\new_start &lt;- 1\new_end &lt;- 52\ney_start &lt;- 2015\ney_end &lt;- 2025\nEm seguida precisamos concatenar a string de consulta a API nossa URL a cada um dos parâmetros separados por &\ncons &lt;- paste0(url,\"geocode=\",geocode,\"&disease=\",disease,\"&format=\",\nformat,\"&ew_start=\",ew_start,\"&ew_end=\",ew_end,\"&ey_start=\",ey_start,\n\"&ey_end=\",ey_end)\nFormando a seguinte URL que fará a consulta ao site do infodengue:\n\nhttps://info.dengue.mat.br/api/alertcity?geocode=4108304&disease=dengue&format=csv &ew_start=1&ew_end=53&ey_start=2015&ey_end=2025\n\nExecutando o código abaixo vamos receber o resultado da nossa consulta.\ndengue_foz &lt;- read_csv(cons)\nnrow(dengue_foz)\nglimpse(dengue_foz)\n\n\n3.2.6 API Open-Meteor\nA API do Open-Meteo é uma interface de programação de aplicativos que oferece dados de previsão do tempo e climáticos de forma gratuita e com código aberto para uso não comercial e para desenvolvedores e não requer nem cadastro e nem chave para poder acessar.\nPrevisão para o Rio\nmeteor &lt;- 'https://api.open-meteo.com/v1/forecast?latitude=-22.875&longitude=-43.25&timezone=America/Sao_Paulo,&current=temperature_2m,relative_humidity_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m'\n\nprevisao &lt;- jsonlite::read_json(meteor,simplifyVector = T)\n\nprevisao\n\n\n\n3.2.7 Importando diretamente do DATASUS\nO datasus além dos conhecidos serviços disponibilizados pela internet também, há muitos anos, possui um servidor FTP de onde podemos descarregar diretamente os dados.\nPodemos usar funções do R para fazer o download de um arquivo usando o protocolo FTP por exemplo:\ndownload.file(\"ftp://ftp.datasus.gov.br/territorio/tabelas/04-base_territorial_abr25.zip\",\"base_terr.zip\")\nVamos agora “descompactar” o arquivo ZIP. Em primeiro lugar vamos selecionar um diretório temporário que está sendo usado usando pelo R no seu sistema operacional. ​\nloc &lt;- tempdir()\nloc\nEm seguida vamos mandar extrair o conteúdo do ZIP nesse diretório temporário e usar o dir desse local para verificar se o conteúdo foi expandido.\nunzip(\"base_terr.zip\",exdir=loc)\ndir(loc)\na partir dai você poderia importar um ou mais dos arquivos que foram expandidos.\narq &lt;- paste0(loc,\"/\",\"tb_uf.csv\")\ntb_uf &lt;- read_csv2(arq,locale = locale(encoding = 'latin1'))\n\n3.2.7.1 Exemplo: Importando SINASC do Acre diretamente do DATASUS\naté aqui você já viu todos os elementos necessários para fazer a importação de dados direto do FTP do DATASUS, o que precisamos é um workflow, ou seja, um plano, para fazer a importação.\n\n\nem primeiro lugar vamos usar o pacote RCurl que permite acesso a paginas http , https, ftp etc…\nlibrary(RCurl)\nurl &lt;- 'ftp://ftp.datasus.gov.br/dissemin/publicos/SINASC/NOV/DNRES/'\nlista &lt;- getURL(url)\nvamos inspecionar o objeto lista. como vocês podem observar esse arquivo precisa ainda ser transformado para ser usado.\nlista &lt;- str_split(lista,'\\n')\napós pedirmos ao R para inserir uma quebra apos o caractere ‘***’ agora fica mais fácil de entender a listagem que obtivemos!\nvamos selecionar como UF o Acre pois o tamanho dos arquivos é pequeno e a rede da ENSP em geral não é rápida.\nl2 &lt;- str_extract(lista[[1]],'(DNAC\\\\d+.[dbc|DBC]{3})')## DN do ACRE\nAqui vamos fazer um parêntesis para falar de um recurso extremamente poderoso, as expressões regulares conhecidas por REGEX\nAgora que vocês já aprenderam um pouco sobre as REGEX vamos selecionar as linhas que possuem as DN do ACRE\nde uma maneira simples selecionamos todas que possuem o padrão DNAC no nome seguido de um ou mais numeros (\\d+) e terminado em .dbc ou .DBC.\nOnde não houve match a função retorna NA, assim vamos nos livra deles!\n    l3 &lt;- as.vector(na.exclude(l2))\nVerifique o objeto l3 para se certificar que tenhamos somente um vetor de caracteres com o nome dos arquivos! ​ Aqui temos 27 nomes de arquivos , vamos tentar trazer todos.\nComo fazer isso?\nVamos precisar criar uma nova função que possa fazer o download dos arquivos DBC e posteriormente importá-los e finalmente juntar todos. Existem diversas maneiras de se fazer isso, mas vamos usar filosofia do tideverse executando a função map do pacote purr.\nle_dbc &lt;- function(arq) {\n  \n  if(!require(read.dbc))  stop(\"o pacote read.dbc deve estar instalado\") # verifica se existe o pacote e carrega \n  origem &lt;- paste0(url,arq)\n  destino &lt;- paste0( tempdir(),'/temp.dbc')\n  \n  download.file(origem,destino,mode=\"wb\")\n  \n  temp &lt;- read.dbc(destino,as.is=T) |&gt; \n    tibble()\n  \n  file.remove(destino)\n  \n  return(temp)\n}\nVamos testar nossa função lendo apenas um arquivo, por exemplo o DNAC2020.dbc que está na posição 25 do vetor l3.\nteste &lt;- le_dbc(l3[25])\nteste\nSe tudo estiver certo você vai receber um objeto assim \nApesar da função ter retornado um tibble podemos notar a necessidade de fazer pequenos ajustes para que ela nos retorne um conjunto de dados melhor para ser trabalhado. Os principais pontos são:\n\ntodas as variáveis retornam como texto, transformar em numéricas, datas e fatores conforme o caso.\nnome das variáveis em maiúsculas, usar a função clean_names() do pacote janitor.\nCriar variáveis novas com código de município de 6 caracteres para município de residência e de notificação\nremover variáveis desnecessárias e com baixo grau de completude\n\nVamos utilizar o pacote naniar que tem funções que ajudam a sumarizar, visualizar e manipular dados falantes (missing data) .\nAs funções miss_var_summary e gg_miss_var() vão nos mostrar quais variáveis tem maior grau de dados faltantes.\n naniar::miss_var_summary(teste)\n\nnaniar::gg_miss_var(teste,show_pct = T)\n\nComo podemos observar as variáveis DTRECORIGA, CODANOMAL e IDADEPAI todas têm alto percentual de NAs . A coluna SERIESCMAEparece em quarto com 58% de dados faltantes baseado nessa amostra das DN do Acre de 2020.\n\nle_dbc &lt;- function(arq) {\n  \n  if(!require(read.dbc))  stop(\"o pacote read.dbc deve estar instalado\") # verifica se existe \n   \n  remover &lt;- c('CODANOMAL','DTRECORIGA','SERIESCMAE','IDADEPAI') ## Vars a remover \n\n  origem &lt;- paste0(url,arq)\n  destino &lt;- paste0( tempdir(),'/temp.dbc')\n  \n  download.file(origem,destino,mode=\"wb\")\n  \n  temp &lt;- read.dbc(destino,as.is=T) |&gt; \n    tibble() |&gt; \n    select(!any_of(remover)) |&gt;\n    janitor::clean_names() |&gt; \n    mutate(peso = as.numeric(peso),\n           idademae = as.numeric(idademae),\n           dtnasc = dmy(dtnasc),\n           codmunres6 = str_sub(codmunres,1,6),\n           codmun6 = str_sub(codmunnasc,1,6))\n  \n  file.remove(destino)\n  \n  return(temp)\n}\nUma vez implementada a nova função atendendo a todo o itens podemos executá-la para todo o nosso conjunto de dados usando a função map do pacote purrr.\nNa verdade, vamos usar a função map_df() que agrega os resultados numa estrutura de dados tipo data.frame ou tibble.\nsinasc_ac &lt;- l3 |&gt; map_df(le_dbc)\nEssa função, na minha máquina, demorou 19,5 segundos e retorno um tibble com 433.265 e um tamanho em memória de 233.7 Mb contendo todos os Nascidos vivos do AC disponíveis no FTP datasus\nVamos ver como estamos em termos de completude\n naniar::miss_var_summary(sinasc_ac)\n\nnaniar::gg_miss_var(sinasc_ac,show_pct = T)\nComo se pode observar ainda há muitas variáveis com alta taxa de dados faltantes. Que tal modificar mais uma vez sua função de leitura e remover também essas?\nObserve que o banco passou de 22 colunas em 1996 para 59 em 2022 (ultimo dado disponível em 2025) !\nParece que o seu trabalho não terminou!\n\n\n\n3.2.8 Acesso a Base de Dados\n\n\n3.2.9 Armazenado os dados",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Importando e arrumando dados</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referencias",
    "section": "",
    "text": "Introdução à Ciência de Dados. Pedro A. Morettin, Julio M. Singer; Março 2020 - PDF\nEstatística e Ciência de Dados. Pedro A. Morettin, Julio M. Singer; Abril 2021 - PDF\nModern Data Science with R 2nd edition\nR for Data Science 2nd Edition online R for Data Science. 2nd Edition. Hadley Wickham, Mine Çetinkaya-Rundel, Garrett Grolemund. O’Reilly Media ; 2023.\nAdvanced R 2nd Edition",
    "crumbs": [
      "Referencias"
    ]
  },
  {
    "objectID": "anexo1.html",
    "href": "anexo1.html",
    "title": "Anexo 1 - Instalando o duckdb",
    "section": "",
    "text": "Principais Características\nO duckDB https://duckdb.org/ é um banco de dados do tipo OLAP de alto desempenho, de código aberto, desenvolvido para análise de dados em ambiente de memória (in-memory) e processamento de grandes volumes de dados com baixa latência e alta velocidade.",
    "crumbs": [
      "Anexo 1 - Instalando o duckdb"
    ]
  },
  {
    "objectID": "anexo1.html#duckdb-online",
    "href": "anexo1.html#duckdb-online",
    "title": "Anexo 1 - Instalando o duckdb",
    "section": "duckDB online",
    "text": "duckDB online\nVocê também pode testar o duckdb online atraves desses dois sites abaixo:\n\nSQL Workbench\nDuckDB Web Shell",
    "crumbs": [
      "Anexo 1 - Instalando o duckdb"
    ]
  },
  {
    "objectID": "anexo_xml_json.html",
    "href": "anexo_xml_json.html",
    "title": "Anexo 2 - XML & JSON",
    "section": "",
    "text": "JSON\nXML, do inglês eXtensible Markup Language, é um formato que permite documentos com dados organizados hierarquicamente em formato textos, pode conter banco de dados ou até mesmo desenhos vetoriais. A linguagem XML é classificada como extensível porque permite definir os elementos de marcação (TAGS).\nOs Elementos XML são extensíveis e têm relacionamentos.\nOs Elementos XML têm regras simples para nomes.\nUm exemplo simples!\nAqui um outro exemplo um pouco mais complexo! O cabeçalho define a versão de XML e o tipo de caractere usado!\nOs elementos XML podem ter atributos na tag de abertura, veja que o Elemento Paciente define um atributo matrícula.\nOs atributos são usados para prover informação adicional sobre os elementos.\nE quando temos mais de 1 registro?\nJSON (JavaScript Object Notation - Notação de Objetos JavaScript) é uma formatação leve de troca de dados. Para seres humanos, é fácil de ler e escrever. Para máquinas, é fácil de interpretar e gerar. Está baseado em um subconjunto da linguagem de programação JavaScript. JSON é em formato texto e completamente independente de linguagem, pois usa convenções que são familiares às linguagens C e familiares. Estas propriedades fazem com que JSON seja um formato ideal de troca de dados.\nUm exemplo um pouco mais complicado:",
    "crumbs": [
      "Anexo 2 - XML & JSON"
    ]
  },
  {
    "objectID": "anexo_xml_json.html#json",
    "href": "anexo_xml_json.html#json",
    "title": "Anexo 2 - XML & JSON",
    "section": "",
    "text": "{ \"nome\":\"João da Silva\", \"idade\":37, \"cidade\":\"Rio de Janeiro\" };\n\n{\n  \"nome\": \"João da Silva\",\n  \"sexo\":\"M\",\n  \"idade\": 37,\n  \"cidade\": \"Rio de Janeiro\",\n  \"telefones\": [\n    {\n      \"tipo\": \"celular\",\n      \"numero\": \"2198777777\"\n    },\n    {\n      \"tipo\": \"residencial\",\n      \"numero\": \"2122233344\"\n    }\n  ],\n  \"casado\": false,\n  \"endereço\": null,\n  \"agravos\":[ \"A90\", \"A16\", \"A92.0\" ]\n}",
    "crumbs": [
      "Anexo 2 - XML & JSON"
    ]
  },
  {
    "objectID": "regex.html",
    "href": "regex.html",
    "title": "Anexo 3 - Expressões Regulares",
    "section": "",
    "text": "Uma Expressão Regular, conhecidas por REGEX, é um método formal de se especificar um padrão de texto.\nÉ uma composição de símbolos, caracteres com funções especiais, chamados “metacaracteres” que, agrupados formam uma sequencia, ou expressão regular\nUma expressão regular é testada em textos e retorna sucesso caso este texto obedeça exatamente a todas as suas condições. Neste caso dizemos que o texto “casa” com (match) a expressão regular.\nAs REGEXs servem para se dizer algo abrangente de forma mais rigorosa. Definido o padrão, tem-se uma lista (finita ou não) de possibilidades de casamento.\nExemplo: [rgp]ato pode casar com “rato”, “gato” e “pato” Mas não “mato”\nUtilidade das Expressões Regulares e a busca de padrões em textos\n\nData e Horário\nNúmero IP,Sites\nEndereço de e-mail\nURL\nNúmero de telefone, CPF, cartão de crédito\n\nVários Linguagens de Programação, editores de texto têm suporte às REGEXs.\nPara exemplificar o uso das REGEX em R , usaremos as funções grep, gsub\ncores &lt;- colors()\ngrep(\"blue\",cores)\nRetorna o índice de todos os elementos do vetor cores que possuem “blue”\n\nAlguns Metacaracteres\n\nCircunflexo ^ - Simboliza o início de uma linha\ngrep(\"^blue\",cores) \n​ ​\nRetorna o índice de todos os elementos do vetor cores começados por “blue”\nCifrão $ - Simboliza o fim de uma linha\ngrep(\"yellow$\",cores,value=T)\n​\n\nRetorna o índice de todos os elementos do vetor cores terminados por “yellow”\n\nListas [] - Simboliza conjunto de caracteres numa determinada posição.\n\n    grep(\"^[aeiou]\",cores,value=T)\ncores começadas por vogais\n    grep(\"[0-9]$\",cores,value=T)\ncores terminadas por numeros\n\nPonto . - Simboliza “qualquer” caracteres numa determinada posição\n\n    grep(\"grey..\",cores,value=T)\ncor cinza seguida por dois caracteres qualquer\n    grep(\"^....$\",cores,value=T)\ncores com apenas 4 caracteres\n\nChaves {} - Simboliza a quantidade de repetições do caractere anterior\n      grep(\"e{2}\",cores,value=T)\npadrão onde cores tem dois “e” seguidos\n      grep(\"[0-9]{3,5}\",cores ,value=T)\n\nonde cores tem de 3 a 5 numeros\n\nCuringa * - Simboliza “qualquer coisa”, inclusive nada.\ngrep(\"^[s].*[0-9]$\",cores,value=T)\ncores começadas por “s” e terminadas por numero!\nOperador logico OU (OR) ‘|’\n\nPara fazer um OU lógico, onde buscamos uma coisa OU outra, deve-se usar o | e delimitar as opções entre parênteses: ​\n    grep(\"^[w]|^[y]\",cores,value=T)\nNão deve conter espaços! Nesse exemplo é o mesmo que\n    \"^[wy]\"\n\nOperador logico NÃO (NOT) [^]\n\nPesquisa para retornar as cores que não começam por vogais\n    grep (\"^[bcdfghjklmnpqrstvwxyz]\",cores)\nOu usar negativa (NOT) usando “^”\n    grep (\"^[^aeiou]\", cores,value=T)\nrepare que o não é o ^ que está dentro das chaves!\n\nIntervalo de Lista [-] - Busca por faixa de caracteres ou números\ngrep (“[0-9]{3,}”, cores,value=T)\n\nBusca por números de três dígitos ou mais\ngrep(\"[r-z]$\",cores,value=T) \nBusca por cores terminadas com letras entre r e z ​\nA função gsub faz substituições em strings usando REGEX ​\nteste &lt;- \"   caracteres em branco, no incio e final  \"\ngsub(\"^\\\\s+|\\\\s+$\", \"\", teste)\nExistem varias funções que podem usar REGEX, o R utiliza dialeto de REGEX aprenda um pouco mais nessa Vignette e no no capítulo sobre strings do livro R for Data Science",
    "crumbs": [
      "Anexo 3 - Expressões Regulares"
    ]
  }
]